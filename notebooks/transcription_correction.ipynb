{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transcription-correction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvKP33bhrINKIi00SWPGsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BYU-Handwriting-Lab/GettingStarted/blob/master/notebooks/transcription_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaFc3NgN8cmC",
        "colab_type": "text"
      },
      "source": [
        "# Language Model Text Error Correction\n",
        "\n",
        "This notebook contains code that corrects the output of a handwriting\n",
        "recognition model using techniques from neural machine translation. We\n",
        "implement a basic encoder/decoder architecture with a transformer to\n",
        "correct the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C3o3IxcH1lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRipraCUHv5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Python\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Data Structures\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Image/Plotting\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Debugging\n",
        "from tqdm import tqdm\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode='Verbose', color_scheme='LightBg', tb_offset=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPtxnInShAdc",
        "colab_type": "text"
      },
      "source": [
        "Download the Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw3sn5UnK1CB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ID: 1w0sumZm2YPxgMAsz9utAm9t2HriIe-JL\n",
        "!wget -q --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1w0sumZm2YPxgMAsz9utAm9t2HriIe-JL' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1w0sumZm2YPxgMAsz9utAm9t2HriIe-JL\" -O error.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-HS9CVohFCp",
        "colab_type": "text"
      },
      "source": [
        "### Character to Index Mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGQ6KPHw8YGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CHAR_SET = {\"idx_to_char\": {\"1\": \" \", \"2\": \"!\", \"3\": \"\\\"\", \"4\": \"#\", \"5\": \"$\", \"6\": \"%\", \"7\": \"&\", \"8\": \"'\", \"9\": \"(\",\"10\": \")\", \"11\": \"*\", \"12\": \"+\", \"13\": \",\", \"14\": \"-\", \"15\": \".\", \"16\": \"/\", \"17\": \"0\", \"18\": \"1\", \"19\": \"2\", \"20\": \"3\", \"21\": \"4\", \"22\": \"5\", \"23\": \"6\", \"24\": \"7\", \"25\": \"8\", \"26\": \"9\", \"27\": \":\", \"28\": \";\", \"29\": \"=\", \"30\": \"?\", \"31\": \"A\", \"32\": \"B\", \"33\": \"C\", \"34\": \"D\", \"35\": \"E\", \"36\": \"F\", \"37\": \"G\", \"38\": \"H\", \"39\": \"I\", \"40\": \"J\", \"41\": \"K\", \"42\": \"L\", \"43\": \"M\", \"44\": \"N\", \"45\": \"O\", \"46\": \"P\", \"47\": \"Q\", \"48\": \"R\", \"49\": \"S\", \"50\": \"T\", \"51\": \"U\", \"52\": \"V\", \"53\": \"W\", \"54\": \"X\", \"55\": \"Y\", \"56\": \"Z\", \"57\": \"[\", \"58\": \"]\", \"59\": \"_\", \"60\": \"`\", \"61\": \"a\", \"62\": \"b\", \"63\": \"c\", \"64\": \"d\", \"65\": \"e\", \"66\": \"f\", \"67\": \"g\", \"68\": \"h\", \"69\": \"i\", \"70\": \"j\", \"71\": \"k\", \"72\": \"l\", \"73\": \"m\", \"74\": \"n\", \"75\": \"o\", \"76\": \"p\", \"77\": \"q\", \"78\": \"r\", \"79\": \"s\", \"80\": \"t\", \"81\": \"u\", \"82\": \"v\", \"83\": \"w\", \"84\": \"x\", \"85\": \"y\", \"86\": \"z\", \"87\": \"|\", \"88\": \"~\", \"89\": \"\\u00a3\", \"90\": \"\\u00a7\", \"91\": \"\\u00a8\", \"92\": \"\\u00ab\", \"93\": \"\\u00ac\", \"94\": \"\\u00ad\", \"95\": \"\\u00b0\", \"96\": \"\\u00b2\", \"97\": \"\\u00b4\", \"98\": \"\\u00b7\", \"99\": \"\\u00ba\", \"100\": \"\\u00bb\", \"101\": \"\\u00bc\", \"102\": \"\\u00bd\", \"103\": \"\\u00be\", \"104\": \"\\u00c0\", \"105\": \"\\u00c2\", \"106\": \"\\u00c4\", \"107\": \"\\u00c7\", \"108\": \"\\u00c8\", \"109\": \"\\u00c9\", \"110\": \"\\u00ca\", \"111\": \"\\u00d4\", \"112\": \"\\u00d6\", \"113\": \"\\u00dc\", \"114\": \"\\u00df\", \"115\": \"\\u00e0\", \"116\": \"\\u00e1\", \"117\": \"\\u00e2\", \"118\": \"\\u00e4\", \"119\": \"\\u00e6\", \"120\": \"\\u00e7\", \"121\": \"\\u00e8\", \"122\": \"\\u00e9\", \"123\": \"\\u00ea\", \"124\": \"\\u00eb\", \"125\": \"\\u00ec\", \"126\": \"\\u00ee\", \"127\": \"\\u00ef\", \"128\": \"\\u00f1\", \"129\": \"\\u00f2\", \"130\": \"\\u00f3\", \"131\": \"\\u00f4\", \"132\": \"\\u00f6\", \"133\": \"\\u00f8\", \"134\": \"\\u00f9\", \"135\": \"\\u00fa\", \"136\": \"\\u00fb\", \"137\": \"\\u00fc\", \"138\": \"\\u00ff\", \"139\": \"\\u0142\", \"140\": \"\\u0152\", \"141\": \"\\u0153\", \"142\": \"\\u0393\", \"143\": \"\\u0396\", \"144\": \"\\u03a4\", \"145\": \"\\u03ac\", \"146\": \"\\u03ae\", \"147\": \"\\u03b1\", \"148\": \"\\u03b4\", \"149\": \"\\u03b5\", \"150\": \"\\u03b7\", \"151\": \"\\u03b9\", \"152\": \"\\u03ba\", \"153\": \"\\u03bb\", \"154\": \"\\u03bc\", \"155\": \"\\u03bd\", \"156\": \"\\u03be\", \"157\": \"\\u03bf\", \"158\": \"\\u03c0\", \"159\": \"\\u03c1\", \"160\": \"\\u03c4\", \"161\": \"\\u03c5\", \"162\": \"\\u03c7\", \"163\": \"\\u03c8\", \"164\": \"\\u03c9\", \"165\": \"\\u03cc\", \"166\": \"\\u03ce\", \"167\": \"\\u0406\", \"168\": \"\\u2012\", \"169\": \"\\u2013\", \"170\": \"\\u2014\", \"171\": \"\\u2020\", \"172\": \"\\u2021\", \"173\": \"\\u2030\", \"174\": \"\\u2039\", \"175\": \"\\u203a\", \"176\": \"\\u2082\", \"177\": \"\\u20a4\", \"178\": \"\\u2114\", \"179\": \"\\u2153\", \"180\": \"\\u2154\", \"181\": \"\\u2155\", \"182\": \"\\u2156\", \"183\": \"\\u2157\", \"184\": \"\\u2158\", \"185\": \"\\u2159\", \"186\": \"\\u215a\", \"187\": \"\\u215b\", \"188\": \"\\u2206\", \"189\": \"\\u2207\", \"190\": \"\\u222b\", \"191\": \"\\u2260\", \"192\": \"\\u25a1\", \"193\": \"\\u2640\", \"194\": \"\\u2642\", \"195\": \"\\u2713\", \"196\": \"\\uff46\"},\n",
        "            # \"char_to_idx\": {\"\\u203a\": 175, \"\\u2014\": 170, \"\\u25a1\": 192, \" \": 1, \"\\u00a3\": 89, \"$\": 5, \"\\u00a7\": 90, \"(\": 9, \"\\u00ab\": 92, \"\\u2206\": 188, \",\": 13, \"\\u03b1\": 147, \"0\": 17, \"\\u03b5\": 149, \"4\": 21, \"\\u00b7\": 98, \"\\u03b9\": 151, \"8\": 25, \"\\u00bb\": 100, \"\\u03bd\": 155, \"\\u03c1\": 159, \"\\u2640\": 193, \"\\u0142\": 139, \"\\u03c5\": 161, \"D\": 34, \"\\u00c7\": 107, \"\\u2260\": 191, \"\\u03c9\": 164, \"H\": 38, \"L\": 42, \"P\": 46, \"\\u0152\": 140, \"T\": 50, \"\\u2156\": 182, \"X\": 54, \"\\u215a\": 186, \"\\u00df\": 114, \"`\": 60, \"d\": 64, \"\\u00e7\": 120, \"h\": 68, \"\\u00eb\": 124, \"l\": 72, \"\\u00ef\": 127, \"p\": 76, \"\\u00f3\": 130, \"t\": 80, \"x\": 84, \"\\u00fb\": 136, \"|\": 87, \"\\u00ff\": 138, \"\\u2207\": 189, \"\\u2153\": 179, \"\\u2013\": 169, \"\\u0396\": 143, \"#\": 4, \"\\u20a4\": 177, \"'\": 8, \"\\u00a8\": 91, \"+\": 12, \"\\u00ac\": 93, \"/\": 16, \"\\u03ae\": 146, \"\\u00b0\": 95, \"3\": 20, \"\\u00b4\": 97, \"7\": 24, \";\": 28, \"\\u03ba\": 152, \"\\u00bc\": 101, \"?\": 30, \"\\u03be\": 156, \"\\u00c0\": 104, \"C\": 33, \"\\u00c4\": 106, \"G\": 37, \"\\u2020\": 171, \"\\u00c8\": 108, \"K\": 41, \"O\": 45, \"\\u03ce\": 166, \"S\": 49, \"\\u2155\": 181, \"\\u00d4\": 111, \"W\": 53, \"\\u2159\": 185, \"[\": 57, \"\\u00dc\": 113, \"_\": 59, \"\\u00e0\": 115, \"c\": 63, \"\\u00e4\": 118, \"g\": 67, \"\\u00e8\": 121, \"k\": 71, \"\\u00ec\": 125, \"o\": 75, \"s\": 79, \"\\u00f4\": 131, \"w\": 83, \"\\u00f8\": 133, \"\\u2021\": 172, \"\\u00fc\": 137, \"\\u2030\": 173, \"\\u0406\": 167, \"\\u0393\": 142, \"\\u2012\": 168, \"\\u2114\": 178, \"\\\"\": 3, \"&\": 7, \"*\": 11, \"\\u00ad\": 94, \".\": 15, \"2\": 19, \"\\u03b7\": 150, \"6\": 23, \"\\u03bb\": 153, \":\": 27, \"\\u00bd\": 102, \"\\u03bf\": 157, \"B\": 32, \"\\u03c7\": 162, \"F\": 36, \"\\u00c9\": 109, \"J\": 40, \"N\": 44, \"R\": 48, \"\\u2154\": 180, \"V\": 52, \"\\u2158\": 184, \"Z\": 56, \"\\u00e1\": 116, \"b\": 62, \"\\u2039\": 174, \"f\": 66, \"\\u00e9\": 122, \"j\": 70, \"n\": 74, \"\\u00f1\": 128, \"r\": 78, \"v\": 82, \"\\u00f9\": 134, \"z\": 86, \"~\": 88, \"\\u2082\": 176, \"\\u2713\": 195, \"\\u2642\": 194, \"!\": 2, \"%\": 6, \"\\u03a4\": 144, \")\": 10, \"\\uff46\": 196, \"-\": 14, \"\\u03ac\": 145, \"1\": 18, \"\\u00b2\": 96, \"5\": 22, \"\\u03b4\": 148, \"9\": 26, \"\\u00ba\": 99, \"=\": 29, \"\\u03bc\": 154, \"\\u00be\": 103, \"A\": 31, \"\\u03c0\": 158, \"\\u00c2\": 105, \"E\": 35, \"\\u03c4\": 160, \"I\": 39, \"\\u03c8\": 163, \"\\u00ca\": 110, \"M\": 43, \"\\u03cc\": 165, \"Q\": 47, \"\\u0153\": 141, \"U\": 51, \"\\u2157\": 183, \"\\u00d6\": 112, \"Y\": 55, \"\\u215b\": 187, \"]\": 58, \"a\": 61, \"\\u00e2\": 117, \"e\": 65, \"\\u00e6\": 119, \"i\": 69, \"\\u00ea\": 123, \"m\": 73, \"\\u00ee\": 126, \"q\": 77, \"\\u00f2\": 129, \"u\": 81, \"\\u00f6\": 132, \"y\": 85, \"\\u00fa\": 135, \"\\u222b\": 190}\n",
        "            # }\n",
        "\n",
        "CHAR_SET = {'char_to_idx': {'Ĵ': '318', '¬': '1', 'Õ': '2', 'Y': '3', 'Į': '4', 'ø': '5', 'Ÿ': '6', ',': '7', '«': '8', 'ĳ': '9', 'e': '10', 'Ô': '11', 'U': '12', '[': '13', 'j': '14', 'Ũ': '15', '3': '16', 'o': '17', 'ï': '18', 'd': '19', 'x': '20', 'ċ': '21', 'Ü': '22', 'ı': '23', 'Ð': '24', 'Ď': '25', 'Ŋ': '26', '2': '27', '®': '28', '9': '29', 'ß': '30', 'ľ': '31', '/': '32', 'V': '33', '½': '34', 'û': '35', 'h': '36', 'ě': '37', 'r': '38', 'm': '39', '¥': '40', 'g': '41', 'ĺ': '42', 'B': '43', 'Ė': '44', 'Ř': '45', 'Ĺ': '46', 'Ò': '47', 'ĥ': '48', 'À': '49', '{': '50', 'Ž': '51', 'ã': '52', ':': '53', 'Ì': '54', 'Ī': '55', 'Ķ': '56', 'ń': '57', 'õ': '58', 'Å': '59', 'G': '60', 'È': '61', 'ſ': '62', 'Ą': '63', '5': '64', 'ë': '65', 'Ō': '66', 'ŋ': '67', 'ţ': '68', 'Ħ': '69', 'Q': '70', 'č': '71', 'Ŀ': '72', '=': '73', 'Ĉ': '74', 'Ş': '75', 'Ū': '76', 'ħ': '77', 'ŗ': '78', 'É': '79', '%': '80', 'ť': '81', 'æ': '82', '±': '83', '?': '84', 'D': '85', '»': '86', 'ż': '87', 'ć': '88', '<': '89', '|': '90', 'C': '91', 'Ġ': '92', '´': '93', 'ŏ': '94', '.': '95', '$': '96', 'ü': '97', '+': '98', 'ġ': '99', 'Ï': '100', 'ŕ': '101', 'Ă': '102', 'i': '103', 'Ý': '104', '\"': '105', 'w': '106', 'Ù': '107', 'Ŝ': '108', 'Đ': '109', 'Ä': '110', 'ì': '111', '`': '112', 'ű': '113', '\\xad': '114', 'ģ': '115', 'î': '116', '7': '117', 'Ö': '118', 'İ': '119', 'ĵ': '120', 'Z': '121', '¶': '122', 'Ņ': '123', '¨': '124', '4': '125', 'R': '126', ']': '127', '^': '128', 'F': '129', 'ļ': '130', 'ğ': '131', 'k': '132', 'ī': '133', 'é': '134', 'ŉ': '135', 'Ń': '136', 'Ľ': '137', '!': '138', 'ù': '139', 'Ĳ': '140', 'S': '141', 'E': '142', 'â': '143', ')': '144', '·': '145', '¾': '146', 'Þ': '147', 'Ł': '148', 'ř': '149', 'Ļ': '150', 'Ê': '151', 'ä': '152', 'n': '153', 'œ': '154', '(': '155', 'ĕ': '156', '§': '157', 'ê': '158', '°': '159', 'ý': '160', '@': '161', 'Ź': '162', '-': '163', 'Ţ': '164', 'ũ': '165', 'ė': '166', '0': '167', 'Ĩ': '168', 'ş': '169', 'š': '170', 'ō': '171', 'ą': '172', 'H': '173', 'ų': '174', 'O': '175', 'ŭ': '176', ' ': '177', 'ñ': '178', 'ś': '179', 'b': '180', '¦': '181', 'Ú': '182', 'Œ': '183', 'ª': '184', 'ĩ': '185', 'W': '186', 'M': '187', 'ă': '188', 'ö': '189', 'ž': '190', 'ò': '191', 'µ': '192', 'f': '193', 'ň': '194', 'þ': '195', '1': '196', 'Ç': '197', 'Ć': '198', '¹': '199', 'Ŗ': '200', 'á': '201', 'c': '202', '>': '203', '8': '204', 'ł': '205', 'Š': '206', 'ő': '207', 'Ģ': '208', 'ŷ': '209', 'Ĕ': '210', 'Ś': '211', 'ŝ': '212', 'ź': '213', 'Â': '214', 'ĭ': '215', '³': '216', 'Ċ': '217', 'Ã': '218', 'į': '219', 'l': '220', 'Û': '221', 'Ĭ': '222', 'Ŧ': '223', 'Ż': '224', 'K': '225', 'N': '226', '¡': '227', '_': '228', 'å': '229', '£': '230', 'ū': '231', 'Ų': '232', '×': '233', 'Ā': '234', 'u': '235', 'ů': '236', 'Ě': '237', '*': '238', 'v': '239', 'T': '240', 'Ŕ': '241', 'ē': '242', 'A': '243', 'X': '244', '¼': '245', 'q': '246', '¤': '247', 's': '248', 'Ű': '249', 't': '250', 'Ŷ': '251', 'Č': '252', 'ĝ': '253', '\\\\': '254', 'Ů': '255', '#': '256', \"'\": '257', 'Á': '258', '¿': '259', '}': '260', 'y': '261', 'Ē': '262', 'Ŭ': '263', 'Ë': '264', '~': '265', 'Ę': '266', 'Ŵ': '267', 'Æ': '268', 'ð': '269', 'º': '270', 'Ó': '271', 'ā': '272', 'ô': '273', 'J': '274', 'ÿ': '275', 'ó': '276', 'Ĝ': '277', '&': '278', 'P': '279', '©': '280', 'Ğ': '281', 'è': '282', 'ę': '283', 'ĸ': '284', '²': '285', 'Ĥ': '286', '¢': '287', 'ŵ': '288', 'Î': '289', 'đ': '290', 'Í': '291', 'a': '292', ';': '293', 'à': '294', '¯': '295', '¸': '296', 'ņ': '297', 'L': '298', 'Ő': '299', 'ķ': '300', 'p': '301', 'Ŏ': '302', 'í': '303', 'ŧ': '304', 'ç': '305', 'Ť': '306', 'ŀ': '307', 'z': '308', 'ď': '309', 'Ň': '310', '6': '311', 'I': '312', '÷': '313', 'ú': '314', 'Ø': '315', 'Ñ': '316', 'ĉ': '317'},\n",
        "            'idx_to_char': {'318': 'Ĵ', '1': '¬', '2': 'Õ', '3': 'Y', '4': 'Į', '5': 'ø', '6': 'Ÿ', '7': ',', '8': '«', '9': 'ĳ', '10': 'e', '11': 'Ô', '12': 'U', '13': '[', '14': 'j', '15': 'Ũ', '16': '3', '17': 'o', '18': 'ï', '19': 'd', '20': 'x', '21': 'ċ', '22': 'Ü', '23': 'ı', '24': 'Ð', '25': 'Ď', '26': 'Ŋ', '27': '2', '28': '®', '29': '9', '30': 'ß', '31': 'ľ', '32': '/', '33': 'V', '34': '½', '35': 'û', '36': 'h', '37': 'ě', '38': 'r', '39': 'm', '40': '¥', '41': 'g', '42': 'ĺ', '43': 'B', '44': 'Ė', '45': 'Ř', '46': 'Ĺ', '47': 'Ò', '48': 'ĥ', '49': 'À', '50': '{', '51': 'Ž', '52': 'ã', '53': ':', '54': 'Ì', '55': 'Ī', '56': 'Ķ', '57': 'ń', '58': 'õ', '59': 'Å', '60': 'G', '61': 'È', '62': 'ſ', '63': 'Ą', '64': '5', '65': 'ë', '66': 'Ō', '67': 'ŋ', '68': 'ţ', '69': 'Ħ', '70': 'Q', '71': 'č', '72': 'Ŀ', '73': '=', '74': 'Ĉ', '75': 'Ş', '76': 'Ū', '77': 'ħ', '78': 'ŗ', '79': 'É', '80': '%', '81': 'ť', '82': 'æ', '83': '±', '84': '?', '85': 'D', '86': '»', '87': 'ż', '88': 'ć', '89': '<', '90': '|', '91': 'C', '92': 'Ġ', '93': '´', '94': 'ŏ', '95': '.', '96': '$', '97': 'ü', '98': '+', '99': 'ġ', '100': 'Ï', '101': 'ŕ', '102': 'Ă', '103': 'i', '104': 'Ý', '105': '\"', '106': 'w', '107': 'Ù', '108': 'Ŝ', '109': 'Đ', '110': 'Ä', '111': 'ì', '112': '`', '113': 'ű', '114': '\\xad', '115': 'ģ', '116': 'î', '117': '7', '118': 'Ö', '119': 'İ', '120': 'ĵ', '121': 'Z', '122': '¶', '123': 'Ņ', '124': '¨', '125': '4', '126': 'R', '127': ']', '128': '^', '129': 'F', '130': 'ļ', '131': 'ğ', '132': 'k', '133': 'ī', '134': 'é', '135': 'ŉ', '136': 'Ń', '137': 'Ľ', '138': '!', '139': 'ù', '140': 'Ĳ', '141': 'S', '142': 'E', '143': 'â', '144': ')', '145': '·', '146': '¾', '147': 'Þ', '148': 'Ł', '149': 'ř', '150': 'Ļ', '151': 'Ê', '152': 'ä', '153': 'n', '154': 'œ', '155': '(', '156': 'ĕ', '157': '§', '158': 'ê', '159': '°', '160': 'ý', '161': '@', '162': 'Ź', '163': '-', '164': 'Ţ', '165': 'ũ', '166': 'ė', '167': '0', '168': 'Ĩ', '169': 'ş', '170': 'š', '171': 'ō', '172': 'ą', '173': 'H', '174': 'ų', '175': 'O', '176': 'ŭ', '177': ' ', '178': 'ñ', '179': 'ś', '180': 'b', '181': '¦', '182': 'Ú', '183': 'Œ', '184': 'ª', '185': 'ĩ', '186': 'W', '187': 'M', '188': 'ă', '189': 'ö', '190': 'ž', '191': 'ò', '192': 'µ', '193': 'f', '194': 'ň', '195': 'þ', '196': '1', '197': 'Ç', '198': 'Ć', '199': '¹', '200': 'Ŗ', '201': 'á', '202': 'c', '203': '>', '204': '8', '205': 'ł', '206': 'Š', '207': 'ő', '208': 'Ģ', '209': 'ŷ', '210': 'Ĕ', '211': 'Ś', '212': 'ŝ', '213': 'ź', '214': 'Â', '215': 'ĭ', '216': '³', '217': 'Ċ', '218': 'Ã', '219': 'į', '220': 'l', '221': 'Û', '222': 'Ĭ', '223': 'Ŧ', '224': 'Ż', '225': 'K', '226': 'N', '227': '¡', '228': '_', '229': 'å', '230': '£', '231': 'ū', '232': 'Ų', '233': '×', '234': 'Ā', '235': 'u', '236': 'ů', '237': 'Ě', '238': '*', '239': 'v', '240': 'T', '241': 'Ŕ', '242': 'ē', '243': 'A', '244': 'X', '245': '¼', '246': 'q', '247': '¤', '248': 's', '249': 'Ű', '250': 't', '251': 'Ŷ', '252': 'Č', '253': 'ĝ', '254': '\\\\', '255': 'Ů', '256': '#', '257': \"'\", '258': 'Á', '259': '¿', '260': '}', '261': 'y', '262': 'Ē', '263': 'Ŭ', '264': 'Ë', '265': '~', '266': 'Ę', '267': 'Ŵ', '268': 'Æ', '269': 'ð', '270': 'º', '271': 'Ó', '272': 'ā', '273': 'ô', '274': 'J', '275': 'ÿ', '276': 'ó', '277': 'Ĝ', '278': '&', '279': 'P', '280': '©', '281': 'Ğ', '282': 'è', '283': 'ę', '284': 'ĸ', '285': '²', '286': 'Ĥ', '287': '¢', '288': 'ŵ', '289': 'Î', '290': 'đ', '291': 'Í', '292': 'a', '293': ';', '294': 'à', '295': '¯', '296': '¸', '297': 'ņ', '298': 'L', '299': 'Ő', '300': 'ķ', '301': 'p', '302': 'Ŏ', '303': 'í', '304': 'ŧ', '305': 'ç', '306': 'Ť', '307': 'ŀ', '308': 'z', '309': 'ď', '310': 'Ň', '311': '6', '312': 'I', '313': '÷', '314': 'ú', '315': 'Ø', '316': 'Ñ', '317': 'ĉ'}\n",
        "           }\n",
        "\n",
        "class CharsetMapper:\n",
        "    def __init__(self, max_sequence_size=128, blank_character=0):\n",
        "        self.max_sequence_size = max_sequence_size\n",
        "        self.blank_character = blank_character\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_duplicates(idxs):\n",
        "        new_idxs = []\n",
        "\n",
        "        for i in range(len(idxs)):\n",
        "            # Only append if the next character in the sequence is not\n",
        "            # identical to the current character. If we're at the end of\n",
        "            # the sequence, add it.\n",
        "            if i + 1 == len(idxs) or idxs[i] != idxs[i + 1]:\n",
        "                new_idxs.append(idxs[i])\n",
        "\n",
        "        return new_idxs\n",
        "\n",
        "    def idx_to_char(self, idx):\n",
        "        if idx == self.blank_character:\n",
        "            return ''  # Return empty string for the blank character\n",
        "        else:\n",
        "          try:\n",
        "            return CHAR_SET['idx_to_char'][str(int(idx))]\n",
        "          except KeyError:\n",
        "            return ''\n",
        "\n",
        "    def char_to_idx(self, char):\n",
        "        try:\n",
        "          return int(CHAR_SET['char_to_idx'][char])\n",
        "        except KeyError:\n",
        "          return self.blank_character\n",
        "\n",
        "    def str_to_idxs(self, string):\n",
        "        idxs = []\n",
        "\n",
        "        zeros = np.full(self.max_sequence_size, self.blank_character)\n",
        "        for char in string:\n",
        "            idxs.append(self.char_to_idx(char))\n",
        "\n",
        "        # Pad the array to the max sequence size\n",
        "        idxs = np.concatenate((idxs, zeros))[:self.max_sequence_size]\n",
        "\n",
        "        return idxs\n",
        "\n",
        "    def idxs_to_str(self, idxs, remove_duplicates=True):\n",
        "        string = ''\n",
        "\n",
        "        if remove_duplicates:\n",
        "            idxs = CharsetMapper.remove_duplicates(idxs)\n",
        "\n",
        "        for idx in idxs:\n",
        "            string += self.idx_to_char(idx)\n",
        "\n",
        "        return string\n",
        "\n",
        "    def str_to_idxs_batch(self, batch):\n",
        "        idxs = []\n",
        "\n",
        "        for string in batch:\n",
        "            idx = self.str_to_idxs(string)\n",
        "            idxs.append(idx)\n",
        "\n",
        "        return idxs\n",
        "\n",
        "    def idxs_to_str_batch(self, batch, remove_duplicates=True):\n",
        "        strings = []\n",
        "\n",
        "        for idxs in batch:\n",
        "            strings.append(self.idxs_to_str(idxs, remove_duplicates=remove_duplicates))\n",
        "\n",
        "        return strings\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "      return len(CHAR_SET['char_to_idx']) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6e0fEdTzQ6w",
        "colab_type": "text"
      },
      "source": [
        "### Data Loading\n",
        "* Keras Sequence\n",
        "* TfRecord Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xDztzvILNTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ErrorSequence(tf.keras.utils.Sequence):\n",
        "  def __init__(self, path='/content/error.csv'):\n",
        "    self.df = pd.read_csv(path, header=None, sep='\\t', names=['original', 'error'])\n",
        "    self.charset_mapper = CharsetMapper()\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    x = self.charset_mapper.str_to_idxs(str(self.df['error'][index]))\n",
        "    y = self.charset_mapper.str_to_idxs(str(self.df['original'][index]))\n",
        "\n",
        "    return tf.constant(x), tf.constant(y)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UajArLvbMGdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f4e79c04-8403-423e-fb09-b67f2e1d8bc0"
      },
      "source": [
        "sequence = ErrorSequence()\n",
        "mapper = CharsetMapper()\n",
        "x, y = sequence[4]\n",
        "\n",
        "print('error:', mapper.idxs_to_str(x.numpy()))\n",
        "print('corrected:', mapper.idxs_to_str(y.numpy()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error: EXste alfarábio, nã¨o o devo ao meu velhÀ cronista do Paseioy PúbJico. É, como se dise no\n",
            "corrected: Este alfarábio, não o devo ao meu velho cronista do Paseio Público. É, como se dise no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKaK9cFCbad",
        "colab_type": "text"
      },
      "source": [
        "Create the TfRecord Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09nAjJIZxauR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tfrecord_from_sequence(sequence, tfrecord_path):\n",
        "    \"\"\"\n",
        "    Create a TfRecord dataset from a sequence\n",
        "\n",
        "    :param sequence: The Keras sequence to load dataasdfs of arbitrary format\n",
        "    :param tfrecord_path: Filepath and name for location of TfRecord dataset\n",
        "    \"\"\"\n",
        "    print('Started creating TFRecord Dataset...')\n",
        "\n",
        "    writer = tf.io.TFRecordWriter(tfrecord_path)\n",
        "\n",
        "    for index, (img, label) in enumerate(sequence):\n",
        "        feature = {'label': _bytes_feature(tf.io.serialize_tensor(label)),\n",
        "                   'image': _bytes_feature(tf.io.serialize_tensor(img))}\n",
        "\n",
        "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "        writer.write(example.SerializeToString())\n",
        "        if index % 1000 == 0:\n",
        "            print(str(index) + '/' + str(len(sequence)))\n",
        "\n",
        "    print(str(len(sequence)) + '/' + str(len(sequence)))\n",
        "\n",
        "    print('Finished: TFRecord created at', tfrecord_path)\n",
        "\n",
        "\n",
        "def read_tfrecord(single_record):\n",
        "    \"\"\"\n",
        "    Function to decode a TfRecord. Usually this function will be called within\n",
        "    a TfDataset map function. Note that out_types for image and label must be\n",
        "    tf.float32 and tf.int64 respectively.\n",
        "\n",
        "    :param single_record: A single TfRecord\n",
        "    :return: A decoded image and label as tensors\n",
        "    \"\"\"\n",
        "    feature_description = {\n",
        "        'label': tf.io.FixedLenFeature((), tf.string),\n",
        "        'image': tf.io.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "\n",
        "    single_record = tf.io.parse_single_example(single_record, feature_description)\n",
        "\n",
        "    image = tf.io.parse_tensor(single_record['image'], out_type=tf.int64)\n",
        "    label = tf.io.parse_tensor(single_record['label'], out_type=tf.int64)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "\n",
        "def _float_feature(value):\n",
        "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "\n",
        "def _int64_feature(value):\n",
        "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H34ngQ6xcVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5fc231fa-fcdb-4658-90da-9845fadf0a07"
      },
      "source": [
        "create_tfrecord_from_sequence(ErrorSequence(), 'error.tfrecord')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started creating TFRecord Dataset...\n",
            "0/341304\n",
            "1000/341304\n",
            "2000/341304\n",
            "3000/341304\n",
            "4000/341304\n",
            "5000/341304\n",
            "6000/341304\n",
            "7000/341304\n",
            "8000/341304\n",
            "9000/341304\n",
            "10000/341304\n",
            "11000/341304\n",
            "12000/341304\n",
            "13000/341304\n",
            "14000/341304\n",
            "15000/341304\n",
            "16000/341304\n",
            "17000/341304\n",
            "18000/341304\n",
            "19000/341304\n",
            "20000/341304\n",
            "21000/341304\n",
            "22000/341304\n",
            "23000/341304\n",
            "24000/341304\n",
            "25000/341304\n",
            "26000/341304\n",
            "27000/341304\n",
            "28000/341304\n",
            "29000/341304\n",
            "30000/341304\n",
            "31000/341304\n",
            "32000/341304\n",
            "33000/341304\n",
            "34000/341304\n",
            "35000/341304\n",
            "36000/341304\n",
            "37000/341304\n",
            "38000/341304\n",
            "39000/341304\n",
            "40000/341304\n",
            "41000/341304\n",
            "42000/341304\n",
            "43000/341304\n",
            "44000/341304\n",
            "45000/341304\n",
            "46000/341304\n",
            "47000/341304\n",
            "48000/341304\n",
            "49000/341304\n",
            "50000/341304\n",
            "51000/341304\n",
            "52000/341304\n",
            "53000/341304\n",
            "54000/341304\n",
            "55000/341304\n",
            "56000/341304\n",
            "57000/341304\n",
            "58000/341304\n",
            "59000/341304\n",
            "60000/341304\n",
            "61000/341304\n",
            "62000/341304\n",
            "63000/341304\n",
            "64000/341304\n",
            "65000/341304\n",
            "66000/341304\n",
            "67000/341304\n",
            "68000/341304\n",
            "69000/341304\n",
            "70000/341304\n",
            "71000/341304\n",
            "72000/341304\n",
            "73000/341304\n",
            "74000/341304\n",
            "75000/341304\n",
            "76000/341304\n",
            "77000/341304\n",
            "78000/341304\n",
            "79000/341304\n",
            "80000/341304\n",
            "81000/341304\n",
            "82000/341304\n",
            "83000/341304\n",
            "84000/341304\n",
            "85000/341304\n",
            "86000/341304\n",
            "87000/341304\n",
            "88000/341304\n",
            "89000/341304\n",
            "90000/341304\n",
            "91000/341304\n",
            "92000/341304\n",
            "93000/341304\n",
            "94000/341304\n",
            "95000/341304\n",
            "96000/341304\n",
            "97000/341304\n",
            "98000/341304\n",
            "99000/341304\n",
            "100000/341304\n",
            "101000/341304\n",
            "102000/341304\n",
            "103000/341304\n",
            "104000/341304\n",
            "105000/341304\n",
            "106000/341304\n",
            "107000/341304\n",
            "108000/341304\n",
            "109000/341304\n",
            "110000/341304\n",
            "111000/341304\n",
            "112000/341304\n",
            "113000/341304\n",
            "114000/341304\n",
            "115000/341304\n",
            "116000/341304\n",
            "117000/341304\n",
            "118000/341304\n",
            "119000/341304\n",
            "120000/341304\n",
            "121000/341304\n",
            "122000/341304\n",
            "123000/341304\n",
            "124000/341304\n",
            "125000/341304\n",
            "126000/341304\n",
            "127000/341304\n",
            "128000/341304\n",
            "129000/341304\n",
            "130000/341304\n",
            "131000/341304\n",
            "132000/341304\n",
            "133000/341304\n",
            "134000/341304\n",
            "135000/341304\n",
            "136000/341304\n",
            "137000/341304\n",
            "138000/341304\n",
            "139000/341304\n",
            "140000/341304\n",
            "141000/341304\n",
            "142000/341304\n",
            "143000/341304\n",
            "144000/341304\n",
            "145000/341304\n",
            "146000/341304\n",
            "147000/341304\n",
            "148000/341304\n",
            "149000/341304\n",
            "150000/341304\n",
            "151000/341304\n",
            "152000/341304\n",
            "153000/341304\n",
            "154000/341304\n",
            "155000/341304\n",
            "156000/341304\n",
            "157000/341304\n",
            "158000/341304\n",
            "159000/341304\n",
            "160000/341304\n",
            "161000/341304\n",
            "162000/341304\n",
            "163000/341304\n",
            "164000/341304\n",
            "165000/341304\n",
            "166000/341304\n",
            "167000/341304\n",
            "168000/341304\n",
            "169000/341304\n",
            "170000/341304\n",
            "171000/341304\n",
            "172000/341304\n",
            "173000/341304\n",
            "174000/341304\n",
            "175000/341304\n",
            "176000/341304\n",
            "177000/341304\n",
            "178000/341304\n",
            "179000/341304\n",
            "180000/341304\n",
            "181000/341304\n",
            "182000/341304\n",
            "183000/341304\n",
            "184000/341304\n",
            "185000/341304\n",
            "186000/341304\n",
            "187000/341304\n",
            "188000/341304\n",
            "189000/341304\n",
            "190000/341304\n",
            "191000/341304\n",
            "192000/341304\n",
            "193000/341304\n",
            "194000/341304\n",
            "195000/341304\n",
            "196000/341304\n",
            "197000/341304\n",
            "198000/341304\n",
            "199000/341304\n",
            "200000/341304\n",
            "201000/341304\n",
            "202000/341304\n",
            "203000/341304\n",
            "204000/341304\n",
            "205000/341304\n",
            "206000/341304\n",
            "207000/341304\n",
            "208000/341304\n",
            "209000/341304\n",
            "210000/341304\n",
            "211000/341304\n",
            "212000/341304\n",
            "213000/341304\n",
            "214000/341304\n",
            "215000/341304\n",
            "216000/341304\n",
            "217000/341304\n",
            "218000/341304\n",
            "219000/341304\n",
            "220000/341304\n",
            "221000/341304\n",
            "222000/341304\n",
            "223000/341304\n",
            "224000/341304\n",
            "225000/341304\n",
            "226000/341304\n",
            "227000/341304\n",
            "228000/341304\n",
            "229000/341304\n",
            "230000/341304\n",
            "231000/341304\n",
            "232000/341304\n",
            "233000/341304\n",
            "234000/341304\n",
            "235000/341304\n",
            "236000/341304\n",
            "237000/341304\n",
            "238000/341304\n",
            "239000/341304\n",
            "240000/341304\n",
            "241000/341304\n",
            "242000/341304\n",
            "243000/341304\n",
            "244000/341304\n",
            "245000/341304\n",
            "246000/341304\n",
            "247000/341304\n",
            "248000/341304\n",
            "249000/341304\n",
            "250000/341304\n",
            "251000/341304\n",
            "252000/341304\n",
            "253000/341304\n",
            "254000/341304\n",
            "255000/341304\n",
            "256000/341304\n",
            "257000/341304\n",
            "258000/341304\n",
            "259000/341304\n",
            "260000/341304\n",
            "261000/341304\n",
            "262000/341304\n",
            "263000/341304\n",
            "264000/341304\n",
            "265000/341304\n",
            "266000/341304\n",
            "267000/341304\n",
            "268000/341304\n",
            "269000/341304\n",
            "270000/341304\n",
            "271000/341304\n",
            "272000/341304\n",
            "273000/341304\n",
            "274000/341304\n",
            "275000/341304\n",
            "276000/341304\n",
            "277000/341304\n",
            "278000/341304\n",
            "279000/341304\n",
            "280000/341304\n",
            "281000/341304\n",
            "282000/341304\n",
            "283000/341304\n",
            "284000/341304\n",
            "285000/341304\n",
            "286000/341304\n",
            "287000/341304\n",
            "288000/341304\n",
            "289000/341304\n",
            "290000/341304\n",
            "291000/341304\n",
            "292000/341304\n",
            "293000/341304\n",
            "294000/341304\n",
            "295000/341304\n",
            "296000/341304\n",
            "297000/341304\n",
            "298000/341304\n",
            "299000/341304\n",
            "300000/341304\n",
            "301000/341304\n",
            "302000/341304\n",
            "303000/341304\n",
            "304000/341304\n",
            "305000/341304\n",
            "306000/341304\n",
            "307000/341304\n",
            "308000/341304\n",
            "309000/341304\n",
            "310000/341304\n",
            "311000/341304\n",
            "312000/341304\n",
            "313000/341304\n",
            "314000/341304\n",
            "315000/341304\n",
            "316000/341304\n",
            "317000/341304\n",
            "318000/341304\n",
            "319000/341304\n",
            "320000/341304\n",
            "321000/341304\n",
            "322000/341304\n",
            "323000/341304\n",
            "324000/341304\n",
            "325000/341304\n",
            "326000/341304\n",
            "327000/341304\n",
            "328000/341304\n",
            "329000/341304\n",
            "330000/341304\n",
            "331000/341304\n",
            "332000/341304\n",
            "333000/341304\n",
            "334000/341304\n",
            "335000/341304\n",
            "336000/341304\n",
            "337000/341304\n",
            "338000/341304\n",
            "339000/341304\n",
            "340000/341304\n",
            "341000/341304\n",
            "341304/341304\n",
            "Finished: TFRecord created at error.tfrecord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhw-p6Bax7oK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "54993919-93cc-401b-b6f7-66c86c971bd0"
      },
      "source": [
        "dataset = tf.data.TFRecordDataset('error.tfrecord').take(30000).map(read_tfrecord)\n",
        "\n",
        "for image_features in dataset.take(1):\n",
        "  print('Error:', mapper.idxs_to_str(image_features[0]))\n",
        "  print('Corrected:', mapper.idxs_to_str(image_features[1]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: A ALMADO LÁZARO\n",
            "Corrected: A ALMA DO LÁZARO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmRsLrjjzJ6T",
        "colab_type": "text"
      },
      "source": [
        "### Create the Sequence-to-Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbZXoOMLAZIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 100\n",
        "EMBEDDING_DIM = 128\n",
        "UNITS = 128\n",
        "\n",
        "mapper = CharsetMapper()\n",
        "dataset = tf.data.TFRecordDataset('error.tfrecord').take(30000).map(read_tfrecord).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5L8DL5NGcmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q-JI__CAUNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkqHWdV9Aopn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "114cda9f-9598-49b9-8bf0-a2e97c517ad2"
      },
      "source": [
        "encoder = Encoder(mapper.get_vocab_size(), EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
        "encoder.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 128, 128)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 128)\n",
            "Model: \"encoder_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      multiple                  40832     \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  multiple                  99072     \n",
            "=================================================================\n",
            "Total params: 139,904\n",
            "Trainable params: 139,904\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp3yGFosBXIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72KPuehXBZtZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2c41c75c-95e4-436c-fde2-ad5699f3fba2"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 128)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asYEefzoBdZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uFnZ8xaBfru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "c9ff4ec6-b3a9-4b14-d1e8-4c739bd61a7a"
      },
      "source": [
        "decoder = Decoder(mapper.get_vocab_size(), EMBEDDING_DIM, UNITS, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
        "decoder.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 319)\n",
            "Model: \"decoder_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      multiple                  40832     \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  multiple                  148224    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             multiple                  41151     \n",
            "_________________________________________________________________\n",
            "bahdanau_attention_6 (Bahdan multiple                  33153     \n",
            "=================================================================\n",
            "Total params: 263,360\n",
            "Trainable params: 263,360\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMCaHD2QBuOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaaELj0LByOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh7MChFkB5GS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([0] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      print('Iter:', t)\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Uo0lgnB6_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a936f1b-1b37-4fc4-aebd-cee5e51e6430"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  train_loop = tqdm(total=30000//BATCH_SIZE, position=0, leave=True)\n",
        "  for (batch, (inp, targ)) in enumerate(dataset):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    train_loss(batch_loss)\n",
        "    train_loop.set_description('Epoch: {}, Loss: {:.4f}'.format(epoch, train_loss.result()))\n",
        "    train_loop.update(1)\n",
        "  train_loop.close()\n",
        "\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 1.0877:  58%|█████▊    | 272/468 [03:51<02:37,  1.24it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUR0yB3IHrok",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEsJxxIqHtaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PemxwGVDHwUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX5M5p1iHy2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}