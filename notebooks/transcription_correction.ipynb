{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transcription-correction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfPyOxbmD6hVHkkgsVZ72/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BYU-Handwriting-Lab/GettingStarted/blob/master/notebooks/transcription_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaFc3NgN8cmC",
        "colab_type": "text"
      },
      "source": [
        "# Language Model Text Error Correction\n",
        "\n",
        "This notebook contains code that corrects the output of a handwriting\n",
        "recognition model using techniques from neural machine translation. We\n",
        "implement a basic encoder/decoder architecture with a transformer to\n",
        "correct the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C3o3IxcH1lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRipraCUHv5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Python\n",
        "import os\n",
        "\n",
        "# Data Structures\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Image/Plotting\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Debugging\n",
        "from tqdm import tqdm\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode='Verbose', color_scheme='LightBg', tb_offset=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPtxnInShAdc",
        "colab_type": "text"
      },
      "source": [
        "Download the Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw3sn5UnK1CB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ID: 1w0sumZm2YPxgMAsz9utAm9t2HriIe-JL\n",
        "!wget -q --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1w0sumZm2YPxgMAsz9utAm9t2HriIe-JL' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1w0sumZm2YPxgMAsz9utAm9t2HriIe-JL\" -O error.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-HS9CVohFCp",
        "colab_type": "text"
      },
      "source": [
        "### Character to Index Mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGQ6KPHw8YGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHAR_SET = {\"idx_to_char\": {\"1\": \" \", \"2\": \"!\", \"3\": \"\\\"\", \"4\": \"#\", \"5\": \"$\", \"6\": \"%\", \"7\": \"&\", \"8\": \"'\", \"9\": \"(\",\"10\": \")\", \"11\": \"*\", \"12\": \"+\", \"13\": \",\", \"14\": \"-\", \"15\": \".\", \"16\": \"/\", \"17\": \"0\", \"18\": \"1\", \"19\": \"2\", \"20\": \"3\", \"21\": \"4\", \"22\": \"5\", \"23\": \"6\", \"24\": \"7\", \"25\": \"8\", \"26\": \"9\", \"27\": \":\", \"28\": \";\", \"29\": \"=\", \"30\": \"?\", \"31\": \"A\", \"32\": \"B\", \"33\": \"C\", \"34\": \"D\", \"35\": \"E\", \"36\": \"F\", \"37\": \"G\", \"38\": \"H\", \"39\": \"I\", \"40\": \"J\", \"41\": \"K\", \"42\": \"L\", \"43\": \"M\", \"44\": \"N\", \"45\": \"O\", \"46\": \"P\", \"47\": \"Q\", \"48\": \"R\", \"49\": \"S\", \"50\": \"T\", \"51\": \"U\", \"52\": \"V\", \"53\": \"W\", \"54\": \"X\", \"55\": \"Y\", \"56\": \"Z\", \"57\": \"[\", \"58\": \"]\", \"59\": \"_\", \"60\": \"`\", \"61\": \"a\", \"62\": \"b\", \"63\": \"c\", \"64\": \"d\", \"65\": \"e\", \"66\": \"f\", \"67\": \"g\", \"68\": \"h\", \"69\": \"i\", \"70\": \"j\", \"71\": \"k\", \"72\": \"l\", \"73\": \"m\", \"74\": \"n\", \"75\": \"o\", \"76\": \"p\", \"77\": \"q\", \"78\": \"r\", \"79\": \"s\", \"80\": \"t\", \"81\": \"u\", \"82\": \"v\", \"83\": \"w\", \"84\": \"x\", \"85\": \"y\", \"86\": \"z\", \"87\": \"|\", \"88\": \"~\", \"89\": \"\\u00a3\", \"90\": \"\\u00a7\", \"91\": \"\\u00a8\", \"92\": \"\\u00ab\", \"93\": \"\\u00ac\", \"94\": \"\\u00ad\", \"95\": \"\\u00b0\", \"96\": \"\\u00b2\", \"97\": \"\\u00b4\", \"98\": \"\\u00b7\", \"99\": \"\\u00ba\", \"100\": \"\\u00bb\", \"101\": \"\\u00bc\", \"102\": \"\\u00bd\", \"103\": \"\\u00be\", \"104\": \"\\u00c0\", \"105\": \"\\u00c2\", \"106\": \"\\u00c4\", \"107\": \"\\u00c7\", \"108\": \"\\u00c8\", \"109\": \"\\u00c9\", \"110\": \"\\u00ca\", \"111\": \"\\u00d4\", \"112\": \"\\u00d6\", \"113\": \"\\u00dc\", \"114\": \"\\u00df\", \"115\": \"\\u00e0\", \"116\": \"\\u00e1\", \"117\": \"\\u00e2\", \"118\": \"\\u00e4\", \"119\": \"\\u00e6\", \"120\": \"\\u00e7\", \"121\": \"\\u00e8\", \"122\": \"\\u00e9\", \"123\": \"\\u00ea\", \"124\": \"\\u00eb\", \"125\": \"\\u00ec\", \"126\": \"\\u00ee\", \"127\": \"\\u00ef\", \"128\": \"\\u00f1\", \"129\": \"\\u00f2\", \"130\": \"\\u00f3\", \"131\": \"\\u00f4\", \"132\": \"\\u00f6\", \"133\": \"\\u00f8\", \"134\": \"\\u00f9\", \"135\": \"\\u00fa\", \"136\": \"\\u00fb\", \"137\": \"\\u00fc\", \"138\": \"\\u00ff\", \"139\": \"\\u0142\", \"140\": \"\\u0152\", \"141\": \"\\u0153\", \"142\": \"\\u0393\", \"143\": \"\\u0396\", \"144\": \"\\u03a4\", \"145\": \"\\u03ac\", \"146\": \"\\u03ae\", \"147\": \"\\u03b1\", \"148\": \"\\u03b4\", \"149\": \"\\u03b5\", \"150\": \"\\u03b7\", \"151\": \"\\u03b9\", \"152\": \"\\u03ba\", \"153\": \"\\u03bb\", \"154\": \"\\u03bc\", \"155\": \"\\u03bd\", \"156\": \"\\u03be\", \"157\": \"\\u03bf\", \"158\": \"\\u03c0\", \"159\": \"\\u03c1\", \"160\": \"\\u03c4\", \"161\": \"\\u03c5\", \"162\": \"\\u03c7\", \"163\": \"\\u03c8\", \"164\": \"\\u03c9\", \"165\": \"\\u03cc\", \"166\": \"\\u03ce\", \"167\": \"\\u0406\", \"168\": \"\\u2012\", \"169\": \"\\u2013\", \"170\": \"\\u2014\", \"171\": \"\\u2020\", \"172\": \"\\u2021\", \"173\": \"\\u2030\", \"174\": \"\\u2039\", \"175\": \"\\u203a\", \"176\": \"\\u2082\", \"177\": \"\\u20a4\", \"178\": \"\\u2114\", \"179\": \"\\u2153\", \"180\": \"\\u2154\", \"181\": \"\\u2155\", \"182\": \"\\u2156\", \"183\": \"\\u2157\", \"184\": \"\\u2158\", \"185\": \"\\u2159\", \"186\": \"\\u215a\", \"187\": \"\\u215b\", \"188\": \"\\u2206\", \"189\": \"\\u2207\", \"190\": \"\\u222b\", \"191\": \"\\u2260\", \"192\": \"\\u25a1\", \"193\": \"\\u2640\", \"194\": \"\\u2642\", \"195\": \"\\u2713\", \"196\": \"\\uff46\"},\n",
        "            \"char_to_idx\": {\"\\u203a\": 175, \"\\u2014\": 170, \"\\u25a1\": 192, \" \": 1, \"\\u00a3\": 89, \"$\": 5, \"\\u00a7\": 90, \"(\": 9, \"\\u00ab\": 92, \"\\u2206\": 188, \",\": 13, \"\\u03b1\": 147, \"0\": 17, \"\\u03b5\": 149, \"4\": 21, \"\\u00b7\": 98, \"\\u03b9\": 151, \"8\": 25, \"\\u00bb\": 100, \"\\u03bd\": 155, \"\\u03c1\": 159, \"\\u2640\": 193, \"\\u0142\": 139, \"\\u03c5\": 161, \"D\": 34, \"\\u00c7\": 107, \"\\u2260\": 191, \"\\u03c9\": 164, \"H\": 38, \"L\": 42, \"P\": 46, \"\\u0152\": 140, \"T\": 50, \"\\u2156\": 182, \"X\": 54, \"\\u215a\": 186, \"\\u00df\": 114, \"`\": 60, \"d\": 64, \"\\u00e7\": 120, \"h\": 68, \"\\u00eb\": 124, \"l\": 72, \"\\u00ef\": 127, \"p\": 76, \"\\u00f3\": 130, \"t\": 80, \"x\": 84, \"\\u00fb\": 136, \"|\": 87, \"\\u00ff\": 138, \"\\u2207\": 189, \"\\u2153\": 179, \"\\u2013\": 169, \"\\u0396\": 143, \"#\": 4, \"\\u20a4\": 177, \"'\": 8, \"\\u00a8\": 91, \"+\": 12, \"\\u00ac\": 93, \"/\": 16, \"\\u03ae\": 146, \"\\u00b0\": 95, \"3\": 20, \"\\u00b4\": 97, \"7\": 24, \";\": 28, \"\\u03ba\": 152, \"\\u00bc\": 101, \"?\": 30, \"\\u03be\": 156, \"\\u00c0\": 104, \"C\": 33, \"\\u00c4\": 106, \"G\": 37, \"\\u2020\": 171, \"\\u00c8\": 108, \"K\": 41, \"O\": 45, \"\\u03ce\": 166, \"S\": 49, \"\\u2155\": 181, \"\\u00d4\": 111, \"W\": 53, \"\\u2159\": 185, \"[\": 57, \"\\u00dc\": 113, \"_\": 59, \"\\u00e0\": 115, \"c\": 63, \"\\u00e4\": 118, \"g\": 67, \"\\u00e8\": 121, \"k\": 71, \"\\u00ec\": 125, \"o\": 75, \"s\": 79, \"\\u00f4\": 131, \"w\": 83, \"\\u00f8\": 133, \"\\u2021\": 172, \"\\u00fc\": 137, \"\\u2030\": 173, \"\\u0406\": 167, \"\\u0393\": 142, \"\\u2012\": 168, \"\\u2114\": 178, \"\\\"\": 3, \"&\": 7, \"*\": 11, \"\\u00ad\": 94, \".\": 15, \"2\": 19, \"\\u03b7\": 150, \"6\": 23, \"\\u03bb\": 153, \":\": 27, \"\\u00bd\": 102, \"\\u03bf\": 157, \"B\": 32, \"\\u03c7\": 162, \"F\": 36, \"\\u00c9\": 109, \"J\": 40, \"N\": 44, \"R\": 48, \"\\u2154\": 180, \"V\": 52, \"\\u2158\": 184, \"Z\": 56, \"\\u00e1\": 116, \"b\": 62, \"\\u2039\": 174, \"f\": 66, \"\\u00e9\": 122, \"j\": 70, \"n\": 74, \"\\u00f1\": 128, \"r\": 78, \"v\": 82, \"\\u00f9\": 134, \"z\": 86, \"~\": 88, \"\\u2082\": 176, \"\\u2713\": 195, \"\\u2642\": 194, \"!\": 2, \"%\": 6, \"\\u03a4\": 144, \")\": 10, \"\\uff46\": 196, \"-\": 14, \"\\u03ac\": 145, \"1\": 18, \"\\u00b2\": 96, \"5\": 22, \"\\u03b4\": 148, \"9\": 26, \"\\u00ba\": 99, \"=\": 29, \"\\u03bc\": 154, \"\\u00be\": 103, \"A\": 31, \"\\u03c0\": 158, \"\\u00c2\": 105, \"E\": 35, \"\\u03c4\": 160, \"I\": 39, \"\\u03c8\": 163, \"\\u00ca\": 110, \"M\": 43, \"\\u03cc\": 165, \"Q\": 47, \"\\u0153\": 141, \"U\": 51, \"\\u2157\": 183, \"\\u00d6\": 112, \"Y\": 55, \"\\u215b\": 187, \"]\": 58, \"a\": 61, \"\\u00e2\": 117, \"e\": 65, \"\\u00e6\": 119, \"i\": 69, \"\\u00ea\": 123, \"m\": 73, \"\\u00ee\": 126, \"q\": 77, \"\\u00f2\": 129, \"u\": 81, \"\\u00f6\": 132, \"y\": 85, \"\\u00fa\": 135, \"\\u222b\": 190}\n",
        "            }\n",
        "\n",
        "# CHAR_SET = {'char_to_idx': {'Ĵ': '318', '¬': '1', 'Õ': '2', 'Y': '3', 'Į': '4', 'ø': '5', 'Ÿ': '6', ',': '7', '«': '8', 'ĳ': '9', 'e': '10', 'Ô': '11', 'U': '12', '[': '13', 'j': '14', 'Ũ': '15', '3': '16', 'o': '17', 'ï': '18', 'd': '19', 'x': '20', 'ċ': '21', 'Ü': '22', 'ı': '23', 'Ð': '24', 'Ď': '25', 'Ŋ': '26', '2': '27', '®': '28', '9': '29', 'ß': '30', 'ľ': '31', '/': '32', 'V': '33', '½': '34', 'û': '35', 'h': '36', 'ě': '37', 'r': '38', 'm': '39', '¥': '40', 'g': '41', 'ĺ': '42', 'B': '43', 'Ė': '44', 'Ř': '45', 'Ĺ': '46', 'Ò': '47', 'ĥ': '48', 'À': '49', '{': '50', 'Ž': '51', 'ã': '52', ':': '53', 'Ì': '54', 'Ī': '55', 'Ķ': '56', 'ń': '57', 'õ': '58', 'Å': '59', 'G': '60', 'È': '61', 'ſ': '62', 'Ą': '63', '5': '64', 'ë': '65', 'Ō': '66', 'ŋ': '67', 'ţ': '68', 'Ħ': '69', 'Q': '70', 'č': '71', 'Ŀ': '72', '=': '73', 'Ĉ': '74', 'Ş': '75', 'Ū': '76', 'ħ': '77', 'ŗ': '78', 'É': '79', '%': '80', 'ť': '81', 'æ': '82', '±': '83', '?': '84', 'D': '85', '»': '86', 'ż': '87', 'ć': '88', '<': '89', '|': '90', 'C': '91', 'Ġ': '92', '´': '93', 'ŏ': '94', '.': '95', '$': '96', 'ü': '97', '+': '98', 'ġ': '99', 'Ï': '100', 'ŕ': '101', 'Ă': '102', 'i': '103', 'Ý': '104', '\"': '105', 'w': '106', 'Ù': '107', 'Ŝ': '108', 'Đ': '109', 'Ä': '110', 'ì': '111', '`': '112', 'ű': '113', '\\xad': '114', 'ģ': '115', 'î': '116', '7': '117', 'Ö': '118', 'İ': '119', 'ĵ': '120', 'Z': '121', '¶': '122', 'Ņ': '123', '¨': '124', '4': '125', 'R': '126', ']': '127', '^': '128', 'F': '129', 'ļ': '130', 'ğ': '131', 'k': '132', 'ī': '133', 'é': '134', 'ŉ': '135', 'Ń': '136', 'Ľ': '137', '!': '138', 'ù': '139', 'Ĳ': '140', 'S': '141', 'E': '142', 'â': '143', ')': '144', '·': '145', '¾': '146', 'Þ': '147', 'Ł': '148', 'ř': '149', 'Ļ': '150', 'Ê': '151', 'ä': '152', 'n': '153', 'œ': '154', '(': '155', 'ĕ': '156', '§': '157', 'ê': '158', '°': '159', 'ý': '160', '@': '161', 'Ź': '162', '-': '163', 'Ţ': '164', 'ũ': '165', 'ė': '166', '0': '167', 'Ĩ': '168', 'ş': '169', 'š': '170', 'ō': '171', 'ą': '172', 'H': '173', 'ų': '174', 'O': '175', 'ŭ': '176', ' ': '177', 'ñ': '178', 'ś': '179', 'b': '180', '¦': '181', 'Ú': '182', 'Œ': '183', 'ª': '184', 'ĩ': '185', 'W': '186', 'M': '187', 'ă': '188', 'ö': '189', 'ž': '190', 'ò': '191', 'µ': '192', 'f': '193', 'ň': '194', 'þ': '195', '1': '196', 'Ç': '197', 'Ć': '198', '¹': '199', 'Ŗ': '200', 'á': '201', 'c': '202', '>': '203', '8': '204', 'ł': '205', 'Š': '206', 'ő': '207', 'Ģ': '208', 'ŷ': '209', 'Ĕ': '210', 'Ś': '211', 'ŝ': '212', 'ź': '213', 'Â': '214', 'ĭ': '215', '³': '216', 'Ċ': '217', 'Ã': '218', 'į': '219', 'l': '220', 'Û': '221', 'Ĭ': '222', 'Ŧ': '223', 'Ż': '224', 'K': '225', 'N': '226', '¡': '227', '_': '228', 'å': '229', '£': '230', 'ū': '231', 'Ų': '232', '×': '233', 'Ā': '234', 'u': '235', 'ů': '236', 'Ě': '237', '*': '238', 'v': '239', 'T': '240', 'Ŕ': '241', 'ē': '242', 'A': '243', 'X': '244', '¼': '245', 'q': '246', '¤': '247', 's': '248', 'Ű': '249', 't': '250', 'Ŷ': '251', 'Č': '252', 'ĝ': '253', '\\\\': '254', 'Ů': '255', '#': '256', \"'\": '257', 'Á': '258', '¿': '259', '}': '260', 'y': '261', 'Ē': '262', 'Ŭ': '263', 'Ë': '264', '~': '265', 'Ę': '266', 'Ŵ': '267', 'Æ': '268', 'ð': '269', 'º': '270', 'Ó': '271', 'ā': '272', 'ô': '273', 'J': '274', 'ÿ': '275', 'ó': '276', 'Ĝ': '277', '&': '278', 'P': '279', '©': '280', 'Ğ': '281', 'è': '282', 'ę': '283', 'ĸ': '284', '²': '285', 'Ĥ': '286', '¢': '287', 'ŵ': '288', 'Î': '289', 'đ': '290', 'Í': '291', 'a': '292', ';': '293', 'à': '294', '¯': '295', '¸': '296', 'ņ': '297', 'L': '298', 'Ő': '299', 'ķ': '300', 'p': '301', 'Ŏ': '302', 'í': '303', 'ŧ': '304', 'ç': '305', 'Ť': '306', 'ŀ': '307', 'z': '308', 'ď': '309', 'Ň': '310', '6': '311', 'I': '312', '÷': '313', 'ú': '314', 'Ø': '315', 'Ñ': '316', 'ĉ': '317'},\n",
        "            # 'idx_to_char': {'318': 'Ĵ', '1': '¬', '2': 'Õ', '3': 'Y', '4': 'Į', '5': 'ø', '6': 'Ÿ', '7': ',', '8': '«', '9': 'ĳ', '10': 'e', '11': 'Ô', '12': 'U', '13': '[', '14': 'j', '15': 'Ũ', '16': '3', '17': 'o', '18': 'ï', '19': 'd', '20': 'x', '21': 'ċ', '22': 'Ü', '23': 'ı', '24': 'Ð', '25': 'Ď', '26': 'Ŋ', '27': '2', '28': '®', '29': '9', '30': 'ß', '31': 'ľ', '32': '/', '33': 'V', '34': '½', '35': 'û', '36': 'h', '37': 'ě', '38': 'r', '39': 'm', '40': '¥', '41': 'g', '42': 'ĺ', '43': 'B', '44': 'Ė', '45': 'Ř', '46': 'Ĺ', '47': 'Ò', '48': 'ĥ', '49': 'À', '50': '{', '51': 'Ž', '52': 'ã', '53': ':', '54': 'Ì', '55': 'Ī', '56': 'Ķ', '57': 'ń', '58': 'õ', '59': 'Å', '60': 'G', '61': 'È', '62': 'ſ', '63': 'Ą', '64': '5', '65': 'ë', '66': 'Ō', '67': 'ŋ', '68': 'ţ', '69': 'Ħ', '70': 'Q', '71': 'č', '72': 'Ŀ', '73': '=', '74': 'Ĉ', '75': 'Ş', '76': 'Ū', '77': 'ħ', '78': 'ŗ', '79': 'É', '80': '%', '81': 'ť', '82': 'æ', '83': '±', '84': '?', '85': 'D', '86': '»', '87': 'ż', '88': 'ć', '89': '<', '90': '|', '91': 'C', '92': 'Ġ', '93': '´', '94': 'ŏ', '95': '.', '96': '$', '97': 'ü', '98': '+', '99': 'ġ', '100': 'Ï', '101': 'ŕ', '102': 'Ă', '103': 'i', '104': 'Ý', '105': '\"', '106': 'w', '107': 'Ù', '108': 'Ŝ', '109': 'Đ', '110': 'Ä', '111': 'ì', '112': '`', '113': 'ű', '114': '\\xad', '115': 'ģ', '116': 'î', '117': '7', '118': 'Ö', '119': 'İ', '120': 'ĵ', '121': 'Z', '122': '¶', '123': 'Ņ', '124': '¨', '125': '4', '126': 'R', '127': ']', '128': '^', '129': 'F', '130': 'ļ', '131': 'ğ', '132': 'k', '133': 'ī', '134': 'é', '135': 'ŉ', '136': 'Ń', '137': 'Ľ', '138': '!', '139': 'ù', '140': 'Ĳ', '141': 'S', '142': 'E', '143': 'â', '144': ')', '145': '·', '146': '¾', '147': 'Þ', '148': 'Ł', '149': 'ř', '150': 'Ļ', '151': 'Ê', '152': 'ä', '153': 'n', '154': 'œ', '155': '(', '156': 'ĕ', '157': '§', '158': 'ê', '159': '°', '160': 'ý', '161': '@', '162': 'Ź', '163': '-', '164': 'Ţ', '165': 'ũ', '166': 'ė', '167': '0', '168': 'Ĩ', '169': 'ş', '170': 'š', '171': 'ō', '172': 'ą', '173': 'H', '174': 'ų', '175': 'O', '176': 'ŭ', '177': ' ', '178': 'ñ', '179': 'ś', '180': 'b', '181': '¦', '182': 'Ú', '183': 'Œ', '184': 'ª', '185': 'ĩ', '186': 'W', '187': 'M', '188': 'ă', '189': 'ö', '190': 'ž', '191': 'ò', '192': 'µ', '193': 'f', '194': 'ň', '195': 'þ', '196': '1', '197': 'Ç', '198': 'Ć', '199': '¹', '200': 'Ŗ', '201': 'á', '202': 'c', '203': '>', '204': '8', '205': 'ł', '206': 'Š', '207': 'ő', '208': 'Ģ', '209': 'ŷ', '210': 'Ĕ', '211': 'Ś', '212': 'ŝ', '213': 'ź', '214': 'Â', '215': 'ĭ', '216': '³', '217': 'Ċ', '218': 'Ã', '219': 'į', '220': 'l', '221': 'Û', '222': 'Ĭ', '223': 'Ŧ', '224': 'Ż', '225': 'K', '226': 'N', '227': '¡', '228': '_', '229': 'å', '230': '£', '231': 'ū', '232': 'Ų', '233': '×', '234': 'Ā', '235': 'u', '236': 'ů', '237': 'Ě', '238': '*', '239': 'v', '240': 'T', '241': 'Ŕ', '242': 'ē', '243': 'A', '244': 'X', '245': '¼', '246': 'q', '247': '¤', '248': 's', '249': 'Ű', '250': 't', '251': 'Ŷ', '252': 'Č', '253': 'ĝ', '254': '\\\\', '255': 'Ů', '256': '#', '257': \"'\", '258': 'Á', '259': '¿', '260': '}', '261': 'y', '262': 'Ē', '263': 'Ŭ', '264': 'Ë', '265': '~', '266': 'Ę', '267': 'Ŵ', '268': 'Æ', '269': 'ð', '270': 'º', '271': 'Ó', '272': 'ā', '273': 'ô', '274': 'J', '275': 'ÿ', '276': 'ó', '277': 'Ĝ', '278': '&', '279': 'P', '280': '©', '281': 'Ğ', '282': 'è', '283': 'ę', '284': 'ĸ', '285': '²', '286': 'Ĥ', '287': '¢', '288': 'ŵ', '289': 'Î', '290': 'đ', '291': 'Í', '292': 'a', '293': ';', '294': 'à', '295': '¯', '296': '¸', '297': 'ņ', '298': 'L', '299': 'Ő', '300': 'ķ', '301': 'p', '302': 'Ŏ', '303': 'í', '304': 'ŧ', '305': 'ç', '306': 'Ť', '307': 'ŀ', '308': 'z', '309': 'ď', '310': 'Ň', '311': '6', '312': 'I', '313': '÷', '314': 'ú', '315': 'Ø', '316': 'Ñ', '317': 'ĉ'}\n",
        "          #  }\n",
        "\n",
        "class CharsetMapper:\n",
        "    def __init__(self, max_sequence_size=128, blank_character=0):\n",
        "        self.max_sequence_size = max_sequence_size\n",
        "        self.blank_character = blank_character\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_duplicates(idxs):\n",
        "        new_idxs = []\n",
        "\n",
        "        for i in range(len(idxs)):\n",
        "            # Only append if the next character in the sequence is not\n",
        "            # identical to the current character. If we're at the end of\n",
        "            # the sequence, add it.\n",
        "            if i + 1 == len(idxs) or idxs[i] != idxs[i + 1]:\n",
        "                new_idxs.append(idxs[i])\n",
        "\n",
        "        return new_idxs\n",
        "\n",
        "    def idx_to_char(self, idx):\n",
        "        if idx == self.blank_character:\n",
        "            return ''  # Return empty string for the blank character\n",
        "        else:\n",
        "          try:\n",
        "            return CHAR_SET['idx_to_char'][str(int(idx))]\n",
        "          except KeyError:\n",
        "            return ''\n",
        "\n",
        "    def char_to_idx(self, char):\n",
        "        try:\n",
        "          return int(CHAR_SET['char_to_idx'][char])\n",
        "        except KeyError:\n",
        "          return self.blank_character\n",
        "\n",
        "    def str_to_idxs(self, string):\n",
        "        idxs = []\n",
        "\n",
        "        zeros = np.full(self.max_sequence_size, self.blank_character)\n",
        "        for char in string:\n",
        "            idxs.append(self.char_to_idx(char))\n",
        "\n",
        "        # Pad the array to the max sequence size\n",
        "        idxs = np.concatenate((idxs, zeros))[:self.max_sequence_size]\n",
        "\n",
        "        return idxs\n",
        "\n",
        "    def idxs_to_str(self, idxs, remove_duplicates=True):\n",
        "        string = ''\n",
        "\n",
        "        if remove_duplicates:\n",
        "            idxs = CharsetMapper.remove_duplicates(idxs)\n",
        "\n",
        "        for idx in idxs:\n",
        "            string += self.idx_to_char(idx)\n",
        "\n",
        "        return string\n",
        "\n",
        "    def str_to_idxs_batch(self, batch):\n",
        "        idxs = []\n",
        "\n",
        "        for string in batch:\n",
        "            idx = self.str_to_idxs(string)\n",
        "            idxs.append(idx)\n",
        "\n",
        "        return idxs\n",
        "\n",
        "    def idxs_to_str_batch(self, batch, remove_duplicates=True):\n",
        "        strings = []\n",
        "\n",
        "        for idxs in batch:\n",
        "            strings.append(self.idxs_to_str(idxs, remove_duplicates=remove_duplicates))\n",
        "\n",
        "        return strings\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "      return len(CHAR_SET['char_to_idx']) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6e0fEdTzQ6w",
        "colab_type": "text"
      },
      "source": [
        "### Data Loading\n",
        "* Keras Sequence\n",
        "* TfRecord Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xDztzvILNTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ErrorSequence(tf.keras.utils.Sequence):\n",
        "  def __init__(self, path='/content/error.csv'):\n",
        "    self.df = pd.read_csv(path, header=None, sep='\\t', names=['original', 'error'])\n",
        "    self.charset_mapper = CharsetMapper()\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    x = self.charset_mapper.str_to_idxs(str(self.df['error'][index]))\n",
        "    y = self.charset_mapper.str_to_idxs(str(self.df['original'][index]))\n",
        "\n",
        "    return tf.constant(x), tf.constant(y)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UajArLvbMGdm",
        "colab_type": "code",
        "outputId": "d5b69ed4-9bc6-4967-deff-1b85a7db5eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sequence = ErrorSequence()\n",
        "mapper = CharsetMapper()\n",
        "x, y = sequence[4]\n",
        "\n",
        "print('error:', mapper.idxs_to_str(x.numpy()))\n",
        "print('corrected:', mapper.idxs_to_str(y.numpy()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error: EXste alf⅓arábio, n¨o o devo ao meu velhÀ cronista do Paseioy PúbJico. É, como se dise no\n",
            "corrected: Este alfarábio, no o devo ao meu velho cronista do Paseio Público. É, como se dise no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKaK9cFCbad",
        "colab_type": "text"
      },
      "source": [
        "Create the TfRecord Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09nAjJIZxauR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tfrecord_from_sequence(sequence, tfrecord_path):\n",
        "    \"\"\"\n",
        "    Create a TfRecord dataset from a sequence\n",
        "\n",
        "    :param sequence: The Keras sequence to load dataasdfs of arbitrary format\n",
        "    :param tfrecord_path: Filepath and name for location of TfRecord dataset\n",
        "    \"\"\"\n",
        "    print('Started creating TFRecord Dataset...')\n",
        "\n",
        "    writer = tf.io.TFRecordWriter(tfrecord_path)\n",
        "\n",
        "    for index, (img, label) in enumerate(sequence):\n",
        "        feature = {'label': _bytes_feature(tf.io.serialize_tensor(label)),\n",
        "                   'image': _bytes_feature(tf.io.serialize_tensor(img))}\n",
        "\n",
        "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "        writer.write(example.SerializeToString())\n",
        "        if index % 1000 == 0:\n",
        "            print(str(index) + '/' + str(len(sequence)))\n",
        "\n",
        "    print(str(len(sequence)) + '/' + str(len(sequence)))\n",
        "\n",
        "    print('Finished: TFRecord created at', tfrecord_path)\n",
        "\n",
        "\n",
        "def read_tfrecord(single_record):\n",
        "    \"\"\"\n",
        "    Function to decode a TfRecord. Usually this function will be called within\n",
        "    a TfDataset map function. Note that out_types for image and label must be\n",
        "    tf.float32 and tf.int64 respectively.\n",
        "\n",
        "    :param single_record: A single TfRecord\n",
        "    :return: A decoded image and label as tensors\n",
        "    \"\"\"\n",
        "    feature_description = {\n",
        "        'label': tf.io.FixedLenFeature((), tf.string),\n",
        "        'image': tf.io.FixedLenFeature((), tf.string)\n",
        "    }\n",
        "\n",
        "    single_record = tf.io.parse_single_example(single_record, feature_description)\n",
        "\n",
        "    image = tf.io.parse_tensor(single_record['image'], out_type=tf.int64)\n",
        "    label = tf.io.parse_tensor(single_record['label'], out_type=tf.int64)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "\n",
        "def _float_feature(value):\n",
        "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "\n",
        "def _int64_feature(value):\n",
        "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H34ngQ6xcVw",
        "colab_type": "code",
        "outputId": "f33ff8a6-afb6-4d18-a362-80256311c679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "create_tfrecord_from_sequence(ErrorSequence(), 'error.tfrecord')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started creating TFRecord Dataset...\n",
            "0/341304\n",
            "1000/341304\n",
            "2000/341304\n",
            "3000/341304\n",
            "4000/341304\n",
            "5000/341304\n",
            "6000/341304\n",
            "7000/341304\n",
            "8000/341304\n",
            "9000/341304\n",
            "10000/341304\n",
            "11000/341304\n",
            "12000/341304\n",
            "13000/341304\n",
            "14000/341304\n",
            "15000/341304\n",
            "16000/341304\n",
            "17000/341304\n",
            "18000/341304\n",
            "19000/341304\n",
            "20000/341304\n",
            "21000/341304\n",
            "22000/341304\n",
            "23000/341304\n",
            "24000/341304\n",
            "25000/341304\n",
            "26000/341304\n",
            "27000/341304\n",
            "28000/341304\n",
            "29000/341304\n",
            "30000/341304\n",
            "31000/341304\n",
            "32000/341304\n",
            "33000/341304\n",
            "34000/341304\n",
            "35000/341304\n",
            "36000/341304\n",
            "37000/341304\n",
            "38000/341304\n",
            "39000/341304\n",
            "40000/341304\n",
            "41000/341304\n",
            "42000/341304\n",
            "43000/341304\n",
            "44000/341304\n",
            "45000/341304\n",
            "46000/341304\n",
            "47000/341304\n",
            "48000/341304\n",
            "49000/341304\n",
            "50000/341304\n",
            "51000/341304\n",
            "52000/341304\n",
            "53000/341304\n",
            "54000/341304\n",
            "55000/341304\n",
            "56000/341304\n",
            "57000/341304\n",
            "58000/341304\n",
            "59000/341304\n",
            "60000/341304\n",
            "61000/341304\n",
            "62000/341304\n",
            "63000/341304\n",
            "64000/341304\n",
            "65000/341304\n",
            "66000/341304\n",
            "67000/341304\n",
            "68000/341304\n",
            "69000/341304\n",
            "70000/341304\n",
            "71000/341304\n",
            "72000/341304\n",
            "73000/341304\n",
            "74000/341304\n",
            "75000/341304\n",
            "76000/341304\n",
            "77000/341304\n",
            "78000/341304\n",
            "79000/341304\n",
            "80000/341304\n",
            "81000/341304\n",
            "82000/341304\n",
            "83000/341304\n",
            "84000/341304\n",
            "85000/341304\n",
            "86000/341304\n",
            "87000/341304\n",
            "88000/341304\n",
            "89000/341304\n",
            "90000/341304\n",
            "91000/341304\n",
            "92000/341304\n",
            "93000/341304\n",
            "94000/341304\n",
            "95000/341304\n",
            "96000/341304\n",
            "97000/341304\n",
            "98000/341304\n",
            "99000/341304\n",
            "100000/341304\n",
            "101000/341304\n",
            "102000/341304\n",
            "103000/341304\n",
            "104000/341304\n",
            "105000/341304\n",
            "106000/341304\n",
            "107000/341304\n",
            "108000/341304\n",
            "109000/341304\n",
            "110000/341304\n",
            "111000/341304\n",
            "112000/341304\n",
            "113000/341304\n",
            "114000/341304\n",
            "115000/341304\n",
            "116000/341304\n",
            "117000/341304\n",
            "118000/341304\n",
            "119000/341304\n",
            "120000/341304\n",
            "121000/341304\n",
            "122000/341304\n",
            "123000/341304\n",
            "124000/341304\n",
            "125000/341304\n",
            "126000/341304\n",
            "127000/341304\n",
            "128000/341304\n",
            "129000/341304\n",
            "130000/341304\n",
            "131000/341304\n",
            "132000/341304\n",
            "133000/341304\n",
            "134000/341304\n",
            "135000/341304\n",
            "136000/341304\n",
            "137000/341304\n",
            "138000/341304\n",
            "139000/341304\n",
            "140000/341304\n",
            "141000/341304\n",
            "142000/341304\n",
            "143000/341304\n",
            "144000/341304\n",
            "145000/341304\n",
            "146000/341304\n",
            "147000/341304\n",
            "148000/341304\n",
            "149000/341304\n",
            "150000/341304\n",
            "151000/341304\n",
            "152000/341304\n",
            "153000/341304\n",
            "154000/341304\n",
            "155000/341304\n",
            "156000/341304\n",
            "157000/341304\n",
            "158000/341304\n",
            "159000/341304\n",
            "160000/341304\n",
            "161000/341304\n",
            "162000/341304\n",
            "163000/341304\n",
            "164000/341304\n",
            "165000/341304\n",
            "166000/341304\n",
            "167000/341304\n",
            "168000/341304\n",
            "169000/341304\n",
            "170000/341304\n",
            "171000/341304\n",
            "172000/341304\n",
            "173000/341304\n",
            "174000/341304\n",
            "175000/341304\n",
            "176000/341304\n",
            "177000/341304\n",
            "178000/341304\n",
            "179000/341304\n",
            "180000/341304\n",
            "181000/341304\n",
            "182000/341304\n",
            "183000/341304\n",
            "184000/341304\n",
            "185000/341304\n",
            "186000/341304\n",
            "187000/341304\n",
            "188000/341304\n",
            "189000/341304\n",
            "190000/341304\n",
            "191000/341304\n",
            "192000/341304\n",
            "193000/341304\n",
            "194000/341304\n",
            "195000/341304\n",
            "196000/341304\n",
            "197000/341304\n",
            "198000/341304\n",
            "199000/341304\n",
            "200000/341304\n",
            "201000/341304\n",
            "202000/341304\n",
            "203000/341304\n",
            "204000/341304\n",
            "205000/341304\n",
            "206000/341304\n",
            "207000/341304\n",
            "208000/341304\n",
            "209000/341304\n",
            "210000/341304\n",
            "211000/341304\n",
            "212000/341304\n",
            "213000/341304\n",
            "214000/341304\n",
            "215000/341304\n",
            "216000/341304\n",
            "217000/341304\n",
            "218000/341304\n",
            "219000/341304\n",
            "220000/341304\n",
            "221000/341304\n",
            "222000/341304\n",
            "223000/341304\n",
            "224000/341304\n",
            "225000/341304\n",
            "226000/341304\n",
            "227000/341304\n",
            "228000/341304\n",
            "229000/341304\n",
            "230000/341304\n",
            "231000/341304\n",
            "232000/341304\n",
            "233000/341304\n",
            "234000/341304\n",
            "235000/341304\n",
            "236000/341304\n",
            "237000/341304\n",
            "238000/341304\n",
            "239000/341304\n",
            "240000/341304\n",
            "241000/341304\n",
            "242000/341304\n",
            "243000/341304\n",
            "244000/341304\n",
            "245000/341304\n",
            "246000/341304\n",
            "247000/341304\n",
            "248000/341304\n",
            "249000/341304\n",
            "250000/341304\n",
            "251000/341304\n",
            "252000/341304\n",
            "253000/341304\n",
            "254000/341304\n",
            "255000/341304\n",
            "256000/341304\n",
            "257000/341304\n",
            "258000/341304\n",
            "259000/341304\n",
            "260000/341304\n",
            "261000/341304\n",
            "262000/341304\n",
            "263000/341304\n",
            "264000/341304\n",
            "265000/341304\n",
            "266000/341304\n",
            "267000/341304\n",
            "268000/341304\n",
            "269000/341304\n",
            "270000/341304\n",
            "271000/341304\n",
            "272000/341304\n",
            "273000/341304\n",
            "274000/341304\n",
            "275000/341304\n",
            "276000/341304\n",
            "277000/341304\n",
            "278000/341304\n",
            "279000/341304\n",
            "280000/341304\n",
            "281000/341304\n",
            "282000/341304\n",
            "283000/341304\n",
            "284000/341304\n",
            "285000/341304\n",
            "286000/341304\n",
            "287000/341304\n",
            "288000/341304\n",
            "289000/341304\n",
            "290000/341304\n",
            "291000/341304\n",
            "292000/341304\n",
            "293000/341304\n",
            "294000/341304\n",
            "295000/341304\n",
            "296000/341304\n",
            "297000/341304\n",
            "298000/341304\n",
            "299000/341304\n",
            "300000/341304\n",
            "301000/341304\n",
            "302000/341304\n",
            "303000/341304\n",
            "304000/341304\n",
            "305000/341304\n",
            "306000/341304\n",
            "307000/341304\n",
            "308000/341304\n",
            "309000/341304\n",
            "310000/341304\n",
            "311000/341304\n",
            "312000/341304\n",
            "313000/341304\n",
            "314000/341304\n",
            "315000/341304\n",
            "316000/341304\n",
            "317000/341304\n",
            "318000/341304\n",
            "319000/341304\n",
            "320000/341304\n",
            "321000/341304\n",
            "322000/341304\n",
            "323000/341304\n",
            "324000/341304\n",
            "325000/341304\n",
            "326000/341304\n",
            "327000/341304\n",
            "328000/341304\n",
            "329000/341304\n",
            "330000/341304\n",
            "331000/341304\n",
            "332000/341304\n",
            "333000/341304\n",
            "334000/341304\n",
            "335000/341304\n",
            "336000/341304\n",
            "337000/341304\n",
            "338000/341304\n",
            "339000/341304\n",
            "340000/341304\n",
            "341000/341304\n",
            "341304/341304\n",
            "Finished: TFRecord created at error.tfrecord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhw-p6Bax7oK",
        "colab_type": "code",
        "outputId": "3dffbe1e-9b1a-4ded-ac4c-7268004b2275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset = tf.data.TFRecordDataset('error.tfrecord').take(30000).map(read_tfrecord)\n",
        "\n",
        "for image_features in dataset.take(1):\n",
        "  print('Error:', mapper.idxs_to_str(image_features[0]))\n",
        "  print('Corrected:', mapper.idxs_to_str(image_features[1]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: A ALMAΤDO LZARO\n",
            "Corrected: A ALMA DO LZARO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6m0wPugFnqd",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlYaTrQKPaPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dNIua4_FqPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, units, embedding_dim, vocab_size, sequence_length=128):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units, return_sequences=True))\n",
        "    self.attention = BahdanauAttention(units)\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.softmax = tf.keras.layers.Softmax(axis=2)\n",
        "\n",
        "  def call(self, enc_out, enc_state1, enc_state2):\n",
        "    batch_size = enc_out.shape[0]\n",
        "\n",
        "    enc_state = tf.concat((enc_state1, enc_state2), axis=1)\n",
        "    \n",
        "    att_out, _ = self.attention(enc_state, enc_out)\n",
        "    att_out = tf.reshape(att_out, (batch_size, self.sequence_length, -1))\n",
        "\n",
        "    out = self.gru(att_out, initial_state=[enc_state1, enc_state2])\n",
        "    out = self.fc(out)\n",
        "    out = self.softmax(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, units, embedding_dim, vocab_size):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units, return_sequences=True, return_state=True))\n",
        "  \n",
        "  def call(self, x):\n",
        "    out = self.embedding(x)\n",
        "    out, state1, state2 = self.gru(out)\n",
        "    return out, state1, state2\n",
        "\n",
        "class CorrectionModel(tf.keras.Model):\n",
        "  def __init__(self, units, embedding_dim, vocab_size, sequence_length=128):\n",
        "    super(CorrectionModel, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(units, embedding_dim, vocab_size)\n",
        "    self.decoder = Decoder(units, embedding_dim, vocab_size, sequence_length=sequence_length)\n",
        "  \n",
        "  def call(self, x):\n",
        "    enc_out, enc_state1, enc_state2 = self.encoder(x)\n",
        "    out = self.decoder(enc_out, enc_state1, enc_state2)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4rz3wa5GtGN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1c5ac678-cbf1-4366-c36a-b4c5dce98d7b"
      },
      "source": [
        "mapper = CharsetMapper()\n",
        "inp, trg = ErrorSequence()[0]\n",
        "\n",
        "enc = Encoder(1024, 512, mapper.get_vocab_size())\n",
        "dec = Decoder(1024, 512, mapper.get_vocab_size())\n",
        "\n",
        "enc_out, enc_state1, enc_state2 = enc(tf.expand_dims(inp, 0))\n",
        "print(enc_out.shape)\n",
        "print(enc_state1.shape)\n",
        "print(enc_state2.shape)\n",
        "\n",
        "dec_out = dec(enc_out, enc_state1, enc_state2)\n",
        "print(mapper.idxs_to_str_batch(tf.argmax(dec_out, axis=2)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 128, 2048)\n",
            "(1, 1024)\n",
            "(1, 1024)\n",
            "['ZFμFｆ℔ｆì[cûłά.ôSôc⅘⅛â⅗ôł=Іr£§TÔ0℔$jK:SÂ=ι—□—⅖Kiéιc⅗□ôiÔë8|τSüρôcì⅛⅘℔⅘rｆîψü§=ï·sÄ3ô℔«`á⅖‰λÔeôøÖ⅖êÖ⅚C7Ô&8k¨Q✓S08ł✓p□:%Nf']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B0BorgbIV2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Train:\n",
        "  def __init__(self):\n",
        "    self.epochs = 100\n",
        "    self.batch_size = 500\n",
        "\n",
        "    units = 256\n",
        "    embedding_dim = 256\n",
        "    sequence_length = 128\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset('error.tfrecord').take(10_000).map(read_tfrecord).shuffle(10000)\n",
        "    dataset_size = 10_000 #len(ErrorSequence())\n",
        "    self.train_size = int(.8 * dataset_size)\n",
        "    self.val_size = dataset_size - self.train_size\n",
        "\n",
        "    self.train_dataset = dataset.take(self.train_size).batch(self.batch_size)\n",
        "    self.val_dataset = dataset.skip(self.train_size).batch(self.batch_size)\n",
        "\n",
        "    self.model = CorrectionModel(units, embedding_dim, mapper.get_vocab_size(), sequence_length=sequence_length) \n",
        "    self.optimizer = tf.keras.optimizers.Adam()\n",
        "    self.objective = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    self.val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, inp, trg):\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = self.model(inp)\n",
        "      loss = self.objective(trg, preds)\n",
        "    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "    self.train_loss(loss)\n",
        "  \n",
        "  @tf.function\n",
        "  def val_step(self, inp, trg):\n",
        "    preds = self.model(inp)\n",
        "    loss = self.objective(trg, preds)\n",
        "    self.val_loss(loss)\n",
        "  \n",
        "  def __call__(self):\n",
        "    try:\n",
        "      train_losses, val_losses = [], []\n",
        "\n",
        "      for epoch in range(self.epochs):\n",
        "        self.train_loss.reset_states()\n",
        "        self.val_loss.reset_states()\n",
        "\n",
        "        # Train Loop\n",
        "        train_loop = tqdm(total=self.train_size//self.batch_size, position=0, leave=True)\n",
        "        for inp, trg in self.train_dataset:\n",
        "          self.train_step(inp, trg)\n",
        "          train_loop.set_description('Train - Epoch: {}, Loss: {:.4f}'.format(epoch, self.train_loss.result()))\n",
        "          train_loop.update(1)\n",
        "        train_loop.close()\n",
        "\n",
        "        # Val Loop\n",
        "        val_loop = tqdm(total=self.val_size//self.batch_size, position=0, leave=True)\n",
        "        for inp, trg in self.val_dataset:\n",
        "          self.val_step(inp, trg)\n",
        "          val_loop.set_description('Val   - Epoch: {}, Loss: {:.4f}'.format(epoch, self.val_loss.result()))\n",
        "          val_loop.update(1)\n",
        "        val_loop.close()\n",
        "\n",
        "        train_losses.append(self.train_loss.result().numpy())\n",
        "        val_losses.append(self.val_loss.result().numpy())\n",
        "      \n",
        "    except Exception as e:\n",
        "      print('Exception caught during training: {0}'.format(e))\n",
        "    finally:\n",
        "      return self.model, (train_losses, val_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLi86dYbNi-Z",
        "colab_type": "code",
        "outputId": "77157623-b2ef-4ed4-bbcc-9cdd6bb0e656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train = Train()\n",
        "model, losses = train()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train - Epoch: 0, Loss: 4.0535: 100%|██████████| 16/16 [00:21<00:00,  1.32s/it]\n",
            "Val   - Epoch: 0, Loss: 2.3396: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]\n",
            "Train - Epoch: 1, Loss: 2.0239: 100%|██████████| 16/16 [00:17<00:00,  1.09s/it]\n",
            "Val   - Epoch: 1, Loss: 1.8831: 100%|██████████| 4/4 [00:02<00:00,  1.76it/s]\n",
            "Train - Epoch: 2, Loss: 1.8036: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
            "Val   - Epoch: 2, Loss: 1.7598: 100%|██████████| 4/4 [00:02<00:00,  1.48it/s]\n",
            "Train - Epoch: 3, Loss: 1.7084: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 3, Loss: 1.6360: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]\n",
            "Train - Epoch: 4, Loss: 1.5666: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
            "Val   - Epoch: 4, Loss: 1.4936: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]\n",
            "Train - Epoch: 5, Loss: 1.4586: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 5, Loss: 1.4118: 100%|██████████| 4/4 [00:02<00:00,  1.82it/s]\n",
            "Train - Epoch: 6, Loss: 1.4122: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 6, Loss: 1.3822: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s]\n",
            "Train - Epoch: 7, Loss: 1.3908: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 7, Loss: 1.3927: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]\n",
            "Train - Epoch: 8, Loss: 1.3585: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 8, Loss: 1.3706: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s]\n",
            "Train - Epoch: 9, Loss: 1.3524: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 9, Loss: 1.3305: 100%|██████████| 4/4 [00:02<00:00,  1.75it/s]\n",
            "Train - Epoch: 10, Loss: 1.3382: 100%|██████████| 16/16 [00:17<00:00,  1.08s/it]\n",
            "Val   - Epoch: 10, Loss: 1.3433: 100%|██████████| 4/4 [00:02<00:00,  1.76it/s]\n",
            "Train - Epoch: 11, Loss: 1.3505: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 11, Loss: 1.3462: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s]\n",
            "Train - Epoch: 12, Loss: 1.3359: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
            "Val   - Epoch: 12, Loss: 1.3310: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]\n",
            "Train - Epoch: 13, Loss: 1.3454: 100%|██████████| 16/16 [00:18<00:00,  1.13s/it]\n",
            "Val   - Epoch: 13, Loss: 1.3491: 100%|██████████| 4/4 [00:02<00:00,  1.75it/s]\n",
            "Train - Epoch: 14, Loss: 1.3371: 100%|██████████| 16/16 [00:18<00:00,  1.18s/it]\n",
            "Val   - Epoch: 14, Loss: 1.3192: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]\n",
            "Train - Epoch: 15, Loss: 1.3319: 100%|██████████| 16/16 [00:18<00:00,  1.13s/it]\n",
            "Val   - Epoch: 15, Loss: 1.3488: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n",
            "Train - Epoch: 16, Loss: 1.3257: 100%|██████████| 16/16 [00:17<00:00,  1.08s/it]\n",
            "Val   - Epoch: 16, Loss: 1.3258: 100%|██████████| 4/4 [00:02<00:00,  1.78it/s]\n",
            "Train - Epoch: 17, Loss: 1.3290: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 17, Loss: 1.3437: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]\n",
            "Train - Epoch: 18, Loss: 1.3289: 100%|██████████| 16/16 [00:17<00:00,  1.08s/it]\n",
            "Val   - Epoch: 18, Loss: 1.3560: 100%|██████████| 4/4 [00:02<00:00,  1.49it/s]\n",
            "Train - Epoch: 19, Loss: 1.3278: 100%|██████████| 16/16 [00:17<00:00,  1.09s/it]\n",
            "Val   - Epoch: 19, Loss: 1.3156: 100%|██████████| 4/4 [00:02<00:00,  1.51it/s]\n",
            "Train - Epoch: 20, Loss: 1.3355: 100%|██████████| 16/16 [00:17<00:00,  1.06s/it]\n",
            "Val   - Epoch: 20, Loss: 1.3304: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s]\n",
            "Train - Epoch: 21, Loss: 1.3276: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 21, Loss: 1.3014: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n",
            "Train - Epoch: 22, Loss: 1.3231: 100%|██████████| 16/16 [00:18<00:00,  1.13s/it]\n",
            "Val   - Epoch: 22, Loss: 1.3099: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]\n",
            "Train - Epoch: 23, Loss: 1.3310: 100%|██████████| 16/16 [00:17<00:00,  1.10s/it]\n",
            "Val   - Epoch: 23, Loss: 1.3029: 100%|██████████| 4/4 [00:02<00:00,  1.76it/s]\n",
            "Train - Epoch: 24, Loss: 1.3261: 100%|██████████| 16/16 [00:18<00:00,  1.13s/it]\n",
            "Val   - Epoch: 24, Loss: 1.3331: 100%|██████████| 4/4 [00:02<00:00,  1.51it/s]\n",
            "Train - Epoch: 25, Loss: 1.3255: 100%|██████████| 16/16 [00:17<00:00,  1.09s/it]\n",
            "Val   - Epoch: 25, Loss: 1.3475: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s]\n",
            "Train - Epoch: 26, Loss: 1.3254: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 26, Loss: 1.3395: 100%|██████████| 4/4 [00:02<00:00,  1.52it/s]\n",
            "Train - Epoch: 27, Loss: 1.3283: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 27, Loss: 1.3363: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n",
            "Train - Epoch: 28, Loss: 1.3295: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 28, Loss: 1.3396: 100%|██████████| 4/4 [00:02<00:00,  1.59it/s]\n",
            "Train - Epoch: 29, Loss: 1.3249: 100%|██████████| 16/16 [00:17<00:00,  1.06s/it]\n",
            "Val   - Epoch: 29, Loss: 1.3106: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]\n",
            "Train - Epoch: 30, Loss: 1.3266: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 30, Loss: 1.3185: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]\n",
            "Train - Epoch: 31, Loss: 1.3219: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
            "Val   - Epoch: 31, Loss: 1.3208: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]\n",
            "Train - Epoch: 32, Loss: 1.3232: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
            "Val   - Epoch: 32, Loss: 1.3079: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s]\n",
            "Train - Epoch: 33, Loss: 1.3151: 100%|██████████| 16/16 [00:17<00:00,  1.10s/it]\n",
            "Val   - Epoch: 33, Loss: 1.3106: 100%|██████████| 4/4 [00:02<00:00,  1.49it/s]\n",
            "Train - Epoch: 34, Loss: 1.3297: 100%|██████████| 16/16 [00:18<00:00,  1.13s/it]\n",
            "Val   - Epoch: 34, Loss: 1.3336: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]\n",
            "Train - Epoch: 35, Loss: 1.3308: 100%|██████████| 16/16 [00:17<00:00,  1.09s/it]\n",
            "Val   - Epoch: 35, Loss: 1.3292: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]\n",
            "Train - Epoch: 36, Loss: 1.3260: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 36, Loss: 1.3143: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n",
            "Train - Epoch: 37, Loss: 1.3264: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 37, Loss: 1.3423: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]\n",
            "Train - Epoch: 38, Loss: 1.3255: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 38, Loss: 1.3257: 100%|██████████| 4/4 [00:02<00:00,  1.51it/s]\n",
            "Train - Epoch: 39, Loss: 1.3211: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 39, Loss: 1.3150: 100%|██████████| 4/4 [00:02<00:00,  1.49it/s]\n",
            "Train - Epoch: 40, Loss: 1.3222: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 40, Loss: 1.3394: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]\n",
            "Train - Epoch: 41, Loss: 1.3160: 100%|██████████| 16/16 [00:18<00:00,  1.15s/it]\n",
            "Val   - Epoch: 41, Loss: 1.3268: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]\n",
            "Train - Epoch: 42, Loss: 1.3215: 100%|██████████| 16/16 [00:18<00:00,  1.15s/it]\n",
            "Val   - Epoch: 42, Loss: 1.3428: 100%|██████████| 4/4 [00:02<00:00,  1.88it/s]\n",
            "Train - Epoch: 43, Loss: 1.3231: 100%|██████████| 16/16 [00:18<00:00,  1.13s/it]\n",
            "Val   - Epoch: 43, Loss: 1.3646: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]\n",
            "Train - Epoch: 44, Loss: 1.3249: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
            "Val   - Epoch: 44, Loss: 1.3226: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]\n",
            "Train - Epoch: 45, Loss: 1.3160: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 45, Loss: 1.3350: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s]\n",
            "Train - Epoch: 46, Loss: 1.3265: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 46, Loss: 1.2812: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]\n",
            "Train - Epoch: 47, Loss: 1.3158: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 47, Loss: 1.3055: 100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n",
            "Train - Epoch: 48, Loss: 1.3177: 100%|██████████| 16/16 [00:16<00:00,  1.05s/it]\n",
            "Val   - Epoch: 48, Loss: 1.3063: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s]\n",
            "Train - Epoch: 49, Loss: 1.3170: 100%|██████████| 16/16 [00:16<00:00,  1.05s/it]\n",
            "Val   - Epoch: 49, Loss: 1.3339: 100%|██████████| 4/4 [00:02<00:00,  1.67it/s]\n",
            "Train - Epoch: 50, Loss: 1.3208: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 50, Loss: 1.3060: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]\n",
            "Train - Epoch: 51, Loss: 1.3145: 100%|██████████| 16/16 [00:18<00:00,  1.16s/it]\n",
            "Val   - Epoch: 51, Loss: 1.3259: 100%|██████████| 4/4 [00:02<00:00,  1.46it/s]\n",
            "Train - Epoch: 52, Loss: 1.3174: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 52, Loss: 1.3041: 100%|██████████| 4/4 [00:02<00:00,  1.76it/s]\n",
            "Train - Epoch: 53, Loss: 1.3196: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 53, Loss: 1.3141: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n",
            "Train - Epoch: 54, Loss: 1.3137: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 54, Loss: 1.3193: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n",
            "Train - Epoch: 55, Loss: 1.3083: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 55, Loss: 1.3152: 100%|██████████| 4/4 [00:02<00:00,  1.76it/s]\n",
            "Train - Epoch: 56, Loss: 1.3186: 100%|██████████| 16/16 [00:16<00:00,  1.05s/it]\n",
            "Val   - Epoch: 56, Loss: 1.3235: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]\n",
            "Train - Epoch: 57, Loss: 1.3154: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 57, Loss: 1.3124: 100%|██████████| 4/4 [00:02<00:00,  1.42it/s]\n",
            "Train - Epoch: 58, Loss: 1.3216: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 58, Loss: 1.3229: 100%|██████████| 4/4 [00:02<00:00,  1.72it/s]\n",
            "Train - Epoch: 59, Loss: 1.3181: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 59, Loss: 1.3229: 100%|██████████| 4/4 [00:02<00:00,  1.76it/s]\n",
            "Train - Epoch: 60, Loss: 1.3179: 100%|██████████| 16/16 [00:17<00:00,  1.08s/it]\n",
            "Val   - Epoch: 60, Loss: 1.3056: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]\n",
            "Train - Epoch: 61, Loss: 1.3168: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 61, Loss: 1.3064: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n",
            "Train - Epoch: 62, Loss: 1.3200: 100%|██████████| 16/16 [00:17<00:00,  1.09s/it]\n",
            "Val   - Epoch: 62, Loss: 1.2914: 100%|██████████| 4/4 [00:02<00:00,  1.54it/s]\n",
            "Train - Epoch: 63, Loss: 1.3181: 100%|██████████| 16/16 [00:18<00:00,  1.15s/it]\n",
            "Val   - Epoch: 63, Loss: 1.3383: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]\n",
            "Train - Epoch: 64, Loss: 1.3179: 100%|██████████| 16/16 [00:17<00:00,  1.09s/it]\n",
            "Val   - Epoch: 64, Loss: 1.3240: 100%|██████████| 4/4 [00:02<00:00,  1.69it/s]\n",
            "Train - Epoch: 65, Loss: 1.3123: 100%|██████████| 16/16 [00:18<00:00,  1.15s/it]\n",
            "Val   - Epoch: 65, Loss: 1.3060: 100%|██████████| 4/4 [00:02<00:00,  1.61it/s]\n",
            "Train - Epoch: 66, Loss: 1.3168: 100%|██████████| 16/16 [00:17<00:00,  1.10s/it]\n",
            "Val   - Epoch: 66, Loss: 1.3147: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]\n",
            "Train - Epoch: 67, Loss: 1.3233: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 67, Loss: 1.2953: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n",
            "Train - Epoch: 68, Loss: 1.3093: 100%|██████████| 16/16 [00:17<00:00,  1.10s/it]\n",
            "Val   - Epoch: 68, Loss: 1.2987: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]\n",
            "Train - Epoch: 69, Loss: 1.3190: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 69, Loss: 1.3356: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n",
            "Train - Epoch: 70, Loss: 1.3216: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 70, Loss: 1.3171: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n",
            "Train - Epoch: 71, Loss: 1.3133: 100%|██████████| 16/16 [00:17<00:00,  1.06s/it]\n",
            "Val   - Epoch: 71, Loss: 1.2945: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n",
            "Train - Epoch: 72, Loss: 1.3049: 100%|██████████| 16/16 [00:16<00:00,  1.05s/it]\n",
            "Val   - Epoch: 72, Loss: 1.2998: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]\n",
            "Train - Epoch: 73, Loss: 1.3141: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 73, Loss: 1.2928: 100%|██████████| 4/4 [00:02<00:00,  1.77it/s]\n",
            "Train - Epoch: 74, Loss: 1.3126: 100%|██████████| 16/16 [00:18<00:00,  1.14s/it]\n",
            "Val   - Epoch: 74, Loss: 1.2892: 100%|██████████| 4/4 [00:02<00:00,  1.52it/s]\n",
            "Train - Epoch: 75, Loss: 1.3121: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 75, Loss: 1.3046: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]\n",
            "Train - Epoch: 76, Loss: 1.3075: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 76, Loss: 1.3202: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n",
            "Train - Epoch: 77, Loss: 1.3150: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 77, Loss: 1.3150: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n",
            "Train - Epoch: 78, Loss: 1.3001: 100%|██████████| 16/16 [00:17<00:00,  1.10s/it]\n",
            "Val   - Epoch: 78, Loss: 1.2879: 100%|██████████| 4/4 [00:02<00:00,  1.72it/s]\n",
            "Train - Epoch: 79, Loss: 1.3076: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 79, Loss: 1.2879: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n",
            "Train - Epoch: 80, Loss: 1.3038: 100%|██████████| 16/16 [00:16<00:00,  1.05s/it]\n",
            "Val   - Epoch: 80, Loss: 1.2731: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]\n",
            "Train - Epoch: 81, Loss: 1.2980: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 81, Loss: 1.3189: 100%|██████████| 4/4 [00:02<00:00,  1.74it/s]\n",
            "Train - Epoch: 82, Loss: 1.2942: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 82, Loss: 1.3109: 100%|██████████| 4/4 [00:02<00:00,  1.77it/s]\n",
            "Train - Epoch: 83, Loss: 1.2974: 100%|██████████| 16/16 [00:17<00:00,  1.09s/it]\n",
            "Val   - Epoch: 83, Loss: 1.2787: 100%|██████████| 4/4 [00:02<00:00,  1.78it/s]\n",
            "Train - Epoch: 84, Loss: 1.3071: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 84, Loss: 1.3033: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]\n",
            "Train - Epoch: 85, Loss: 1.2927: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
            "Val   - Epoch: 85, Loss: 1.2862: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n",
            "Train - Epoch: 86, Loss: 1.3014: 100%|██████████| 16/16 [00:17<00:00,  1.10s/it]\n",
            "Val   - Epoch: 86, Loss: 1.2821: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]\n",
            "Train - Epoch: 87, Loss: 1.2998: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
            "Val   - Epoch: 87, Loss: 1.2944: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s]\n",
            "Train - Epoch: 88, Loss: 1.2987: 100%|██████████| 16/16 [00:17<00:00,  1.08s/it]\n",
            "Val   - Epoch: 88, Loss: 1.2887: 100%|██████████| 4/4 [00:02<00:00,  1.55it/s]\n",
            "Train - Epoch: 89, Loss: 1.2948: 100%|██████████| 16/16 [00:17<00:00,  1.08s/it]\n",
            "Val   - Epoch: 89, Loss: 1.3114: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]\n",
            "Train - Epoch: 90, Loss: 1.2956: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 90, Loss: 1.2861: 100%|██████████| 4/4 [00:02<00:00,  1.57it/s]\n",
            "Train - Epoch: 91, Loss: 1.2904: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 91, Loss: 1.2679: 100%|██████████| 4/4 [00:02<00:00,  1.78it/s]\n",
            "Train - Epoch: 92, Loss: 1.2957: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 92, Loss: 1.3191: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]\n",
            "Train - Epoch: 93, Loss: 1.2955: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
            "Val   - Epoch: 93, Loss: 1.3062: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]\n",
            "Train - Epoch: 94, Loss: 1.2813: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
            "Val   - Epoch: 94, Loss: 1.2966: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n",
            "Train - Epoch: 95, Loss: 1.2895: 100%|██████████| 16/16 [00:18<00:00,  1.15s/it]\n",
            "Val   - Epoch: 95, Loss: 1.2846: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]\n",
            "Train - Epoch: 96, Loss: 1.2881: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
            "Val   - Epoch: 96, Loss: 1.2949: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n",
            "Train - Epoch: 97, Loss: 1.2910: 100%|██████████| 16/16 [00:18<00:00,  1.14s/it]\n",
            "Val   - Epoch: 97, Loss: 1.2831: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n",
            "Train - Epoch: 98, Loss: 1.2909: 100%|██████████| 16/16 [00:17<00:00,  1.08s/it]\n",
            "Val   - Epoch: 98, Loss: 1.2589: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n",
            "Train - Epoch: 99, Loss: 1.2873: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
            "Val   - Epoch: 99, Loss: 1.2793: 100%|██████████| 4/4 [00:02<00:00,  1.73it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRFrcd5oHiIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6aa7ef98-e81e-4c89-9436-8b1479ef8be1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"correction_model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_7 (Encoder)          multiple                  9550336   \n",
            "_________________________________________________________________\n",
            "decoder_7 (Decoder)          multiple                  11003078  \n",
            "=================================================================\n",
            "Total params: 20,553,414\n",
            "Trainable params: 20,553,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DNyFaRAUaBa",
        "colab_type": "code",
        "outputId": "d29f7b3d-679f-492c-d3c2-1680d156fc12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "mapper = CharsetMapper()\n",
        "inp, trg = ErrorSequence()[0]\n",
        "\n",
        "out = model(tf.expand_dims(inp, 0))\n",
        "out_max = tf.argmax(out, axis=2)\n",
        "print('Original:', mapper.idxs_to_str(inp))\n",
        "print('Predicted: ', mapper.idxs_to_str_batch(out_max)[0])\n",
        "print('Target:', mapper.idxs_to_str(trg))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: A ALMAΤDO LZARO\n",
            "Predicted:  A AE ADASR\n",
            "Target: A ALMA DO LZARO\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}