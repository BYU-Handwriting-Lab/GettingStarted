{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language-model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzMkrT6xqKkY0cxWhpZqmQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BYU-Handwriting-Lab/GettingStarted/blob/solution/notebooks/language-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3SVor5MHLtj",
        "colab_type": "text"
      },
      "source": [
        "# Language Model\n",
        "\n",
        "This notebook provides code to create a character-level language model in \n",
        "TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc4bV3ezKMne",
        "colab_type": "text"
      },
      "source": [
        "### Dependencies\n",
        "\n",
        "Import the necessary dependencies and download our character set and corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibhkP7GXGwhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhHYfrFDKh5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/ericburdett/named-entity-recognition/master/char_set.json\n",
        "!wget -q --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ZsJ8cZSDU98GpcK-kl_Cq3eTt-R2YvSJ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ZsJ8cZSDU98GpcK-kl_Cq3eTt-R2YvSJ\" -O french_ner_dataset.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBV9lreFKUuY",
        "colab_type": "text"
      },
      "source": [
        "### Character Set Mapping\n",
        "\n",
        "Create a Character Set Mapper to go between string and integer representations.\n",
        "\n",
        "Specify the starting and ending character token. These are useful when feeding\n",
        "sentences into our language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DhUAyVxHn1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class CharsetMapper():\n",
        "    def __init__(self, filepath='char_set.json', sequence_size=20, start_char=197, end_char=198):\n",
        "        self.start_char = start_char\n",
        "        self.end_char = end_char\n",
        "        with open(filepath) as f:\n",
        "            self.char_dict = json.load(f)\n",
        "    \n",
        "    def char_to_idx(self, char):\n",
        "        if char in self.char_dict['char_to_idx']:\n",
        "            return int(self.char_dict['char_to_idx'][char])\n",
        "        else:\n",
        "            return 0\n",
        "  \n",
        "    def idx_to_char(self, idx):\n",
        "        if str(int(idx)) in self.char_dict['idx_to_char']:\n",
        "            return self.char_dict['idx_to_char'][str(int(idx))]\n",
        "        else:\n",
        "            return ''\n",
        "  \n",
        "    def str_to_idxs(self, string):\n",
        "        assert type(string) == str\n",
        "\n",
        "        idxs = [self.start_char]\n",
        "        for char in string:\n",
        "            idxs.append(self.char_to_idx(char))\n",
        "        idxs.append(self.end_char)\n",
        "\n",
        "        return np.array(idxs)\n",
        "  \n",
        "    def idxs_to_str(self, idxs):\n",
        "        chars = ''\n",
        "\n",
        "        for idx in idxs:\n",
        "            chars += self.idx_to_char(idx)\n",
        "    \n",
        "        return chars"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkzbYsW_liVM",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Creation\n",
        "\n",
        "Create our dataset by reading from the CSV using pandas, joining sentences, and\n",
        "mapping char representations to integer representations.\n",
        "\n",
        "Notice the use of tf.ragged.constant. This allows us to create a tensor with\n",
        "unequal sequence lengths. Without this, we would be forced to use padding so\n",
        "that our sequence lengths would be constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1pPSxg6MUSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mapper = CharsetMapper()\n",
        "\n",
        "df = pd.read_csv('french_ner_dataset.csv', sep='\\t', header=None, names=['word', 'entity', 'id'])\n",
        "df_size = df['id'].max()\n",
        "\n",
        "sentences_str = []\n",
        "sentences = []\n",
        "for i in range(df_size):\n",
        "    ith_sentence_words = df.loc[df['id'] == i]\n",
        "    sentence = \" \".join(ith_sentence_words['word'].to_list())\n",
        "    sentences_str.append(sentence)\n",
        "    sentences.append(mapper.str_to_idxs(sentence))\n",
        "\n",
        "sentences_tensor = tf.ragged.constant(sentences)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K-Ta9cd-5vo",
        "colab_type": "text"
      },
      "source": [
        "### Word-Level Tokenization\n",
        "\n",
        "We can also tokenize at a word-level instead of the character level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVSRjMTN8L1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e82d3b90-30ed-42f2-f447-66dc8c9ba657"
      },
      "source": [
        "words = list(set(df['word'].values))\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(len(words), filters='', lower=False)\n",
        "tokenizer.fit_on_texts(words)\n",
        "\n",
        "phrase = 'L\\'an mil huit cent'\n",
        "sequences = tokenizer.texts_to_sequences([phrase])\n",
        "back_to_phrase = tokenizer.sequences_to_texts(sequences)[0]\n",
        "\n",
        "print('Total Tokens:', len(words))\n",
        "print('Original Phrase:', phrase)\n",
        "print('Tokenized:', sequences)\n",
        "print('Round Trip:', back_to_phrase)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Tokens: 1042\n",
            "Original Phrase: L'an mil huit cent\n",
            "Tokenized: [[195, 782, 935, 592]]\n",
            "Round Trip: L'an mil huit cent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnLf5GE6_iRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3829c39e-a477-47b5-d262-dfe964b92c13"
      },
      "source": [
        "tokenized_sentences = tokenizer.texts_to_sequences(sentences_str)\n",
        "\n",
        "new_sentences = [[1042] + sentence + [1043] for sentence in tokenized_sentences]\n",
        "sentences_tensor = tf.ragged.constant(new_sentences)\n",
        "sentences_tensor"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[1042, 195, 782, 935, 592, 557, 622, 719, 108, 634, 448, 19, 1027, 107, 83, 984, 448, 43, 176, 885, 26, 255, 270, 660, 912, 1001, 278, 535, 1001, 983, 435, 1001, 92, 310, 344, 108, 776, 406, 1001, 279, 174, 1001, 372, 1008, 448, 776, 536, 977, 138, 176, 285, 410, 1001, 385, 230, 256, 719, 138, 41, 384, 410, 1001, 143, 303, 256, 416, 23, 31, 144, 20, 957, 297, 719, 202, 448, 370, 495, 26, 599, 718, 115, 423, 249, 107, 230, 984, 448, 840, 138, 176, 285, 816, 851, 92, 310, 344, 108, 776, 108, 696, 83, 832, 782, 935, 592, 263, 42, 202, 861, 719, 1001, 567, 1024, 1001, 400, 765, 920, 807, 671, 20, 969, 271, 107, 92, 310, 344, 108, 776, 77, 115, 26, 26, 20, 941, 311, 719, 108, 440, 599, 460, 210, 26, 108, 242, 652, 608, 770, 852, 138, 41, 384, 138, 176, 285, 817, 660, 862, 1043], [1042, 195, 782, 935, 592, 634, 263, 696, 303, 719, 108, 696, 935, 122, 107, 634, 984, 448, 840, 942, 885, 725, 255, 270, 660, 1004, 890, 245, 617, 1001, 278, 535, 1001, 983, 435, 1001, 123, 310, 344, 108, 403, 406, 1001, 35, 853, 1001, 752, 448, 776, 536, 977, 250, 41, 304, 31, 469, 20, 957, 435, 410, 1001, 143, 634, 256, 719, 254, 543, 229, 1035, 448, 922, 613, 310, 410, 1001, 337, 634, 256, 495, 26, 599, 877, 115, 250, 384, 224, 554, 448, 563, 977, 719, 1001, 384, 583, 410, 1001, 42, 256, 490, 107, 417, 384, 807, 160, 423, 249, 107, 42, 984, 448, 269, 1026, 300, 1001, 969, 177, 77, 115, 26, 26, 20, 941, 327, 719, 429, 539, 108, 242, 652, 115, 890, 440, 599, 460, 729, 26, 608, 770, 257, 368, 254, 250, 817, 239, 862, 1043], [1042, 195, 782, 935, 592, 634, 263, 696, 303, 719, 108, 83, 448, 19, 427, 851, 600, 984, 448, 269, 670, 26, 229, 373, 302, 397, 890, 245, 617, 1001, 278, 535, 1001, 983, 435, 1001, 123, 310, 344, 108, 444, 406, 1001, 35, 174, 1001, 752, 28, 448, 776, 536, 977, 176, 260, 568, 410, 1001, 143, 256, 112, 851, 578, 667, 719, 138, 41, 410, 1001, 385, 815, 914, 89, 922, 92, 310, 344, 108, 776, 890, 428, 26, 599, 232, 115, 568, 971, 441, 107, 834, 667, 108, 696, 303, 180, 782, 935, 592, 634, 263, 696, 303, 554, 448, 563, 656, 719, 1001, 991, 184, 807, 671, 423, 249, 851, 303, 984, 448, 269, 448, 300, 448, 681, 138, 41, 77, 115, 26, 26, 20, 941, 7, 719, 711, 29, 108, 854, 652, 115, 890, 440, 599, 460, 608, 770, 852, 138, 817, 660, 866, 862, 1043], [1042, 817, 143, 719, 50, 467, 782, 935, 592, 634, 263, 696, 303, 107, 83, 984, 448, 840, 26, 58, 386, 660, 1001, 92, 310, 344, 108, 776, 711, 638, 719, 199, 108, 242, 463, 396, 715, 1001, 634, 200, 1001, 312, 817, 660, 862, 1043], [1042, 195, 782, 935, 592, 634, 263, 696, 935, 719, 108, 85, 448, 19, 1001, 832, 107, 696, 984, 448, 269, 942, 1001, 509, 26, 58, 270, 660, 912, 1001, 278, 171, 1001, 983, 435, 1001, 123, 310, 344, 108, 499, 406, 1001, 59, 174, 1001, 752, 142, 926, 776, 536, 699, 10, 422, 410, 1001, 143, 23, 256, 350, 591, 719, 175, 523, 170, 410, 1001, 120, 719, 50, 256, 416, 23, 810, 851, 182, 667, 495, 26, 599, 718, 115, 10, 702, 128, 410, 1001, 120, 230, 256, 1007, 1001, 500, 456, 391, 1001, 375, 469, 851, 182, 667, 807, 671, 107, 123, 310, 344, 108, 499, 108, 618, 832, 77, 115, 26, 26, 20, 941, 311, 719, 890, 440, 599, 460, 729, 26, 108, 242, 652, 608, 770, 257, 817, 660, 588, 10, 672, 424, 862, 1043], [1042, 195, 782, 935, 592, 634, 263, 696, 935, 719, 108, 263, 448, 19, 1001, 1025, 851, 83, 984, 448, 586, 942, 885, 26, 255, 270, 660, 912, 1001, 278, 535, 1001, 983, 878, 1001, 92, 310, 344, 108, 403, 406, 1001, 59, 174, 1043], [1042, 195, 782, 83, 592, 719, 108, 230, 832, 851, 600, 984, 719, 633, 448, 269, 942, 885, 26, 822, 270, 239, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 344, 108, 776, 211, 1001, 59, 174, 1001, 752, 961, 599, 977, 20, 983, 965, 537, 373, 6, 1001, 263, 303, 256, 719, 537, 753, 6, 1001, 923, 256, 416, 23, 31, 719, 144, 20, 957, 435, 719, 416, 23, 202, 448, 326, 26, 599, 492, 115, 537, 285, 41, 816, 108, 263, 42, 365, 782, 935, 592, 664, 851, 362, 667, 31, 202, 1001, 502, 719, 1001, 384, 424, 416, 23, 814, 851, 362, 667, 1036, 20, 563, 190, 1001, 679, 384, 943, 20, 601, 190, 851, 1014, 125, 807, 671, 691, 851, 600, 984, 448, 840, 107, 331, 77, 115, 26, 26, 20, 941, 311, 719, 890, 440, 599, 803, 729, 26, 108, 242, 652, 608, 770, 257, 817, 239, 537, 647, 537, 753, 862, 1043], [1042, 195, 152, 268, 719, 108, 85, 153, 107, 935, 984, 448, 269, 942, 885, 26, 255, 270, 384, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 108, 776, 406, 1001, 59, 174, 1001, 752, 961, 599, 977, 138, 41, 384, 6, 1001, 385, 50, 256, 51, 448, 326, 719, 194, 41, 6, 1001, 337, 935, 256, 168, 201, 448, 370, 416, 23, 459, 719, 144, 107, 461, 310, 751, 108, 776, 604, 495, 26, 599, 718, 115, 138, 176, 285, 816, 108, 739, 705, 782, 935, 592, 120, 50, 851, 461, 310, 344, 108, 280, 202, 414, 1001, 138, 176, 285, 719, 1001, 765, 506, 416, 23, 797, 107, 461, 310, 344, 108, 776, 470, 1001, 161, 1021, 807, 646, 20, 969, 999, 107, 461, 310, 344, 108, 403, 423, 249, 107, 303, 984, 448, 849, 77, 115, 26, 26, 20, 951, 504, 719, 890, 440, 599, 803, 729, 26, 108, 242, 777, 608, 770, 852, 66, 138, 41, 817, 660, 862, 1043], [1042, 195, 782, 935, 592, 634, 263, 696, 83, 719, 108, 263, 230, 706, 662, 107, 23, 984, 448, 840, 942, 885, 725, 255, 270, 239, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 751, 108, 776, 103, 1001, 863, 174, 1001, 752, 961, 599, 977, 20, 983, 965, 540, 176, 6, 1001, 143, 50, 956, 202, 448, 370, 719, 773, 366, 399, 1001, 740, 1039, 815, 402, 448, 370, 416, 23, 31, 719, 144, 107, 461, 310, 751, 108, 776, 890, 428, 26, 599, 718, 115, 540, 41, 816, 108, 477, 665, 501, 935, 761, 385, 107, 651, 667, 31, 202, 1001, 384, 540, 518, 1036, 1001, 917, 384, 74, 1007, 20, 601, 190, 505, 47, 208, 807, 646, 691, 107, 634, 984, 448, 43, 20, 969, 300, 851, 461, 310, 751, 108, 340, 77, 115, 26, 26, 20, 941, 311, 719, 890, 440, 599, 803, 729, 26, 108, 272, 608, 770, 852, 817, 660, 540, 883, 862, 1043], [1042, 195, 782, 935, 592, 634, 263, 696, 83, 719, 108, 1012, 584, 107, 935, 984, 448, 269, 942, 885, 725, 255, 270, 787, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 344, 108, 776, 211, 1001, 59, 174, 1001, 752, 961, 599, 203, 20, 983, 965, 512, 837, 6, 1001, 120, 1039, 14, 811, 1001, 983, 400, 719, 512, 753, 6, 1001, 385, 303, 815, 416, 23, 31, 719, 144, 27, 310, 751, 108, 776, 495, 26, 599, 718, 115, 947, 743, 31, 441, 884, 401, 686, 667, 108, 83, 180, 782, 935, 592, 739, 554, 1001, 478, 743, 719, 1001, 629, 289, 416, 23, 305, 741, 1001, 100, 886, 807, 745, 691, 739, 792, 107, 1039, 984, 448, 840, 20, 969, 466, 851, 461, 310, 344, 108, 776, 77, 115, 26, 26, 20, 951, 311, 719, 890, 440, 599, 803, 729, 26, 108, 242, 652, 608, 770, 852, 108, 660, 682, 658, 658, 1043], [1042, 817, 143, 50, 792, 782, 935, 592, 634, 263, 696, 83, 107, 83, 984, 448, 840, 26, 239, 1001, 461, 310, 751, 108, 776, 429, 638, 719, 323, 108, 854, 778, 396, 715, 1001, 1039, 200, 817, 660, 862, 1043], [1042, 195, 782, 83, 592, 719, 108, 230, 832, 851, 600, 984, 719, 633, 448, 269, 942, 885, 26, 255, 270, 239, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 344, 108, 776, 406, 1001, 59, 204, 1001, 752, 961, 599, 977, 20, 983, 965, 537, 373, 6, 1001, 263, 303, 256, 719, 537, 753, 281, 1001, 263, 42, 256, 416, 23, 31, 719, 144, 20, 957, 435, 719, 416, 23, 202, 448, 326, 26, 599, 492, 115, 537, 285, 41, 816, 108, 263, 42, 365, 782, 935, 592, 143, 42, 851, 362, 529, 31, 202, 1001, 502, 719, 1001, 384, 424, 416, 23, 814, 851, 362, 667, 1036, 20, 563, 190, 1001, 679, 384, 943, 20, 601, 190, 851, 1014, 273, 807, 671, 691, 851, 600, 984, 448, 840, 107, 461, 310, 344, 108, 403, 77, 115, 26, 26, 20, 941, 311, 719, 890, 440, 599, 803, 729, 26, 108, 242, 652, 608, 770, 257, 537, 270, 537, 753, 817, 660, 862, 1043], [1042, 195, 782, 83, 592, 719, 108, 263, 42, 1025, 851, 23, 984, 448, 840, 942, 885, 26, 844, 575, 270, 239, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 751, 108, 776, 406, 1001, 59, 204, 1001, 752, 961, 599, 977, 20, 983, 965, 512, 753, 6, 1001, 744, 256, 202, 448, 370, 719, 138, 176, 753, 963, 6, 1001, 385, 42, 256, 416, 23, 551, 719, 144, 20, 957, 435, 495, 599, 718, 115, 512, 837, 816, 108, 263, 659, 365, 782, 935, 592, 126, 851, 616, 334, 667, 31, 202, 1001, 837, 719, 1001, 384, 867, 416, 23, 814, 1036, 1001, 743, 991, 807, 671, 423, 249, 851, 600, 73, 448, 269, 20, 969, 300, 851, 461, 310, 344, 108, 776, 77, 115, 26, 26, 20, 941, 648, 719, 890, 656, 599, 460, 729, 26, 108, 242, 652, 608, 770, 852, 138, 675, 963, 512, 817, 239, 862, 1043], [1042, 195, 782, 83, 592, 719, 108, 263, 303, 180, 851, 935, 73, 448, 269, 942, 885, 26, 255, 270, 239, 912, 1001, 278, 535, 1001, 983, 498, 121, 1001, 461, 310, 751, 108, 776, 406, 1001, 59, 174, 1001, 752, 961, 599, 977, 20, 983, 965, 506, 176, 865, 448, 757, 6, 1001, 874, 256, 719, 282, 176, 6, 1001, 385, 42, 256, 416, 23, 31, 719, 357, 768, 20, 957, 435, 495, 599, 718, 115, 506, 100, 308, 816, 108, 42, 905, 782, 83, 592, 851, 461, 310, 344, 108, 776, 961, 202, 99, 506, 176, 719, 1001, 562, 384, 969, 364, 416, 23, 31, 851, 461, 799, 344, 108, 776, 807, 671, 423, 249, 851, 230, 984, 448, 269, 1026, 300, 1001, 93, 944, 851, 461, 310, 344, 108, 776, 77, 115, 26, 26, 20, 951, 311, 719, 890, 440, 599, 460, 729, 26, 108, 242, 652, 608, 770, 257, 683, 282, 817, 660, 862, 1043], [1042, 195, 782, 83, 719, 108, 263, 230, 234, 83, 984, 448, 269, 670, 26, 993, 57, 239, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 875, 661, 406, 1001, 59, 853, 1001, 752, 961, 599, 977, 20, 983, 787, 255, 100, 270, 639, 6, 1001, 263, 42, 256, 202, 1001, 983, 400, 719, 996, 753, 847, 6, 1001, 385, 634, 256, 416, 23, 89, 20, 957, 435, 495, 26, 599, 718, 115, 869, 224, 441, 851, 769, 667, 108, 85, 345, 782, 935, 592, 385, 83, 554, 1001, 260, 719, 1001, 382, 172, 470, 1001, 100, 255, 719, 388, 448, 569, 656, 807, 745, 691, 851, 935, 984, 448, 43, 54, 969, 300, 851, 461, 875, 108, 776, 77, 115, 26, 26, 20, 941, 311, 719, 890, 440, 599, 460, 729, 26, 108, 242, 652, 608, 115, 770, 409, 20, 107, 474, 257, 817, 239, 934, 996, 255, 585, 993, 1043], [1042, 195, 782, 83, 592, 1031, 108, 263, 83, 832, 851, 600, 984, 448, 269, 670, 725, 993, 57, 239, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 344, 108, 776, 211, 1001, 59, 174, 1001, 372, 403, 599, 101, 20, 983, 965, 512, 837, 31, 6, 1001, 120, 42, 256, 719, 996, 149, 286, 6, 1001, 385, 42, 815, 416, 23, 144, 20, 957, 435, 495, 26, 599, 718, 115, 512, 308, 270, 816, 851, 461, 606, 108, 23, 180, 782, 83, 592, 202, 1001, 512, 753, 719, 1001, 282, 545, 668, 807, 671, 454, 851, 935, 984, 448, 269, 54, 108, 300, 1001, 93, 177, 719, 823, 851, 461, 220, 190, 448, 146, 77, 115, 26, 26, 20, 941, 311, 719, 890, 440, 599, 460, 729, 26, 108, 242, 652, 1001, 312, 608, 115, 770, 409, 20, 107, 474, 852, 817, 660, 934, 996, 512, 993, 1043], [1042, 195, 782, 83, 592, 23, 1018, 851, 230, 984, 448, 840, 363, 725, 993, 462, 660, 719, 912, 1001, 910, 535, 1001, 983, 953, 461, 310, 344, 108, 403, 406, 1001, 59, 826, 1001, 372, 444, 599, 977, 851, 983, 965, 548, 188, 639, 6, 1001, 143, 42, 256, 719, 996, 753, 687, 6, 1001, 385, 42, 256, 416, 23, 144, 20, 957, 297, 495, 26, 599, 718, 115, 548, 214, 1017, 526, 1001, 23, 19, 554, 448, 563, 656, 719, 1001, 102, 918, 719, 745, 53, 851, 634, 984, 448, 43, 1026, 300, 1001, 93, 177, 719, 823, 190, 1001, 1022, 77, 115, 26, 26, 20, 941, 648, 719, 890, 440, 599, 460, 729, 26, 108, 242, 652, 1001, 360, 608, 115, 770, 409, 20, 107, 474, 257, 817, 660, 934, 996, 548, 424, 993, 1043], [1042, 195, 782, 83, 592, 1031, 108, 83, 365, 851, 88, 73, 448, 840, 363, 26, 993, 462, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 921, 776, 406, 1001, 863, 174, 1001, 372, 403, 599, 977, 851, 983, 965, 255, 113, 6, 1001, 263, 42, 256, 719, 255, 531, 6, 1001, 120, 23, 815, 416, 23, 31, 144, 851, 318, 190, 1001, 967, 495, 26, 599, 718, 895, 114, 860, 851, 696, 984, 448, 269, 54, 108, 183, 1001, 405, 190, 784, 94, 1001, 405, 596, 1001, 1030, 310, 983, 219, 791, 398, 441, 851, 461, 310, 344, 108, 776, 108, 477, 707, 782, 935, 592, 263, 83, 554, 1001, 791, 270, 719, 1001, 962, 981, 13, 1001, 138, 612, 39, 26, 936, 311, 1001, 423, 360, 719, 895, 354, 890, 359, 299, 890, 440, 599, 460, 729, 26, 108, 242, 652, 608, 115, 770, 409, 20, 107, 474, 852, 255, 871, 255, 176, 817, 660, 993, 1043], [1042, 195, 782, 83, 592, 23, 108, 634, 76, 851, 230, 984, 448, 788, 670, 725, 993, 57, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 344, 108, 444, 406, 1001, 863, 174, 1001, 752, 776, 599, 977, 851, 983, 787, 487, 485, 700, 410, 1001, 143, 303, 815, 202, 1001, 983, 400, 89, 851, 182, 667, 719, 1011, 753, 410, 1001, 143, 23, 256, 966, 1001, 766, 89, 899, 851, 769, 320, 1001, 983, 60, 495, 26, 599, 718, 115, 157, 994, 355, 95, 1001, 337, 696, 256, 369, 851, 769, 667, 441, 851, 461, 176, 1001, 827, 667, 20, 1018, 782, 935, 592, 143, 1031, 554, 1001, 478, 676, 846, 719, 1001, 994, 496, 13, 1001, 753, 485, 107, 474, 486, 246, 54, 890, 430, 448, 776, 344, 108, 596, 1001, 983, 435, 1001, 461, 310, 344, 776, 851, 303, 984, 448, 110, 39, 26, 936, 311, 448, 360, 719, 895, 517, 890, 845, 299, 890, 440, 599, 460, 729, 26, 108, 242, 652, 1001, 360, 608, 115, 770, 409, 20, 107, 474, 257, 104, 524, 753, 968, 817, 239, 993, 1043], [1042, 195, 782, 83, 592, 634, 108, 263, 230, 685, 851, 935, 984, 448, 849, 670, 26, 993, 57, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 331, 406, 1001, 59, 174, 1001, 752, 403, 599, 699, 78, 548, 188, 31, 410, 1001, 143, 303, 815, 469, 20, 957, 435, 719, 130, 958, 31, 410, 1001, 385, 50, 256, 251, 343, 851, 461, 733, 495, 26, 599, 718, 115, 548, 176, 424, 816, 851, 690, 667, 108, 696, 935, 707, 782, 83, 592, 713, 202, 1001, 351, 719, 1001, 258, 384, 765, 807, 671, 53, 851, 42, 984, 448, 269, 1026, 300, 448, 563, 656, 39, 26, 936, 311, 448, 360, 890, 440, 599, 460, 729, 26, 108, 242, 652, 1001, 360, 608, 115, 770, 409, 20, 107, 474, 257, 424, 522, 812, 817, 660, 993, 1043], [1042, 195, 782, 83, 592, 634, 108, 477, 448, 19, 455, 851, 23, 984, 448, 43, 26, 993, 462, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 105, 776, 406, 1001, 59, 848, 693, 1001, 752, 961, 26, 941, 69, 344, 983, 830, 309, 448, 776, 190, 784, 611, 654, 971, 510, 448, 415, 449, 1001, 769, 719, 448, 71, 1001, 771, 1001, 461, 733, 316, 34, 884, 542, 623, 719, 851, 1034, 283, 931, 632, 448, 499, 942, 108, 681, 2, 438, 31, 851, 461, 606, 519, 931, 448, 937, 566, 538, 786, 448, 624, 609, 876, 283, 116, 1001, 222, 395, 719, 924, 410, 1001, 143, 851, 143, 42, 256, 855, 284, 321, 868, 20, 735, 439, 137, 677, 367, 114, 344, 973, 338, 850, 1001, 838, 719, 552, 108, 790, 609, 879, 818, 16, 333, 206, 275, 247, 541, 807, 851, 313, 115, 553, 876, 283, 287, 505, 781, 26, 429, 1037, 108, 242, 652, 1001, 360, 20, 1029, 1001, 2, 260, 410, 1001, 143, 83, 815, 719, 1001, 916, 169, 410, 1001, 346, 815, 416, 23, 31, 89, 851, 461, 310, 344, 108, 403, 495, 599, 460, 729, 26, 108, 242, 652, 608, 770, 257, 472, 2, 108, 660, 993, 519, 143, 50, 467, 782, 83, 592, 634, 851, 484, 26, 993, 462, 660, 719, 912, 1001, 278, 535, 1001, 461, 921, 403, 976, 884, 418, 448, 673, 535, 429, 638, 719, 323, 108, 242, 231, 396, 23, 516, 1001, 1002, 817, 660, 993, 1043], [1042, 195, 782, 83, 592, 570, 108, 263, 303, 980, 851, 935, 984, 448, 849, 670, 725, 993, 57, 239, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 875, 661, 406, 1001, 863, 174, 1001, 752, 28, 448, 776, 599, 977, 851, 983, 965, 240, 240, 839, 255, 100, 373, 639, 815, 36, 202, 1001, 983, 400, 719, 996, 149, 286, 410, 1001, 120, 815, 416, 23, 144, 20, 957, 498, 121, 495, 26, 599, 718, 115, 959, 573, 1040, 872, 441, 851, 565, 468, 108, 23, 315, 782, 935, 592, 263, 554, 1001, 41, 959, 719, 1001, 593, 695, 797, 719, 13, 1001, 255, 100, 807, 518, 20, 969, 300, 851, 461, 241, 691, 851, 230, 984, 719, 633, 448, 419, 39, 26, 936, 843, 448, 360, 890, 440, 599, 460, 729, 26, 108, 242, 652, 608, 764, 409, 20, 107, 474, 982, 770, 817, 239, 934, 996, 255, 993, 1043], [1042, 195, 782, 83, 592, 570, 108, 1039, 17, 851, 83, 984, 448, 269, 670, 725, 993, 462, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 606, 406, 1001, 35, 174, 1001, 372, 28, 448, 776, 599, 977, 851, 983, 787, 240, 240, 97, 896, 260, 31, 410, 1001, 619, 815, 320, 448, 757, 719, 255, 8, 270, 639, 410, 1001, 143, 23, 815, 925, 448, 757, 416, 23, 89, 851, 461, 310, 129, 495, 26, 599, 718, 115, 378, 270, 31, 816, 851, 461, 755, 610, 108, 477, 81, 782, 935, 592, 143, 42, 202, 927, 719, 1001, 224, 138, 470, 1001, 964, 288, 807, 671, 53, 851, 1039, 984, 448, 269, 54, 969, 300, 851, 461, 606, 190, 1001, 627, 39, 26, 936, 311, 448, 796, 890, 440, 599, 460, 729, 26, 108, 242, 652, 608, 115, 770, 409, 20, 474, 257, 817, 239, 896, 260, 255, 993, 1043], [1042, 195, 782, 83, 592, 303, 108, 230, 17, 851, 634, 984, 448, 840, 363, 725, 993, 462, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 129, 406, 1001, 59, 174, 1001, 372, 28, 448, 403, 599, 977, 851, 983, 965, 240, 240, 819, 896, 438, 639, 410, 1001, 143, 23, 256, 320, 1001, 983, 60, 719, 255, 100, 270, 65, 410, 1001, 619, 815, 925, 1001, 983, 60, 416, 23, 251, 343, 851, 461, 241, 495, 26, 599, 718, 115, 964, 447, 929, 441, 20, 957, 435, 108, 263, 905, 782, 935, 592, 143, 634, 554, 1001, 176, 304, 719, 1001, 384, 939, 364, 719, 13, 1001, 948, 373, 807, 745, 454, 851, 88, 73, 719, 633, 448, 840, 54, 969, 300, 851, 461, 241, 190, 1001, 55, 39, 26, 936, 311, 448, 360, 890, 440, 599, 460, 729, 26, 108, 854, 652, 608, 115, 770, 409, 20, 107, 474, 257, 896, 260, 255, 817, 239, 993, 1043], [1042, 195, 782, 83, 592, 303, 108, 85, 995, 107, 696, 984, 448, 849, 670, 725, 993, 462, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 606, 406, 1001, 59, 722, 1001, 752, 1008, 448, 776, 599, 977, 851, 983, 239, 723, 655, 41, 31, 410, 1001, 263, 230, 256, 202, 1001, 757, 719, 255, 176, 304, 639, 410, 1001, 385, 50, 256, 320, 448, 757, 416, 23, 89, 851, 318, 1013, 495, 26, 599, 718, 115, 655, 903, 31, 816, 884, 243, 776, 108, 618, 995, 782, 935, 592, 663, 202, 1001, 304, 719, 1001, 981, 765, 83, 1001, 453, 224, 807, 949, 454, 851, 42, 984, 448, 849, 54, 969, 300, 851, 461, 921, 776, 190, 1001, 983, 11, 39, 26, 936, 311, 448, 480, 890, 933, 599, 460, 729, 26, 108, 242, 777, 608, 115, 770, 409, 20, 107, 474, 852, 255, 176, 304, 817, 239, 655, 41, 993, 1043], [1042, 195, 782, 83, 592, 303, 108, 42, 685, 851, 935, 984, 448, 269, 670, 26, 993, 462, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 241, 406, 1001, 35, 204, 1001, 372, 28, 448, 776, 599, 977, 851, 983, 787, 240, 240, 901, 746, 970, 459, 31, 410, 1001, 337, 1039, 815, 253, 448, 653, 719, 996, 149, 687, 410, 1001, 120, 256, 416, 23, 89, 851, 461, 606, 495, 26, 599, 718, 115, 746, 270, 970, 816, 851, 461, 606, 108, 263, 303, 122, 782, 935, 592, 634, 637, 202, 1001, 270, 155, 384, 319, 1030, 671, 691, 851, 42, 984, 448, 840, 54, 969, 300, 851, 461, 310, 129, 190, 1001, 983, 136, 608, 26, 936, 311, 448, 480, 890, 440, 599, 460, 115, 26, 108, 854, 652, 324, 770, 257, 892, 996, 746, 817, 239, 993, 1043], [1042, 195, 782, 83, 303, 108, 143, 972, 851, 42, 984, 448, 840, 670, 26, 993, 57, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 606, 406, 1001, 59, 174, 1001, 752, 1008, 448, 776, 599, 977, 851, 983, 965, 723, 255, 100, 270, 639, 410, 1001, 156, 23, 815, 202, 448, 326, 719, 996, 753, 290, 410, 1001, 120, 815, 416, 23, 89, 851, 461, 241, 495, 26, 599, 718, 115, 255, 100, 31, 816, 20, 37, 435, 108, 143, 50, 345, 782, 935, 592, 385, 23, 202, 1001, 100, 719, 1001, 404, 479, 797, 719, 1036, 1001, 412, 657, 869, 807, 671, 454, 851, 23, 984, 448, 840, 54, 969, 300, 851, 461, 310, 344, 776, 190, 1001, 55, 608, 26, 936, 448, 480, 890, 440, 599, 460, 729, 26, 108, 242, 652, 608, 115, 770, 409, 20, 107, 474, 257, 817, 239, 255, 934, 996, 993, 519, 143, 50, 467, 782, 83, 592, 303, 851, 484, 26, 993, 462, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 921, 776, 406, 1001, 59, 888, 1001, 752, 961, 347, 884, 418, 448, 673, 535, 429, 638, 719, 323, 108, 242, 463, 396, 230, 516, 1001, 480, 107, 1030, 606, 108, 919, 467, 782, 83, 592, 303, 817, 660, 993, 1043], [1042, 195, 782, 83, 592, 992, 108, 263, 230, 345, 851, 634, 984, 448, 840, 670, 26, 993, 57, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 344, 108, 403, 211, 1001, 887, 1001, 372, 28, 448, 403, 599, 977, 851, 983, 965, 240, 534, 748, 260, 410, 1001, 385, 42, 815, 858, 1001, 329, 719, 906, 424, 22, 410, 1001, 143, 634, 256, 376, 416, 23, 144, 851, 804, 276, 890, 437, 495, 26, 599, 718, 895, 692, 54, 983, 236, 1, 333, 955, 662, 574, 344, 108, 596, 1001, 983, 435, 1001, 461, 310, 344, 108, 336, 108, 790, 730, 141, 95, 511, 337, 618, 851, 337, 739, 815, 63, 306, 503, 26, 789, 106, 800, 178, 108, 415, 754, 154, 316, 262, 108, 480, 26, 107, 982, 928, 749, 519, 851, 781, 26, 429, 1032, 108, 859, 851, 1039, 984, 448, 840, 719, 890, 235, 277, 599, 460, 729, 26, 108, 242, 652, 608, 115, 770, 409, 20, 107, 106, 852, 817, 239, 748, 374, 703, 993, 1043], [1042, 195, 782, 83, 592, 935, 108, 1039, 747, 851, 935, 984, 448, 849, 670, 725, 993, 57, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 1030, 606, 406, 1001, 59, 987, 693, 1001, 372, 28, 448, 403, 599, 977, 20, 983, 965, 723, 916, 748, 589, 21, 410, 806, 120, 935, 815, 719, 743, 285, 193, 639, 410, 1001, 385, 1039, 256, 416, 23, 144, 20, 957, 435, 495, 26, 599, 718, 115, 423, 269, 851, 42, 984, 508, 599, 114, 969, 108, 736, 555, 893, 1001, 409, 442, 435, 1001, 461, 921, 776, 108, 790, 283, 116, 809, 851, 88, 335, 448, 922, 950, 611, 897, 1001, 461, 684, 544, 107, 114, 54, 93, 907, 88, 451, 1001, 581, 614, 161, 1001, 461, 841, 911, 610, 5, 115, 117, 50, 1010, 163, 86, 410, 1001, 120, 83, 256, 802, 816, 1026, 52, 233, 1005, 605, 630, 108, 263, 634, 345, 782, 83, 592, 935, 719, 756, 742, 546, 108, 727, 1025, 507, 541, 390, 20, 636, 718, 851, 969, 762, 851, 581, 764, 131, 202, 1001, 716, 719, 1001, 24, 145, 640, 348, 694, 669, 56, 942, 983, 965, 448, 52, 541, 1019, 115, 761, 116, 842, 481, 3, 344, 890, 547, 1001, 278, 535, 1001, 957, 435, 39, 890, 475, 625, 26, 429, 982, 928, 579, 719, 890, 440, 599, 794, 729, 26, 108, 854, 652, 608, 115, 770, 409, 20, 107, 474, 852, 358, 594, 817, 660, 993, 1043], [1042, 195, 782, 83, 592, 935, 108, 696, 315, 107, 83, 984, 448, 849, 670, 725, 993, 57, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 162, 211, 1001, 59, 174, 1001, 752, 28, 448, 776, 599, 426, 851, 983, 787, 723, 282, 176, 639, 410, 1001, 120, 1039, 256, 89, 851, 461, 310, 776, 202, 448, 370, 719, 255, 270, 979, 411, 410, 1001, 385, 230, 815, 300, 851, 461, 276, 680, 776, 320, 448, 349, 495, 26, 599, 718, 908, 851, 42, 984, 448, 269, 282, 488, 816, 20, 957, 435, 108, 263, 83, 315, 782, 935, 592, 143, 833, 202, 1001, 502, 719, 1001, 1009, 641, 814, 719, 1036, 20, 87, 26, 1001, 384, 964, 719, 20, 576, 443, 1001, 642, 628, 807, 197, 54, 969, 300, 904, 458, 558, 190, 1001, 983, 835, 608, 26, 936, 985, 480, 890, 235, 277, 599, 460, 729, 26, 108, 242, 652, 608, 770, 852, 817, 660, 383, 590, 993, 817, 143, 50, 805, 782, 83, 592, 935, 851, 484, 26, 993, 57, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 921, 499, 406, 1001, 59, 204, 1001, 752, 961, 429, 638, 719, 323, 108, 854, 463, 396, 1039, 516, 1001, 480, 368, 461, 558, 108, 143, 252, 805, 782, 83, 592, 935, 817, 660, 993, 1043], [1042, 195, 782, 83, 592, 83, 108, 83, 995, 851, 935, 984, 448, 110, 670, 725, 993, 57, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 606, 406, 1001, 59, 446, 307, 1001, 372, 28, 448, 403, 599, 977, 20, 983, 965, 723, 512, 913, 31, 410, 1001, 263, 634, 256, 177, 1001, 983, 801, 719, 658, 149, 322, 410, 1001, 120, 230, 815, 416, 23, 89, 851, 461, 606, 495, 26, 599, 718, 115, 691, 851, 600, 984, 719, 633, 448, 840, 512, 697, 441, 20, 957, 435, 108, 263, 935, 467, 782, 83, 592, 992, 554, 1001, 913, 512, 719, 1001, 139, 384, 44, 807, 745, 54, 108, 300, 1001, 93, 177, 719, 388, 851, 461, 310, 344, 108, 776, 190, 448, 216, 608, 26, 936, 311, 448, 480, 890, 440, 599, 460, 729, 26, 108, 854, 652, 608, 115, 770, 409, 20, 107, 474, 257, 817, 660, 512, 913, 512, 993, 1043], [1042, 195, 782, 83, 592, 678, 108, 263, 665, 851, 473, 1001, 388, 670, 725, 993, 57, 239, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 241, 406, 1001, 863, 174, 1001, 752, 28, 448, 499, 599, 977, 107, 983, 965, 723, 1000, 650, 31, 410, 1001, 385, 256, 202, 1001, 983, 400, 719, 996, 753, 286, 410, 1001, 120, 23, 815, 416, 23, 144, 20, 957, 297, 495, 26, 599, 70, 115, 947, 1023, 929, 441, 851, 186, 533, 108, 42, 1018, 782, 935, 592, 385, 554, 333, 4, 285, 1023, 719, 1001, 90, 765, 741, 1001, 753, 1000, 807, 671, 53, 107, 600, 984, 448, 269, 54, 108, 300, 448, 563, 223, 851, 461, 380, 190, 1001, 983, 82, 209, 26, 936, 311, 448, 480, 890, 933, 599, 460, 729, 26, 108, 242, 652, 324, 115, 770, 409, 20, 107, 474, 852, 817, 660, 934, 996, 1000, 993, 1043], [1042, 195, 782, 83, 1003, 108, 263, 50, 707, 851, 696, 984, 448, 849, 670, 26, 993, 57, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 1030, 558, 406, 1001, 35, 174, 1001, 752, 28, 448, 776, 599, 977, 851, 983, 965, 723, 643, 41, 639, 410, 1001, 385, 83, 256, 719, 996, 753, 290, 410, 1001, 120, 23, 256, 416, 23, 89, 20, 957, 435, 495, 26, 599, 70, 115, 889, 445, 929, 441, 851, 493, 452, 667, 108, 23, 905, 782, 935, 592, 263, 935, 554, 1001, 285, 719, 1001, 889, 825, 13, 1001, 255, 149, 797, 75, 745, 691, 851, 935, 984, 448, 840, 1026, 300, 448, 563, 656, 190, 448, 213, 608, 26, 936, 311, 448, 339, 890, 933, 599, 460, 729, 26, 108, 854, 652, 324, 115, 770, 409, 20, 107, 938, 852, 738, 934, 996, 817, 660, 993, 817, 143, 50, 467, 782, 83, 592, 851, 484, 26, 993, 462, 660, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 129, 406, 1001, 59, 853, 1001, 752, 961, 429, 638, 719, 323, 108, 242, 463, 396, 1039, 516, 1001, 480, 368, 461, 387, 108, 143, 50, 467, 782, 83, 592, 83, 108, 660, 993, 1043], [1042, 195, 782, 83, 592, 12, 108, 263, 42, 779, 851, 42, 984, 448, 840, 670, 725, 993, 462, 239, 719, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 317, 406, 1001, 59, 863, 174, 1001, 372, 28, 448, 499, 977, 20, 983, 787, 78, 743, 436, 285, 291, 410, 1001, 385, 303, 256, 202, 448, 757, 719, 2, 260, 265, 410, 1001, 385, 303, 815, 320, 448, 326, 416, 23, 89, 851, 461, 241, 689, 26, 599, 718, 115, 743, 304, 393, 31, 816, 54, 983, 795, 435, 108, 618, 315, 782, 935, 592, 143, 634, 202, 1001, 304, 719, 1001, 991, 413, 83, 1001, 382, 758, 807, 671, 53, 851, 1039, 984, 448, 840, 20, 969, 300, 851, 461, 241, 190, 1001, 986, 608, 26, 936, 311, 448, 988, 890, 580, 599, 460, 729, 26, 108, 242, 777, 608, 770, 257, 817, 239, 743, 2, 993, 1043], [1042, 195, 782, 83, 592, 12, 108, 85, 972, 851, 935, 984, 448, 269, 363, 725, 746, 176, 559, 1026, 660, 513, 245, 617, 1001, 278, 535, 1001, 983, 435, 1001, 461, 124, 406, 1001, 59, 174, 1001, 372, 28, 448, 776, 599, 977, 20, 983, 965, 240, 534, 226, 708, 31, 410, 1001, 263, 83, 815, 320, 448, 757, 719, 643, 169, 193, 639, 410, 1001, 120, 23, 815, 925, 448, 326, 416, 23, 89, 851, 461, 606, 495, 26, 599, 718, 115, 993, 304, 314, 31, 816, 20, 983, 795, 435, 108, 477, 132, 782, 935, 592, 385, 42, 202, 1001, 270, 871, 993, 719, 1001, 384, 629, 49, 470, 1001, 964, 294, 181, 807, 671, 53, 851, 23, 984, 448, 269, 54, 969, 300, 851, 461, 162, 190, 448, 267, 39, 26, 936, 311, 448, 360, 890, 580, 599, 460, 729, 26, 108, 242, 652, 608, 115, 770, 409, 20, 107, 474, 257, 892, 559, 187, 643, 746, 1043], [1042, 195, 782, 83, 592, 618, 108, 1039, 45, 851, 634, 984, 448, 43, 670, 725, 407, 314, 660, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 124, 406, 1001, 863, 174, 1001, 372, 556, 448, 776, 599, 977, 851, 983, 965, 240, 78, 909, 185, 41, 410, 1001, 263, 83, 815, 639, 177, 1001, 147, 719, 643, 41, 639, 410, 1001, 120, 23, 815, 89, 416, 23, 851, 461, 310, 135, 890, 428, 26, 599, 718, 115, 909, 753, 373, 816, 108, 303, 905, 782, 83, 592, 618, 54, 983, 795, 435, 202, 448, 563, 656, 719, 1001, 514, 1009, 384, 807, 671, 53, 851, 303, 984, 448, 269, 54, 108, 300, 1001, 93, 177, 719, 823, 190, 1001, 986, 39, 26, 936, 311, 448, 360, 890, 580, 599, 460, 729, 26, 108, 854, 652, 608, 115, 770, 409, 20, 107, 474, 852, 909, 1041, 643, 817, 239, 527, 1043], [1042, 195, 782, 83, 592, 618, 719, 108, 263, 23, 365, 851, 634, 984, 448, 43, 942, 885, 26, 407, 393, 302, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 344, 776, 406, 1001, 476, 961, 599, 977, 158, 100, 285, 6, 448, 385, 634, 815, 858, 1001, 244, 719, 597, 871, 410, 1001, 385, 230, 256, 376, 416, 23, 144, 851, 461, 276, 890, 437, 495, 26, 599, 718, 895, 114, 1026, 1038, 448, 776, 344, 108, 596, 1001, 983, 435, 1001, 461, 310, 108, 793, 283, 116, 259, 379, 179, 780, 450, 54, 960, 356, 475, 623, 599, 474, 852, 942, 983, 897, 719, 108, 415, 225, 1001, 461, 1020, 640, 890, 30, 352, 464, 344, 108, 757, 541, 196, 283, 1010, 150, 100, 816, 983, 40, 961, 108, 263, 230, 467, 782, 935, 592, 337, 701, 470, 1001, 1009, 384, 945, 39, 890, 264, 623, 26, 429, 515, 724, 774, 107, 20, 190, 108, 263, 1039, 365, 851, 634, 984, 448, 840, 505, 781, 26, 429, 1037, 108, 854, 652, 115, 890, 580, 599, 460, 729, 26, 608, 770, 205, 100, 817, 239, 104, 407, 1043], [1042, 817, 143, 50, 467, 782, 83, 592, 618, 851, 484, 26, 407, 871, 660, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 776, 406, 1001, 59, 961, 429, 638, 946, 323, 108, 242, 463, 396, 634, 67, 1001, 480, 817, 660, 104, 407, 1043], [1042, 195, 782, 83, 592, 85, 108, 696, 303, 832, 851, 634, 984, 448, 840, 885, 26, 407, 798, 302, 912, 1001, 278, 535, 1001, 983, 435, 1001, 461, 310, 344, 776, 406, 1001, 35, 301, 599, 977, 240, 240, 643, 41, 31, 410, 1001, 120, 23, 256, 719, 602, 502, 286, 410, 1001, 148, 303, 256, 416, 23, 572, 851, 461, 310, 190, 448, 432, 925, 1001, 983, 400, 495, 26, 599, 718, 115, 898, 728, 364, 478, 46, 95, 1001, 337, 1039, 815, 441, 851, 998, 554, 333, 470, 753, 728, 719, 882, 595, 807, 745, 423, 269, 851, 230, 984, 54, 969, 300, 190, 1001, 986, 39, 26, 936, 311, 333, 360, 890, 940, 599, 460, 729, 26, 108, 854, 652, 324, 770, 602, 491, 817, 660, 759, 407, 1043], [1042, 817, 696, 935, 1018, 782, 83, 592, 85, 851, 634, 984, 719, 820, 448, 191, 107, 474, 831, 809, 1026, 295, 448, 880, 1001, 644, 1001, 405, 108, 325, 283, 997, 536, 1034, 98, 851, 726, 936, 824, 719, 33, 720, 108, 560, 894, 1001, 385, 42, 851, 120, 815, 408, 829, 954, 855, 284, 719, 371, 198, 353, 167, 38, 775, 15, 712, 292, 420, 674, 719, 930, 1033, 127, 228, 18, 902, 856, 598, 118, 227, 108, 709, 72, 857, 719, 900, 893, 1001, 983, 989, 571, 582, 1006, 821, 1006, 341, 890, 23, 698, 1016, 1001, 983, 813, 84, 238, 890, 542, 704, 623, 983, 520, 80, 111, 1026, 696, 303, 122, 696, 984, 448, 840, 164, 108, 696, 83, 122, 782, 83, 592, 85, 634, 984, 448, 840, 344, 983, 67, 1001, 974, 587, 286, 263, 303, 256, 719, 643, 41, 31, 120, 23, 815, 416, 23, 144, 20, 957, 435, 774, 770, 852, 599, 460, 729, 416, 342, 407, 660, 1001, 461, 310, 344, 776, 974, 817, 239, 643, 759, 407, 1043], [1042, 817, 303, 180, 782, 83, 592, 85, 935, 984, 448, 269, 2, 260, 589, 549, 816, 851, 769, 667, 108, 263, 180, 782, 935, 592, 337, 189, 202, 1001, 41, 719, 1001, 384, 743, 1001, 1017, 425, 814, 470, 1001, 384, 743, 75, 635, 20, 68, 300, 190, 1001, 577, 164, 293, 303, 180, 782, 83, 592, 85, 634, 984, 448, 840, 344, 983, 67, 1001, 643, 41, 31, 120, 23, 256, 719, 974, 502, 286, 263, 303, 815, 416, 23, 296, 448, 298, 719, 144, 20, 957, 297, 774, 770, 852, 599, 460, 729, 26, 871, 407, 660, 1001, 461, 310, 344, 776, 974, 643, 817, 239, 759, 407, 1043], [1042, 817, 263, 42, 707, 782, 83, 592, 85, 696, 984, 448, 269, 381, 377, 915, 441, 20, 957, 435, 108, 935, 905, 782, 83, 592, 85, 554, 333, 470, 381, 753, 719, 548, 248, 31, 461, 310, 499, 190, 1001, 55, 807, 745, 1026, 300, 1001, 93, 944, 164, 108, 263, 42, 707, 782, 83, 592, 85, 634, 984, 448, 840, 983, 67, 1001, 643, 41, 31, 120, 23, 256, 974, 502, 286, 263, 303, 256, 416, 23, 89, 20, 957, 25, 392, 719, 296, 1001, 983, 60, 890, 428, 770, 852, 599, 460, 79, 26, 975, 407, 660, 1001, 461, 310, 344, 776, 974, 434, 381, 643, 817, 239, 759, 407, 1043], [1042, 817, 23, 467, 782, 83, 85, 851, 48, 783, 169, 494, 816, 851, 332, 667, 108, 263, 50, 905, 782, 935, 592, 143, 303, 202, 1001, 532, 719, 1001, 384, 932, 814, 1007, 1001, 384, 261, 62, 807, 671, 20, 969, 300, 190, 1001, 405, 164, 108, 23, 467, 782, 83, 592, 85, 634, 984, 448, 840, 344, 983, 67, 1001, 643, 41, 31, 120, 1039, 256, 719, 1001, 974, 502, 286, 263, 935, 815, 416, 23, 572, 20, 666, 435, 719, 925, 448, 370, 774, 324, 770, 767, 460, 108, 242, 652, 729, 26, 393, 714, 660, 1001, 461, 310, 344, 499, 974, 643, 817, 239, 759, 407, 817, 143, 50, 467, 782, 83, 592, 465, 696, 984, 448, 43, 26, 407, 891, 870, 660, 1001, 461, 310, 344, 499, 429, 638, 719, 323, 108, 242, 463, 1001, 133, 396, 42, 200, 817, 660, 759, 407, 1043], [1042, 448, 192, 935, 394, 249, 448, 19, 1001, 180, 696, 935, 592, 83, 652, 1001, 480, 448, 530, 836, 13, 91, 176, 603, 328, 108, 32, 23, 134, 854, 19, 107, 303, 984, 448, 269, 391, 1001, 710, 607, 615, 1001, 337, 50, 140, 50, 20, 983, 435, 96, 89, 107, 497, 828, 389, 836, 719, 96, 218, 237, 93, 865, 719, 561, 344, 983, 471, 107, 159, 852, 942, 108, 681, 176, 237, 89, 107, 165, 774, 26, 107, 922, 266, 599, 160, 54, 969, 654, 107, 166, 719, 20, 1029, 333, 361, 176, 482, 459, 1026, 922, 166, 719, 386, 151, 645, 89, 107, 652, 719, 599, 808, 729, 217, 109, 942, 159, 260, 649, 239, 1001, 983, 435, 750, 513, 890, 245, 617, 760, 873, 535, 626, 482, 151, 212, 1043]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "579eYIWYLm0h",
        "colab_type": "text"
      },
      "source": [
        "### Model Creation\n",
        "\n",
        "Build our simple model that includes an embedding layer, recurrent layer, and\n",
        "dense layer to get us down to the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnnRRSRvLI9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size=199):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128)\n",
        "        self.gru = tf.keras.layers.GRU(128, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = tf.expand_dims(x, 0)\n",
        "        x = self.gru(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.softmax(x)\n",
        "        x = tf.squeeze(x, 0)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0sp09YtmNIN",
        "colab_type": "text"
      },
      "source": [
        "Test it out just to make sure it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wePrq3sYNFye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1d57600d-00a0-4683-cd2f-732cca7d4b41"
      },
      "source": [
        "model = LanguageModel(vocab_size=199)\n",
        "\n",
        "sequence = tf.constant(np.random.randint(0, 199, size=(100)))\n",
        "output = model(sequence)\n",
        "\n",
        "print('Sequence:', sequence.shape)\n",
        "print('Output:', output.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence: (100,)\n",
            "Output: (100, 199)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix0fE8BVmtJu",
        "colab_type": "text"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "Train the model based on the text in our corpus.\n",
        "\n",
        "The goal is to predict the next character. Thus, the target is the input tensor\n",
        "rolled by one character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50RhKLmCUVgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "39503479-c407-45a2-93d9-5e1ff61b5df4"
      },
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "def process_sentence(sentence, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        output = model(sentence)\n",
        "        loss = loss_fn(target, output)\n",
        "        \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(target, tf.argmax(output, axis=1))\n",
        "\n",
        "epochs = 50\n",
        "dataset = tf.data.Dataset.from_tensor_slices(sentences_tensor)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    train_loop = tqdm(total=len(dataset), position=0, leave=True)\n",
        "    for sentence in dataset:\n",
        "        model.gru.reset_states()\n",
        "\n",
        "        process_sentence(sentence, tf.roll(sentence, -1, 0))\n",
        "        train_loop.set_description('Train - Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch, train_loss.result(), train_accuracy.result()))\n",
        "        train_loop.update(1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train - Epoch: 0, Loss: 3.8689, Accuracy: 0.1690: 100%|██████████| 44/44 [00:04<00:00, 10.61it/s]\n",
            "Train - Epoch: 1, Loss: 2.9965, Accuracy: 0.1984: 100%|██████████| 44/44 [00:01<00:00, 33.77it/s]\n",
            "Train - Epoch: 2, Loss: 2.7309, Accuracy: 0.2724: 100%|██████████| 44/44 [00:01<00:00, 33.65it/s]\n",
            "Train - Epoch: 3, Loss: 2.4409, Accuracy: 0.3254: 100%|██████████| 44/44 [00:01<00:00, 33.39it/s]\n",
            "Train - Epoch: 4, Loss: 2.2543, Accuracy: 0.3645: 100%|██████████| 44/44 [00:01<00:00, 31.72it/s]\n",
            "Train - Epoch: 6, Loss: 2.0014, Accuracy: 0.4358: 100%|██████████| 44/44 [00:01<00:00, 34.09it/s]\n",
            "Train - Epoch: 7, Loss: 1.8863, Accuracy: 0.4674: 100%|██████████| 44/44 [00:01<00:00, 33.45it/s]\n",
            "Train - Epoch: 8, Loss: 1.7707, Accuracy: 0.5109: 100%|██████████| 44/44 [00:01<00:00, 33.22it/s]\n",
            "Train - Epoch: 9, Loss: 1.6587, Accuracy: 0.5438: 100%|██████████| 44/44 [00:01<00:00, 33.36it/s]\n",
            "Train - Epoch: 10, Loss: 1.5501, Accuracy: 0.5774: 100%|██████████| 44/44 [00:01<00:00, 32.95it/s]\n",
            "Train - Epoch: 11, Loss: 1.4486, Accuracy: 0.6066: 100%|██████████| 44/44 [00:01<00:00, 33.27it/s]\n",
            "Train - Epoch: 13, Loss: 1.2766, Accuracy: 0.6539: 100%|██████████| 44/44 [00:01<00:00, 33.44it/s]\n",
            "Train - Epoch: 14, Loss: 1.2046, Accuracy: 0.6745: 100%|██████████| 44/44 [00:01<00:00, 32.98it/s]\n",
            "Train - Epoch: 15, Loss: 1.1399, Accuracy: 0.6925: 100%|██████████| 44/44 [00:01<00:00, 33.10it/s]\n",
            "Train - Epoch: 16, Loss: 1.0819, Accuracy: 0.7085: 100%|██████████| 44/44 [00:01<00:00, 33.13it/s]\n",
            "Train - Epoch: 17, Loss: 1.0300, Accuracy: 0.7231: 100%|██████████| 44/44 [00:01<00:00, 33.58it/s]\n",
            "Train - Epoch: 18, Loss: 0.9833, Accuracy: 0.7356: 100%|██████████| 44/44 [00:01<00:00, 32.50it/s]\n",
            "Train - Epoch: 19, Loss: 0.9415, Accuracy: 0.7469: 100%|██████████| 44/44 [00:01<00:00, 32.97it/s]\n",
            "Train - Epoch: 21, Loss: 0.8692, Accuracy: 0.7637: 100%|██████████| 44/44 [00:01<00:00, 32.34it/s]\n",
            "Train - Epoch: 22, Loss: 0.8378, Accuracy: 0.7716: 100%|██████████| 44/44 [00:01<00:00, 33.53it/s]\n",
            "Train - Epoch: 23, Loss: 0.8096, Accuracy: 0.7789: 100%|██████████| 44/44 [00:01<00:00, 32.98it/s]\n",
            "Train - Epoch: 24, Loss: 0.7859, Accuracy: 0.7846: 100%|██████████| 44/44 [00:01<00:00, 31.39it/s]\n",
            "Train - Epoch: 25, Loss: 0.7686, Accuracy: 0.7885: 100%|██████████| 44/44 [00:01<00:00, 32.54it/s]\n",
            "Train - Epoch: 26, Loss: 0.7524, Accuracy: 0.7909: 100%|██████████| 44/44 [00:01<00:00, 33.62it/s]\n",
            "Train - Epoch: 27, Loss: 0.6471, Accuracy: 0.8140:  18%|█▊        | 8/44 [00:00<00:00, 39.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0d29792185f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train - Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mreplica_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_strategy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreplica_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m       \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0;31m# TODO(psv): Test distribution of metrics using different distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mresult_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mcontrol_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0mag_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mag_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whitelisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM_OVER_BATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     ]:\n\u001b[0;32m--> 401\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_no_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       raise NotImplementedError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mdiv_no_nan\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1343\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"div_no_nan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_no_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[0;34m(var, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m   1907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_dense_var_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_var_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_dense_var_to_tensor\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iadd__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_other\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHdK9tPEskSd",
        "colab_type": "text"
      },
      "source": [
        "### Character-Level Results\n",
        "\n",
        "Observe the results by generating text one character at a time.\n",
        "\n",
        "Run this code block if you chose the character-level dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE3FD01sfk4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "053157e4-c60e-4582-de28-e02cffa4178d"
      },
      "source": [
        "input = tf.constant([197])\n",
        "string_output = ''\n",
        "k = 2\n",
        "model.gru.reset_states()\n",
        "for _ in range(200):  # Max number of iterations\n",
        "    output = model(input)\n",
        "    char_idx = np.random.choice(tf.math.top_k(output, k=k).indices.numpy()[0])\n",
        "    if char_idx == 198:\n",
        "        break\n",
        "    string_output += mapper.idx_to_char(char_idx)\n",
        "    input = tf.constant([char_idx])\n",
        "\n",
        "print(string_output)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c'une dux-sevatier à civellie he Mremen de querarancisquin maite de Stint née àa Marine mandien Avels neur en sept ans, tors apons du secour en,, dé laven apés neufante du sorre et de la cinq hons som\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKRq7fbMBcVd",
        "colab_type": "text"
      },
      "source": [
        "### Word-Level Results\n",
        "\n",
        "Observe the results by generating text one word at a time.\n",
        "\n",
        "Run this code block if you chose the word-level dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoF4jM5jEnIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c97889b-130c-4cf4-dde6-9edf920450c6"
      },
      "source": [
        "input = tf.constant([1042])  # Start token\n",
        "k = 30\n",
        "model.gru.reset_states()\n",
        "sequences = []\n",
        "for _ in range(15):\n",
        "    output = model(input)\n",
        "    char_idx = np.random.choice(tf.math.top_k(output, k=k).indices.numpy()[0])\n",
        "    if char_idx == 1043:\n",
        "        break\n",
        "    sequences.append(char_idx)\n",
        "\n",
        "print(tokenizer.sequences_to_texts([sequences]))"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['huit la mil en deux deux à Francois trente quatre la en deux à cinq']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}