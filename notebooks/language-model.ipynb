{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language-model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLmuNnmbLJAJievIS4gH4P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BYU-Handwriting-Lab/GettingStarted/blob/solution/notebooks/language-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3SVor5MHLtj",
        "colab_type": "text"
      },
      "source": [
        "# Language Model\n",
        "\n",
        "This notebook provides code to create a character-level language model in \n",
        "TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc4bV3ezKMne",
        "colab_type": "text"
      },
      "source": [
        "### Dependencies\n",
        "\n",
        "Import the necessary dependencies and download our character set and corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibhkP7GXGwhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhHYfrFDKh5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/ericburdett/named-entity-recognition/master/char_set.json\n",
        "!wget -q --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ZsJ8cZSDU98GpcK-kl_Cq3eTt-R2YvSJ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ZsJ8cZSDU98GpcK-kl_Cq3eTt-R2YvSJ\" -O french_ner_dataset.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBV9lreFKUuY",
        "colab_type": "text"
      },
      "source": [
        "### Character Set Mapping\n",
        "\n",
        "Create a Character Set Mapper to go between string and integer representations.\n",
        "\n",
        "Specify the starting and ending character token. These are useful when feeding\n",
        "sentences into our language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DhUAyVxHn1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class CharsetMapper():\n",
        "    def __init__(self, filepath='char_set.json', sequence_size=20, start_char=197, end_char=198):\n",
        "        self.start_char = start_char\n",
        "        self.end_char = end_char\n",
        "        with open(filepath) as f:\n",
        "            self.char_dict = json.load(f)\n",
        "    \n",
        "    def char_to_idx(self, char):\n",
        "        if char in self.char_dict['char_to_idx']:\n",
        "            return int(self.char_dict['char_to_idx'][char])\n",
        "        else:\n",
        "            return 0\n",
        "  \n",
        "    def idx_to_char(self, idx):\n",
        "        if str(int(idx)) in self.char_dict['idx_to_char']:\n",
        "            return self.char_dict['idx_to_char'][str(int(idx))]\n",
        "        else:\n",
        "            return ''\n",
        "  \n",
        "    def str_to_idxs(self, string):\n",
        "        assert type(string) == str\n",
        "\n",
        "        idxs = [self.start_char]\n",
        "        for char in string:\n",
        "            idxs.append(self.char_to_idx(char))\n",
        "        idxs.append(self.end_char)\n",
        "\n",
        "        return np.array(idxs)\n",
        "  \n",
        "    def idxs_to_str(self, idxs):\n",
        "        chars = ''\n",
        "\n",
        "        for idx in idxs:\n",
        "            chars += self.idx_to_char(idx)\n",
        "    \n",
        "        return chars"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkzbYsW_liVM",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Creation\n",
        "\n",
        "Create our dataset by reading from the CSV using pandas, joining sentences, and\n",
        "mapping char representations to integer representations.\n",
        "\n",
        "Notice the use of tf.ragged.constant. This allows us to create a tensor with\n",
        "unequal sequence lengths. Without this, we would be forced to use padding so\n",
        "that our sequence lengths would be constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1pPSxg6MUSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mapper = CharsetMapper()\n",
        "\n",
        "df = pd.read_csv('french_ner_dataset.csv', sep='\\t', header=None, names=['word', 'entity', 'id'])\n",
        "df_size = df['id'].max()\n",
        "\n",
        "sentences_str = []\n",
        "sentences = []\n",
        "for i in range(df_size):\n",
        "    ith_sentence_words = df.loc[df['id'] == i]\n",
        "    sentence = \" \".join(ith_sentence_words['word'].to_list())\n",
        "    sentences_str.append(sentence)\n",
        "    sentences.append(mapper.str_to_idxs(sentence))\n",
        "\n",
        "sentences_tensor = tf.ragged.constant(sentences)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "579eYIWYLm0h",
        "colab_type": "text"
      },
      "source": [
        "### Model Creation\n",
        "\n",
        "Build our simple model that includes an embedding layer, recurrent layer, and\n",
        "dense layer to get us down to the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnnRRSRvLI9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size=199):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128)\n",
        "        self.gru = tf.keras.layers.GRU(128, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = tf.expand_dims(x, 0)\n",
        "        x = self.gru(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.softmax(x)\n",
        "        x = tf.squeeze(x, 0)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0sp09YtmNIN",
        "colab_type": "text"
      },
      "source": [
        "Test it out just to make sure it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wePrq3sYNFye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cbb8f5b4-6137-451f-a6cd-f22fa9dc8646"
      },
      "source": [
        "model = LanguageModel()\n",
        "\n",
        "sequence = tf.constant(np.random.randint(0, 197, size=(100)))\n",
        "output = model(sequence)\n",
        "\n",
        "print('Sequence:', sequence.shape)\n",
        "print('Output:', output.shape)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence: (100,)\n",
            "Output: (100, 199)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix0fE8BVmtJu",
        "colab_type": "text"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "Train the model based on the text in our corpus.\n",
        "\n",
        "The goal is to predict the next character. Thus, the target is the input tensor\n",
        "rolled by one character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50RhKLmCUVgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "23d5e9b7-bf9f-463c-bcfa-168d8112b336"
      },
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "def process_sentence(sentence, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        output = model(sentence)\n",
        "        loss = loss_fn(target, output)\n",
        "        \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(target, tf.argmax(output, axis=1))\n",
        "\n",
        "epochs = 50\n",
        "dataset = tf.data.Dataset.from_tensor_slices(sentences_tensor)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    train_loop = tqdm(total=len(dataset), position=0, leave=True)\n",
        "    for sentence in dataset:\n",
        "        model.gru.reset_states()\n",
        "\n",
        "        process_sentence(sentence, tf.roll(sentence, -1, 0))\n",
        "        train_loop.set_description('Train - Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch, train_loss.result(), train_accuracy.result()))\n",
        "        train_loop.update(1)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train - Epoch: 0, Loss: 3.8581, Accuracy: 0.1571: 100%|██████████| 44/44 [00:03<00:00, 11.67it/s]\n",
            "Train - Epoch: 1, Loss: 2.9793, Accuracy: 0.1934: 100%|██████████| 44/44 [00:01<00:00, 35.21it/s]\n",
            "Train - Epoch: 2, Loss: 2.7175, Accuracy: 0.2761: 100%|██████████| 44/44 [00:01<00:00, 37.74it/s]\n",
            "Train - Epoch: 4, Loss: 2.2863, Accuracy: 0.3547: 100%|██████████| 44/44 [00:01<00:00, 38.00it/s]\n",
            "Train - Epoch: 5, Loss: 2.1470, Accuracy: 0.3899: 100%|██████████| 44/44 [00:01<00:00, 36.67it/s]\n",
            "Train - Epoch: 6, Loss: 2.0215, Accuracy: 0.4299: 100%|██████████| 44/44 [00:01<00:00, 37.62it/s]\n",
            "Train - Epoch: 7, Loss: 1.9019, Accuracy: 0.4649: 100%|██████████| 44/44 [00:01<00:00, 34.90it/s]\n",
            "Train - Epoch: 8, Loss: 1.7847, Accuracy: 0.5076: 100%|██████████| 44/44 [00:01<00:00, 34.93it/s]\n",
            "Train - Epoch: 9, Loss: 1.6700, Accuracy: 0.5471: 100%|██████████| 44/44 [00:01<00:00, 36.73it/s]\n",
            "Train - Epoch: 10, Loss: 1.5634, Accuracy: 0.5795: 100%|██████████| 44/44 [00:01<00:00, 34.63it/s]\n",
            "Train - Epoch: 12, Loss: 1.3765, Accuracy: 0.6336: 100%|██████████| 44/44 [00:01<00:00, 37.11it/s]\n",
            "Train - Epoch: 13, Loss: 1.2958, Accuracy: 0.6543: 100%|██████████| 44/44 [00:01<00:00, 34.27it/s]\n",
            "Train - Epoch: 14, Loss: 1.2228, Accuracy: 0.6747: 100%|██████████| 44/44 [00:01<00:00, 33.91it/s]\n",
            "Train - Epoch: 15, Loss: 1.1569, Accuracy: 0.6925: 100%|██████████| 44/44 [00:01<00:00, 35.48it/s]\n",
            "Train - Epoch: 16, Loss: 1.0978, Accuracy: 0.7064: 100%|██████████| 44/44 [00:01<00:00, 36.92it/s]\n",
            "Train - Epoch: 17, Loss: 1.0448, Accuracy: 0.7194: 100%|██████████| 44/44 [00:01<00:00, 37.29it/s]\n",
            "Train - Epoch: 18, Loss: 0.9971, Accuracy: 0.7302: 100%|██████████| 44/44 [00:01<00:00, 36.08it/s]\n",
            "Train - Epoch: 19, Loss: 0.9538, Accuracy: 0.7403: 100%|██████████| 44/44 [00:01<00:00, 35.51it/s]\n",
            "Train - Epoch: 21, Loss: 0.8790, Accuracy: 0.7600: 100%|██████████| 44/44 [00:01<00:00, 37.56it/s]\n",
            "Train - Epoch: 22, Loss: 0.8471, Accuracy: 0.7690: 100%|██████████| 44/44 [00:01<00:00, 36.32it/s]\n",
            "Train - Epoch: 23, Loss: 0.8186, Accuracy: 0.7758: 100%|██████████| 44/44 [00:01<00:00, 36.37it/s]\n",
            "Train - Epoch: 24, Loss: 0.7927, Accuracy: 0.7818: 100%|██████████| 44/44 [00:01<00:00, 37.58it/s]\n",
            "Train - Epoch: 25, Loss: 0.7670, Accuracy: 0.7882: 100%|██████████| 44/44 [00:01<00:00, 35.86it/s]\n",
            "Train - Epoch: 26, Loss: 0.7430, Accuracy: 0.7944: 100%|██████████| 44/44 [00:01<00:00, 36.78it/s]\n",
            "Train - Epoch: 27, Loss: 0.7215, Accuracy: 0.7989: 100%|██████████| 44/44 [00:01<00:00, 37.53it/s]\n",
            "Train - Epoch: 29, Loss: 0.6927, Accuracy: 0.8049: 100%|██████████| 44/44 [00:01<00:00, 37.52it/s]\n",
            "Train - Epoch: 30, Loss: 0.6870, Accuracy: 0.8054: 100%|██████████| 44/44 [00:01<00:00, 36.56it/s]\n",
            "Train - Epoch: 31, Loss: 0.6633, Accuracy: 0.8123: 100%|██████████| 44/44 [00:01<00:00, 36.72it/s]\n",
            "Train - Epoch: 32, Loss: 0.6391, Accuracy: 0.8182: 100%|██████████| 44/44 [00:01<00:00, 37.03it/s]\n",
            "Train - Epoch: 33, Loss: 0.6233, Accuracy: 0.8227: 100%|██████████| 44/44 [00:01<00:00, 37.40it/s]\n",
            "Train - Epoch: 34, Loss: 0.6109, Accuracy: 0.8261: 100%|██████████| 44/44 [00:01<00:00, 36.66it/s]\n",
            "Train - Epoch: 35, Loss: 0.5991, Accuracy: 0.8291: 100%|██████████| 44/44 [00:01<00:00, 34.97it/s]\n",
            "Train - Epoch: 37, Loss: 0.5768, Accuracy: 0.8333: 100%|██████████| 44/44 [00:01<00:00, 37.49it/s]\n",
            "Train - Epoch: 38, Loss: 0.5667, Accuracy: 0.8364: 100%|██████████| 44/44 [00:01<00:00, 36.17it/s]\n",
            "Train - Epoch: 39, Loss: 0.5563, Accuracy: 0.8375: 100%|██████████| 44/44 [00:01<00:00, 37.29it/s]\n",
            "Train - Epoch: 40, Loss: 0.5453, Accuracy: 0.8406: 100%|██████████| 44/44 [00:01<00:00, 36.02it/s]\n",
            "Train - Epoch: 41, Loss: 0.5357, Accuracy: 0.8433: 100%|██████████| 44/44 [00:01<00:00, 35.48it/s]\n",
            "Train - Epoch: 42, Loss: 0.5279, Accuracy: 0.8457: 100%|██████████| 44/44 [00:01<00:00, 37.24it/s]\n",
            "Train - Epoch: 43, Loss: 0.5227, Accuracy: 0.8470: 100%|██████████| 44/44 [00:01<00:00, 37.11it/s]\n",
            "Train - Epoch: 44, Loss: 0.5174, Accuracy: 0.8484: 100%|██████████| 44/44 [00:01<00:00, 37.07it/s]\n",
            "Train - Epoch: 46, Loss: 0.5309, Accuracy: 0.8425: 100%|██████████| 44/44 [00:01<00:00, 36.30it/s]\n",
            "Train - Epoch: 47, Loss: 0.5272, Accuracy: 0.8430: 100%|██████████| 44/44 [00:01<00:00, 37.21it/s]\n",
            "Train - Epoch: 48, Loss: 0.5082, Accuracy: 0.8478: 100%|██████████| 44/44 [00:01<00:00, 36.58it/s]\n",
            "Train - Epoch: 49, Loss: 0.4876, Accuracy: 0.8546:  98%|█████████▊| 43/44 [00:01<00:00, 36.20it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHdK9tPEskSd",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "\n",
        "Observe the results by generating text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE3FD01sfk4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f550794-0477-4328-a958-ad8ae9aa014b"
      },
      "source": [
        "input = tf.constant([197])\n",
        "string_output = ''\n",
        "k = 1\n",
        "model.gru.reset_states()\n",
        "for _ in range(200):  # Max number of iterations\n",
        "    output = model(input)\n",
        "    char_idx = np.random.choice(tf.math.top_k(output, k=k).indices.numpy()[0])\n",
        "    if char_idx == 198:\n",
        "        break\n",
        "    string_output += mapper.idx_to_char(char_idx)\n",
        "    input = tf.constant([char_idx])\n",
        "\n",
        "print(string_output)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L'an mil neuf cent deux heures du matin pardevant nous Bonnefond Francisque Maire et officier de l'etat civil de la commune de St Cyr-sur-Rhône canton de Condrieu arrondissement de Lyon département du\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}