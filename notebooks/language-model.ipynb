{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language-model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxSFB14xEUWZfVn1LUO+KO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BYU-Handwriting-Lab/GettingStarted/blob/solution/notebooks/language-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3SVor5MHLtj"
      },
      "source": [
        "# Language Model\n",
        "\n",
        "This notebook provides code to create a character-level language model in \n",
        "TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc4bV3ezKMne"
      },
      "source": [
        "### Dependencies\n",
        "\n",
        "Import the necessary dependencies and download our character set and corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibhkP7GXGwhI"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhHYfrFDKh5I"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/ericburdett/named-entity-recognition/master/char_set.json\n",
        "!wget -q --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ZsJ8cZSDU98GpcK-kl_Cq3eTt-R2YvSJ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ZsJ8cZSDU98GpcK-kl_Cq3eTt-R2YvSJ\" -O french_ner_dataset.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBgarB6HN_-9"
      },
      "source": [
        "# ID: 1M26Gpca8Ug4YvRLxoUDDCjMBeJtojITY\n",
        "!wget -q --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1wDMLz9hTmfvPhkhCHTylbeAU6Utpkqb1' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1wDMLz9hTmfvPhkhCHTylbeAU6Utpkqb1\" -O french_text.txt && rm -rf /tmp/cookies.tx"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEOHweMX3i-a"
      },
      "source": [
        "## Load the Corpus\n",
        "\n",
        "Define some constants to help us know which characters are used for words and\n",
        "which are used for punctuation/digits.\n",
        "\n",
        "Load the corpus to be used for tokenization and dataset creation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moYNpZedrzLG"
      },
      "source": [
        "DEFAULT_CHARS = ' !\"#$%&\\'()*+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz|~£§¨«¬\\xad' \\\n",
        "                '°²´·º»¼½¾ÀÂÄÇÈÉÊÔÖÜßàáâäæçèéêëìîïñòóôöøùúûüÿłŒœΓΖΤάήαδεηικλμνξοπρτυχψωόώІ‒–—†‡‰‹›₂₤℔⅓⅔⅕⅖⅗⅘⅙⅚⅛∆∇∫≠□♀♂✓ｆ'\n",
        "# The default list of non-punctuation characters needed for the word beam search decoding algorithm\n",
        "DEFAULT_NON_PUNCTUATION = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÀÂÄÇÈÉÊÔÖÜßàáâäæçèéêëìîïñòóôöøùúûüÿ' \\\n",
        "                          'łŒœΓΖΤάήαδεηικλμνξοπρτυχψωόώІ'\n",
        "\n",
        "DEFAULT_PUNCTUATION = string.punctuation + '0123456789'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zibqC-fN9DM"
      },
      "source": [
        "lines = open('french_text.txt', 'r', encoding='utf8').readlines()\n",
        "\n",
        "french_words = []\n",
        "for line in lines:\n",
        "    french_words.extend(line.split())\n",
        "french_words = ' '.join(french_words)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv7qG-xbv4Ll"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "One of the hardest parts is creating a good tokenization method.\n",
        "\n",
        "This tokenizer will create a token for each word. Each punctuation or digit\n",
        "character will have its own token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsm67koTOF8I"
      },
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, corpus, word_chars, punctuation, lower=False):\n",
        "        self.word_chars = word_chars\n",
        "        self.punctuation = punctuation\n",
        "        self.regex = r\"[\" + self.word_chars + r\"]+|[^\\s]\" \n",
        "\n",
        "        words = self.split(corpus)\n",
        "        all_words_list = words + list(punctuation)\n",
        "        all_words_list_unique = list(set(all_words_list))\n",
        "        all_words = [' '.join(all_words_list_unique)]\n",
        "\n",
        "        self.total_tokens = len(all_words_list_unique) + 2 # +2 to account for 0 (reserved) and 1 (OOV)\n",
        "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=self.total_tokens, filters='', lower=lower, oov_token='<OOV>')\n",
        "        self.tokenizer.fit_on_texts(all_words)\n",
        "\n",
        "    def split(self, text):\n",
        "        return re.findall(self.regex, text)\n",
        "\n",
        "    def texts_to_sequences(self, text):\n",
        "        words = self.split(text)\n",
        "        return self.tokenizer.texts_to_sequences([' '.join(words)])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktU759SWQ3Ov",
        "outputId": "5efc0760-00ad-4a03-9bfc-cf78f3590949",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer = Tokenizer(french_words, DEFAULT_NON_PUNCTUATION, DEFAULT_PUNCTUATION, lower=False)\n",
        "sentence = 'acte de deces-de..1832(hello)5eme'\n",
        "\n",
        "print('Original Sentence:', sentence)\n",
        "print('Split Sentence:', tokenizer.split(sentence))\n",
        "print('Tokenized Sentence:', tokenizer.texts_to_sequences(sentence))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Sentence: acte de deces-de..1832(hello)5eme\n",
            "Split Sentence: ['acte', 'de', 'deces', '-', 'de', '.', '.', '1', '8', '3', '2', '(', 'hello', ')', '5', 'eme']\n",
            "Tokenized Sentence: [[33383, 33521, 3575, 38751, 33521, 23647, 23647, 17975, 4601, 457, 31340, 41614, 1, 5024, 33450, 41406]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAhBZVvgwwVX",
        "outputId": "6b7f7242-507c-4995-964a-6c0ea9f73fce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedding = tf.keras.layers.Embedding(tokenizer.total_tokens, 1024)\n",
        "\n",
        "sequence = tokenizer.texts_to_sequences(sentence)\n",
        "sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=1)\n",
        "\n",
        "tf.squeeze(embedding(sequence))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
              "array([-0.01646097,  0.03190211, -0.02221891, ...,  0.02684306,\n",
              "        0.02210755,  0.01925501], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122a22FM3SyZ"
      },
      "source": [
        "## Dataset Creation\n",
        "\n",
        "Create the Tensorflow dataset using the tokenizer created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py6Kz97fqYgu"
      },
      "source": [
        "def create_context_pairs(words, window_size, negative_sample_size):\n",
        "    focus_words = []\n",
        "    context_words = []\n",
        "    labels = []\n",
        "\n",
        "    # Add positive samples\n",
        "    for index in tqdm(range(len(words)), desc='Positive Samples'):\n",
        "\n",
        "        # Grab words to the left:\n",
        "        for i in range(1, window_size + 1):\n",
        "            left_index = index - i\n",
        "            if left_index >= 0:\n",
        "                focus_words.append(words[index])\n",
        "                context_words.append(words[left_index])\n",
        "                labels.append(1)\n",
        "        \n",
        "        # Grab words to the right:\n",
        "        for i in range(1, window_size + 1):\n",
        "            right_index = index + i\n",
        "            if right_index < len(words):\n",
        "                focus_words.append(words[index])\n",
        "                context_words.append(words[right_index])\n",
        "                labels.append(1)               \n",
        "    \n",
        "    # Add negative samples\n",
        "    for word in tqdm(words, desc='Negative Samples'):\n",
        "        for i in range(negative_sample_size):\n",
        "            index = random.randint(0, len(words) - 1)\n",
        "            focus_words.append(word)\n",
        "            context_words.append(words[index])\n",
        "            labels.append(0)\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    print('Shuffling dataset...')\n",
        "    zipped = list(zip(focus_words, context_words, labels))\n",
        "    random.shuffle(zipped)\n",
        "    focus_words, context_words, labels = zip(*zipped)\n",
        "    print('Done.')\n",
        "    \n",
        "    return tf.constant(focus_words, dtype=tf.int32), tf.constant(context_words, dtype=tf.int32), tf.constant(labels, dtype=tf.int32)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWMSAV8g4yUI",
        "outputId": "6f396369-d43e-4157-ff7d-e52272befcf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Tokenize the entire corpus\n",
        "tokenized_french_words = tokenizer.texts_to_sequences(french_words)[0]\n",
        "\n",
        "# Create pairs of words that should be similar\n",
        "focus_words, context_words, labels = create_context_pairs(tokenized_french_words, 3, 5)\n",
        "\n",
        "# Create the dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((focus_words, context_words, labels))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Samples: 100%|██████████| 1086647/1086647 [00:03<00:00, 330448.60it/s]\n",
            "Negative Samples: 100%|██████████| 1086647/1086647 [00:10<00:00, 105845.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shuffling dataset...\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKcfhDSl7Wkj",
        "outputId": "e8eac0a4-0546-4500-d410-48d15c18e49a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Show one batch of 100 words\n",
        "for focus_word, context_word, label in dataset.batch(100).take(1):\n",
        "    print(focus_word)\n",
        "    print(context_word)\n",
        "    print(label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 6220 32265 26996 36247  8593  3543 25441 38751 32265 40117 32644 28966\n",
            " 32265 21783  9741 33521 38377  8094 38196  2280  9971 32051 33521 34232\n",
            " 17770 41328  7097 27098  8718 23690 12659 41968 23258 38967  9971 17975\n",
            " 17401 12131  9971 19565 17015  3575  5088 17975 28540 10746  6281 21050\n",
            "  7413 30593 18125  6675 25859 32644  9230 29035 14308 42149 14251 16043\n",
            " 17975 24481 32265 38967 38196 30593 33521 24097 28668  7477 14768 36029\n",
            " 28035 23690  7363  1903 32265 31127  9035 41309 19559 33521 24322 25936\n",
            " 11710 36247 17091  6194  9971 16798  2966 33521 38751 21050 21375  3575\n",
            " 16756 41113 38377 33450], shape=(100,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[12994 27098 14503 28966 35770  8008 26223 20821 21783  4099 20439 38196\n",
            "  6220 25859 33042 10104 19681 18125 41490 27454 32644 37856 37651 19795\n",
            "   736  7477  7363 21050 16019 15099  9971 33616 38196 26009 23690 40825\n",
            " 33521  8805  5659 29035  4104 41328 19735   593 14267 31831  1430  3584\n",
            " 31340 17975 25080 27609 22645 38377 19565 25859  6220 12605 22025 27098\n",
            " 25441 23003 32265 10541  7351 17975 38377 21050 18125  1903 33584 28849\n",
            "  5267 24807 27026 28966 16798 41328  1833 35770 39640 34481 36623  8095\n",
            " 32515 37818 18800 13669 32265 33521 14423 33521  3442 34690 25859 33521\n",
            " 19559 33521 32265 33450], shape=(100,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0\n",
            " 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0\n",
            " 0 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1], shape=(100,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "579eYIWYLm0h"
      },
      "source": [
        "### Model Creation\n",
        "\n",
        "Build our simple model that includes an embedding layer, recurrent layer, and\n",
        "dense layer to get us down to the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnnRRSRvLI9E"
      },
      "source": [
        "class LanguageModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim=128):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "        self.context = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "        self.dot = tf.keras.layers.Dot(axes=(1,1))\n",
        "    \n",
        "    def call(self, target, context, training=False):\n",
        "        if training:\n",
        "            target = self.embedding(target)\n",
        "            context = self.context(context)\n",
        "            dot = self.dot([target, context])\n",
        "            out = tf.keras.activations.sigmoid(dot)\n",
        "        else:\n",
        "            target = self.embedding(target)\n",
        "            context = self.embedding(context)\n",
        "            dot = self.dot([target, context])\n",
        "            out = tf.keras.activations.sigmoid(dot)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0sp09YtmNIN"
      },
      "source": [
        "Test it out just to make sure it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy8MOv64kKQo",
        "outputId": "80fb12bf-423a-4ac2-d355-07f4383c459d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = LanguageModel(tokenizer.total_tokens)\n",
        "\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "for focus_word, context_word, label in dataset.shuffle(5000).batch(5).take(1):\n",
        "    output = model(focus_word, context_word)\n",
        "    loss = loss_fn(label, output)\n",
        "    print('Output Similarity Predictions:', output.numpy())\n",
        "    print('Output Similarity Actual', label.numpy())\n",
        "    print('Loss:', loss.numpy())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output Similarity Predictions: [[0.50147766]\n",
            " [0.49982056]\n",
            " [0.4956222 ]\n",
            " [0.49872485]\n",
            " [0.49849376]]\n",
            "Output Similarity Actual [1 1 1 1 0]\n",
            "Loss: 0.69429654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix0fE8BVmtJu"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "Train the model based on the text in our corpus.\n",
        "\n",
        "The goal is to predict the next character. Thus, the target is the input tensor\n",
        "rolled by one character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUeShyDoNqq7",
        "outputId": "c28911c8-fa16-4c3b-ad09-9c379480a8f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset.cardinality().numpy() * 0.8"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9562484.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50RhKLmCUVgA"
      },
      "source": [
        "TRAIN_SPLIT_SIZE = 1.0\n",
        "SHUFFLE_SIZE = 100_000\n",
        "BATCH_SIZE = 25_000\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 5e-3\n",
        "\n",
        "\n",
        "dataset_size = dataset.cardinality().numpy()\n",
        "train_dataset_size = int(dataset_size * TRAIN_SPLIT_SIZE)\n",
        "val_dataset_size = dataset_size - train_dataset_size\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = dataset.take(train_dataset_size)\\\n",
        "                    .shuffle(SHUFFLE_SIZE)\\\n",
        "                    .batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = dataset.skip(train_dataset_size)\\\n",
        "                    .batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "# model = LanguageModel(tokenizer.total_tokens)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "@tf.function\n",
        "def train_step(focus_word, context_word, label):\n",
        "    with tf.GradientTape() as tape:\n",
        "        output = model(focus_word, context_word, training=True)\n",
        "        loss = loss_fn(label, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def val_step(focus_word, context_word, label):\n",
        "    output = model(focus_word, context_word, training=True)\n",
        "    loss = loss_fn(label, output)\n",
        "    val_loss(loss)\n",
        "\n",
        "\n",
        "# Main Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss.reset_states()\n",
        "    val_loss.reset_states()\n",
        "\n",
        "    # Train Loop\n",
        "    train_loop = tqdm(total=train_dataset.cardinality().numpy(), position=0, leave=True)\n",
        "    for focus_word, context_word, label in train_dataset:\n",
        "        train_step(focus_word, context_word, label)\n",
        "        train_loop.set_description('Train - Epoch: {}, Loss: {:.4f}'.format(epoch, train_loss.result()))\n",
        "        train_loop.update(1)\n",
        "    train_loop.close()\n",
        "\n",
        "    # Validation Loop\n",
        "    val_loop = tqdm(total=val_dataset.cardinality().numpy(), position=0, leave=True)\n",
        "    for focus_word, context_word, label in val_dataset:\n",
        "        val_step(focus_word, context_word, label)\n",
        "        val_loop.set_description('Val   - Epoch: {}, Loss: {:.4f}'.format(epoch, val_loss.result()))\n",
        "        val_loop.update(1)\n",
        "    val_loop.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1KDZK0Qb01X",
        "outputId": "04098112-f05d-4e8f-e83d-2fcb3e916476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.keras.layers.Embedding(tokenizer.total_tokens, output_dim=512, input_length=2)(tf.constant([1255, 28966], dtype=tf.int32))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
              "array([[ 0.01976549,  0.04826181, -0.01614144, ..., -0.0477993 ,\n",
              "         0.00367948, -0.00162051],\n",
              "       [ 0.02253098,  0.03827525, -0.03255246, ...,  0.00961969,\n",
              "         0.04651088, -0.018433  ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fzbKSmWXszM",
        "outputId": "9bc1f11a-8f5e-4951-947d-7da5e8040f7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.texts_to_sequences('jean mary')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[22097, 7923]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRhWG3MeYlec",
        "outputId": "026c3c2b-ecbb-4644-89e8-44da70cae3f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loss_fn([0], [0.1])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.10536041>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_AQAkRRYFAT",
        "outputId": "1d4fae17-ebbd-4832-8931-dcf353818b8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model(tf.constant([22097]), tf.constant([7923]))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.99998677]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMUjFfBaXjlU"
      },
      "source": [
        "tokenizer.texts_to_sequences([['Jean', 'Mary']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cv57ZZZXcv8"
      },
      "source": [
        "model(tokenizer.t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHdK9tPEskSd"
      },
      "source": [
        "### Character-Level Results\n",
        "\n",
        "Observe the results by generating text one character at a time.\n",
        "\n",
        "Run this code block if you chose the character-level dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE3FD01sfk4G",
        "outputId": "053157e4-c60e-4582-de28-e02cffa4178d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = tf.constant([197])\n",
        "string_output = ''\n",
        "k = 2\n",
        "model.gru.reset_states()\n",
        "for _ in range(200):  # Max number of iterations\n",
        "    output = model(input)\n",
        "    char_idx = np.random.choice(tf.math.top_k(output, k=k).indices.numpy()[0])\n",
        "    if char_idx == 198:\n",
        "        break\n",
        "    string_output += mapper.idx_to_char(char_idx)\n",
        "    input = tf.constant([char_idx])\n",
        "\n",
        "print(string_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c'une dux-sevatier à civellie he Mremen de querarancisquin maite de Stint née àa Marine mandien Avels neur en sept ans, tors apons du secour en,, dé laven apés neufante du sorre et de la cinq hons som\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKRq7fbMBcVd"
      },
      "source": [
        "### Word-Level Results\n",
        "\n",
        "Observe the results by generating text one word at a time.\n",
        "\n",
        "Run this code block if you chose the word-level dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoF4jM5jEnIb",
        "outputId": "4c97889b-130c-4cf4-dde6-9edf920450c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input = tf.constant([1042])  # Start token\n",
        "k = 30\n",
        "model.gru.reset_states()\n",
        "sequences = []\n",
        "for _ in range(15):\n",
        "    output = model(input)\n",
        "    char_idx = np.random.choice(tf.math.top_k(output, k=k).indices.numpy()[0])\n",
        "    if char_idx == 1043:\n",
        "        break\n",
        "    sequences.append(char_idx)\n",
        "\n",
        "print(tokenizer.sequences_to_texts([sequences]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['huit la mil en deux deux à Francois trente quatre la en deux à cinq']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}