{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language-model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2UE6tZLzmdMOPJo0AWUOw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BYU-Handwriting-Lab/GettingStarted/blob/master/notebooks/language-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3SVor5MHLtj",
        "colab_type": "text"
      },
      "source": [
        "# Language Model\n",
        "\n",
        "This notebook provides code to create a character-level language model in \n",
        "TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc4bV3ezKMne",
        "colab_type": "text"
      },
      "source": [
        "### Dependencies\n",
        "\n",
        "Import the necessary dependencies and download our character set and corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibhkP7GXGwhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhHYfrFDKh5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/ericburdett/named-entity-recognition/master/char_set.json\n",
        "!wget -q --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ZsJ8cZSDU98GpcK-kl_Cq3eTt-R2YvSJ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ZsJ8cZSDU98GpcK-kl_Cq3eTt-R2YvSJ\" -O french_ner_dataset.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBV9lreFKUuY",
        "colab_type": "text"
      },
      "source": [
        "### Character Set Mapping\n",
        "\n",
        "Create a Character Set Mapper to go between string and integer representations.\n",
        "\n",
        "Specify the starting and ending character token. These are useful when feeding\n",
        "sentences into our language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DhUAyVxHn1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class CharsetMapper():\n",
        "    def __init__(self, filepath='char_set.json', sequence_size=20, start_char=197, end_char=198):\n",
        "        self.start_char = start_char\n",
        "        self.end_char = end_char\n",
        "        with open(filepath) as f:\n",
        "            self.char_dict = json.load(f)\n",
        "    \n",
        "    def char_to_idx(self, char):\n",
        "        if char in self.char_dict['char_to_idx']:\n",
        "            return int(self.char_dict['char_to_idx'][char])\n",
        "        else:\n",
        "            return 0\n",
        "  \n",
        "    def idx_to_char(self, idx):\n",
        "        if str(int(idx)) in self.char_dict['idx_to_char']:\n",
        "            return self.char_dict['idx_to_char'][str(int(idx))]\n",
        "        else:\n",
        "            return ''\n",
        "  \n",
        "    def str_to_idxs(self, string):\n",
        "        assert type(string) == str\n",
        "\n",
        "        idxs = [self.start_char]\n",
        "        for char in string:\n",
        "            idxs.append(self.char_to_idx(char))\n",
        "        idxs.append(self.end_char)\n",
        "\n",
        "        return np.array(idxs)\n",
        "  \n",
        "    def idxs_to_str(self, idxs):\n",
        "        chars = ''\n",
        "\n",
        "        for idx in idxs:\n",
        "            chars += self.idx_to_char(idx)\n",
        "    \n",
        "        return chars"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkzbYsW_liVM",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Creation\n",
        "\n",
        "Create our dataset by reading from the CSV using pandas, joining sentences, and\n",
        "mapping char representations to integer representations.\n",
        "\n",
        "Notice the use of tf.ragged.constant. This allows us to create a tensor with\n",
        "unequal sequence lengths. Without this, we would be forced to use padding so\n",
        "that our sequence lengths would be constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1pPSxg6MUSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mapper = CharsetMapper()\n",
        "\n",
        "df = pd.read_csv('french_ner_dataset.csv', sep='\\t', header=None, names=['word', 'entity', 'id'])\n",
        "df_size = df['id'].max()\n",
        "\n",
        "sentences_str = []\n",
        "sentences = []\n",
        "for i in range(df_size):\n",
        "    ith_sentence_words = df.loc[df['id'] == i]\n",
        "    sentence = \" \".join(ith_sentence_words['word'].to_list())\n",
        "    sentences_str.append(sentence)\n",
        "    sentences.append(mapper.str_to_idxs(sentence))\n",
        "\n",
        "sentences_tensor = tf.ragged.constant(sentences)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "579eYIWYLm0h",
        "colab_type": "text"
      },
      "source": [
        "### Model Creation\n",
        "\n",
        "Build our simple model that includes an embedding layer, recurrent layer, and\n",
        "dense layer to get us down to the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnnRRSRvLI9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size=199):\n",
        "        pass\n",
        "    \n",
        "    def call(self, x):\n",
        "        pass"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0sp09YtmNIN",
        "colab_type": "text"
      },
      "source": [
        "Test it out just to make sure it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wePrq3sYNFye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cbb8f5b4-6137-451f-a6cd-f22fa9dc8646"
      },
      "source": [
        "model = LanguageModel()\n",
        "\n",
        "sequence = tf.constant(np.random.randint(0, 197, size=(100)))\n",
        "output = model(sequence)\n",
        "\n",
        "print('Sequence:', sequence.shape)\n",
        "print('Output:', output.shape)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence: (100,)\n",
            "Output: (100, 199)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix0fE8BVmtJu",
        "colab_type": "text"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "Train the model based on the text in our corpus.\n",
        "\n",
        "The goal is to predict the next character. Thus, the target is the input tensor\n",
        "rolled by one character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50RhKLmCUVgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "def process_sentence(sentence, target):\n",
        "    output = pass\n",
        "    loss = pass\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(target, tf.argmax(output, 1))\n",
        "\n",
        "epochs = 50\n",
        "dataset = pass\n",
        "loss_fn = pass\n",
        "optimizer = pass\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Accuracy(name='train_accuracy')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    train_loop = tqdm(total=len(dataset), position=0, leave=True)\n",
        "    for sentence in dataset:\n",
        "        model.gru.reset_states()\n",
        "\n",
        "        process_sentence(sentence, tf.roll(sentence, -1, 0))\n",
        "        train_loop.set_description('Train - Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch, train_loss.result(), train_accuracy.result()))\n",
        "        train_loop.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHdK9tPEskSd",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "\n",
        "Observe the results by generating text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE3FD01sfk4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56aab02c-9fa8-4108-c11a-683df1ea56b2"
      },
      "source": [
        "input = tf.constant([197]) # First input is the starting character token\n",
        "string_output = ''\n",
        "# More Stuff...\n",
        "\n",
        "for _ in range(200):  # Max number of iterations\n",
        "    pass\n",
        "\n",
        "print('Generated Text:', string_output)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated Text: \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}