{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HWRActiveTransferLearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPofvQkN1ufT7xU/kM3P49N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BYU-Handwriting-Lab/GettingStarted/blob/master/notebooks/HWRActiveTransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCYwlAtU1xbC"
      },
      "source": [
        "# Active Transfer Learning for Handwriting Recognition\n",
        "\n",
        "This notebook contains code to combine active and transfer learning into a\n",
        "cohesive framework. The hope is to allow for new handwriting datasets to be\n",
        "fitted quickly.\n",
        "\n",
        "In this notebook, we will start the process of fitting the Washington dataset\n",
        "when our list of pre-trained models were trained on the IAM, Rimes, and Bentham\n",
        "datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqyKe20vLq4Z"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "First, let's import our dependencies such as Tensorflow, numpy, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5vaA6qQ1xFj"
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as kl\n",
        "import tensorflow.keras.constraints as kc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8yHuublJyfb"
      },
      "source": [
        "## Download Datasets and Pre-trained Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHqNg1tqJx-5"
      },
      "source": [
        "# IAM HWR LINE LEVEL MODEL\n",
        "# ID: 1LmI0kZLGCwE3uD34AbTsmOilvRaWLoIW\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1LmI0kZLGCwE3uD34AbTsmOilvRaWLoIW' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1LmI0kZLGCwE3uD34AbTsmOilvRaWLoIW\" -O iam_hwr_line_level_model.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip iam_hwr_line_level_model.zip\n",
        "!rm iam_hwr_line_level_model.zip\n",
        "\n",
        "# RIMES HWR LINE LEVEL MODEL\n",
        "# ID: 1MJgNMILauGNfw8fQpEj00pJc6iPDG1uE\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1MJgNMILauGNfw8fQpEj00pJc6iPDG1uE' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1MJgNMILauGNfw8fQpEj00pJc6iPDG1uE\" -O rimes_hwr_line_level_model.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip iam_hwr_line_level_model.zip\n",
        "!rm iam_hwr_line_level_model.zip\n",
        "\n",
        "# BENTHAM HWR LINE LEVEL MODEL\n",
        "# ID: 1rBCjaLGX0kXjeF4Gwg6JjE3PC5Z_QDq2\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1rBCjaLGX0kXjeF4Gwg6JjE3PC5Z_QDq2' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1rBCjaLGX0kXjeF4Gwg6JjE3PC5Z_QDq2\" -O bentham_hwr_line_level_model.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip iam_hwr_line_level_model.zip\n",
        "!rm iam_hwr_line_level_model.zip\n",
        "\n",
        "# WASHINGTON HWR LINE LEVEL MODEL\n",
        "# ID: 10ia9GA1j0KY48CvQRUds2c-nbi1AeQAm\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=10ia9GA1j0KY48CvQRUds2c-nbi1AeQAm' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=10ia9GA1j0KY48CvQRUds2c-nbi1AeQAm\" -O washington_hwr_line_level_model.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip iam_hwr_line_level_model.zip\n",
        "!rm iam_hwr_line_level_model.zip\n",
        "\n",
        "\n",
        "# IAM HWR LINE LEVEL DATASET\n",
        "# ID: 1ProJyrkE9NhflpYG0nQTfp-ZSvKLyVS1\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ProJyrkE9NhflpYG0nQTfp-ZSvKLyVS1' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ProJyrkE9NhflpYG0nQTfp-ZSvKLyVS1\" -O iam_hwr_line_level.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip iam_hwr_line_level.zip\n",
        "!rm iam_hwr_line_level.zip\n",
        "\n",
        "# RIMES HWR LINE LEVEL DATASET\n",
        "# ID: 1baqdK4RJuxvlPErxkFipYaK1-5-SpEww\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1baqdK4RJuxvlPErxkFipYaK1-5-SpEww' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1baqdK4RJuxvlPErxkFipYaK1-5-SpEww\" -O rimes_hwr_line_level.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip rimes_hwr_line_level.zip\n",
        "!rm rimes_hwr_line_level.zip\n",
        "\n",
        "# BENTHAM HWR LINE LEVEL DATASET\n",
        "# ID: 1rFyNkYCkaMG9Mo3d8AFXaLe1fh6Hm3nF\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1rFyNkYCkaMG9Mo3d8AFXaLe1fh6Hm3nF' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1rFyNkYCkaMG9Mo3d8AFXaLe1fh6Hm3nF\" -O bentham_hwr_line_level.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip bentham_hwr_line_level.zip\n",
        "!rm bentham_hwr_line_level.zip\n",
        "!rm -r __MACOSX\n",
        "\n",
        "# WASHINGTON HWR LINE LEVEL DATASET\n",
        "# ID: 10WLliclnhq3NnEjrHaNcGvrAnrafg94T\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=10WLliclnhq3NnEjrHaNcGvrAnrafg94T' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=10WLliclnhq3NnEjrHaNcGvrAnrafg94T\" -O washington_hwr_line_level.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip washington_hwr_line_level.zip\n",
        "!rm washington_hwr_line_level.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qOstdHaXB-l",
        "outputId": "71d4d542-ff9d-41bd-d8dc-24bc84d9d154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bentham_hwr_line_level\t  iam_model\t\twashington_hwr_line_level\n",
            "iam_hwr_line_level\t  rimes_hwr_line_level\n",
            "iam_hwr_line_level_model  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSCiJt_YO4Ae"
      },
      "source": [
        "!mv iam_hwr_line_level_model/ iam_model/"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE_wT1BGToWU",
        "outputId": "63e04d5b-a988-499d-e36e-10bf69916a98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.load_weights('iam_model/run1')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f980200f470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnQ5NY45XHKh",
        "outputId": "8c8a1ddf-4eff-4664-d8bf-467179abf5f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f980193c358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXFbTnW0PB9X",
        "outputId": "466e1e5a-96d1-49cd-b49a-7023e4c9ee13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "char2idx = get_char2idx(DEFAULT_CHARS)\n",
        "idx2char = get_idx2char(DEFAULT_CHARS)\n",
        "\n",
        "dataset = get_encoded_dataset_from_csv('iam_hwr_line_level/iam_validation1.tsv', char2idx, 128, (64, 1024))\n",
        "\n",
        "model = FlorRecognizer(len(DEFAULT_CHARS) + 1)\n",
        "model.load_weights('iam_hwr_line_level_model/run1')\n",
        "\n",
        "for img, label in dataset.take(3):\n",
        "    output = model(tf.expand_dims(img, 0))\n",
        "    output = tf.nn.softmax(output)\n",
        "    prediction = tf.squeeze(tf.argmax(output, 2))\n",
        "    plt.imshow(tf.squeeze(img), cmap='gray')\n",
        "    plt.pause(0.01)\n",
        "    print(tf.reduce_max(output, 2))\n",
        "    print(prediction)\n",
        "    print('Prediction', idxs_to_str(prediction, idx2char))\n",
        "    print('Label:', idxs_to_str(label, idx2char, merge_repeated=False))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.99219644 0.9987124  0.9790725  0.999913   0.9991856  0.99433386\n",
            "  0.6873795  0.99914014 0.998611   0.99946994 0.95614207 0.99768484\n",
            "  0.9856639  0.99937266 0.97124046 0.9984666  0.99887186 0.69754905\n",
            "  0.9811871  0.9994947  0.99893314 0.9872751  0.9992449  0.9930715\n",
            "  0.8866467  0.5776214  0.7860524  0.9997595  0.9976036  0.9977137\n",
            "  0.9998386  0.8917285  0.76713103 0.8918675  0.99835473 0.59225166\n",
            "  0.9939766  0.94296485 0.8877005  0.99602073 0.99090827 0.99815506\n",
            "  0.9996499  0.999353   0.9996418  0.9980209  0.9998479  0.9989095\n",
            "  0.99695873 0.93216056 0.9403406  0.977958   0.9901833  0.9854785\n",
            "  0.99872464 0.9984674  0.9633972  0.9982942  0.9987697  0.7167892\n",
            "  0.99638534 0.9977895  0.98076564 0.8714543  0.99942446 0.9936819\n",
            "  0.9996197  0.9988005  0.9986577  0.9333749  0.9757857  0.9958405\n",
            "  0.99679106 0.9834552  0.99991953 0.9867736  0.9996972  0.9997031\n",
            "  0.99954814 0.99209094 0.9959254  0.9996271  0.93837804 0.8732678\n",
            "  0.9815105  0.9739353  0.41885766 0.99997866 0.9735025  0.5954321\n",
            "  0.97815955 0.99951375 0.99575484 0.9989157  0.9943838  0.47776547\n",
            "  0.5930113  0.9240736  0.996403   0.998789   0.6417883  0.9869186\n",
            "  0.99820745 0.7138778  0.9969625  0.9822953  0.8353546  0.99983156\n",
            "  0.99627554 0.88746023 0.9976229  0.999848   0.9874493  0.99991393\n",
            "  0.99919194 0.9981804  0.9970387  0.9914772  0.9998729  0.998106\n",
            "  0.9439649  0.996591   0.99865794 0.98128134 0.9986104  0.72724926\n",
            "  0.99670094 0.9994677 ]], shape=(1, 128), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 0  0 79  0  0 68  0  0  0 65  0  0 85  0  0 61  0 72 72  0 79  0 75  0\n",
            "  1  0 68  0 61  0  0 79  1  1 80 80 75  0  0  1  1 63  0 75  0 76  0  0\n",
            " 65  0  1  1 83  0  0 69 69 80  0 68 68  0  1  1 80  0 68  0 65  0  1  1\n",
            " 81  0  0 79  0 81  0  0 61  0 72  0  1  1 80  0 72 72 75  0 75  0  0 79\n",
            "  1  1 75  0 66  0  0  1  1  1 74  0 81  0  0 73  0  0 75  0 81  0  0 78\n",
            "  0  0 79  1 12  1 79  0], shape=(128,), dtype=int64)\n",
            "Prediction tf.Tensor(b'sheyalso has to cope with the usual tloos of numours , s', shape=(), dtype=string)\n",
            "Label: tf.Tensor(b'They also had to cope with the usual flood of rumours , so-', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.99992585 0.9222655  0.9978034  0.9979315  0.939628   0.9754876\n",
            "  0.9849954  0.4792418  0.9956483  0.9997379  0.9995852  0.9944431\n",
            "  0.9995708  0.79195416 0.9996433  0.99777454 0.748743   0.98992515\n",
            "  0.98694766 0.99991596 0.9852527  0.9990061  0.9937483  0.9925073\n",
            "  0.9984738  0.99689025 0.99376017 0.9961261  0.99768996 0.9948862\n",
            "  0.9927012  0.9039982  0.7172193  0.8120581  0.9534945  0.77754784\n",
            "  0.94675285 0.99938524 0.9953023  0.8132974  0.9979488  0.9999893\n",
            "  0.7442533  0.9996723  0.9975311  0.9985286  0.6799718  0.92594707\n",
            "  0.9981433  0.9842679  0.99678504 0.85005397 0.9703822  0.8886754\n",
            "  0.99531615 0.8810318  0.6281013  0.9957801  0.997227   0.52810866\n",
            "  0.55387217 0.99923754 0.957645   0.99061817 0.7528645  0.9993531\n",
            "  0.79013455 0.9993316  0.99799216 0.9982768  0.9998841  0.9984768\n",
            "  0.99876666 0.5308     0.9998964  0.99772435 0.96102375 0.7933008\n",
            "  0.9886868  0.99905044 0.9991636  0.95848674 0.7297539  0.99874926\n",
            "  0.99789387 0.9958495  0.9955604  0.9118412  0.9978078  0.99571925\n",
            "  0.970147   0.996292   0.9969875  0.98062366 0.99965954 0.83097714\n",
            "  0.99987984 0.96344006 0.9976476  0.91630036 0.99882835 0.7864127\n",
            "  0.9971462  0.9959818  0.9990225  0.993022   0.9927047  0.9846005\n",
            "  0.99919444 0.99660033 0.99900657 0.9322863  0.9905274  0.9991171\n",
            "  0.79523706 0.9916135  0.99650925 0.96451    0.97632253 0.9461675\n",
            "  0.9957301  0.99491286 0.99935454 0.6349305  0.9979315  0.926973\n",
            "  0.99804235 0.9992632 ]], shape=(1, 128), dtype=float32)\n",
            "tf.Tensor(\n",
            "[73  0  0  0 65  0  1  0 83  0  0 65  0 72  0 72  0  1  1 73  0  0  0 65\n",
            "  0  0 61  0  0 74  0  0 80  1 69  1 79  0 75 75  0 73  0  0  0 65  0  1\n",
            " 73  0  0 69 69 69 79  0  0  1  1  1 63  0 68  0  0 69 69 65  0 82  0 75\n",
            "  0 81  0 79  1 12 12  1 69 69 74 74  0 63  0 72  0 81  0  0 79  0 69  0\n",
            " 74  0 67 67  0  1  1  1 75  0  0 74  0  0 65  0  1  1  0 80  0 78  0 61\n",
            "  0  0 80  1  1  1 61  0], shape=(128,), dtype=int64)\n",
            "Prediction tf.Tensor(b'me well meant i some mis chievous , inclusing one trat a', shape=(), dtype=string)\n",
            "Label: tf.Tensor(b'me well meant , some mischievous , including one that a', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.99974865 0.9518518  0.99787104 0.99984777 0.99855655 0.8863352\n",
            "  0.99934906 0.99860317 0.9979837  0.99889165 0.62110806 0.9797544\n",
            "  0.86464715 0.99548906 0.9789357  0.9997009  0.9997584  0.93148\n",
            "  0.99982077 0.99939895 0.992334   0.98582447 0.9925444  0.9959751\n",
            "  0.9986829  0.9996469  0.9963761  0.9892406  0.99872166 0.8597144\n",
            "  0.99953663 0.9983139  0.9992494  0.99982685 0.72337776 0.98656076\n",
            "  0.64578384 0.9919338  0.9999391  0.9989133  0.997401   0.98647654\n",
            "  0.9819857  0.9993191  0.99394834 0.99954826 0.9999558  0.8161927\n",
            "  0.99269813 0.93486166 0.32685706 0.9999523  0.9980597  0.99323\n",
            "  0.9996854  0.7179607  0.99924445 0.9623346  0.5323955  0.999627\n",
            "  0.94364357 0.9990734  0.9985183  0.99892753 0.92010075 0.9390423\n",
            "  0.92273176 0.9951298  0.99041826 0.9903808  0.9990552  0.9811584\n",
            "  0.9942948  0.995168   0.999871   0.9807978  0.99833435 0.90939426\n",
            "  0.9997601  0.99991846 0.77131647 0.9940848  0.73284405 0.9115328\n",
            "  0.9994697  0.99945647 0.9880256  0.9999386  0.88839185 0.99955815\n",
            "  0.98774034 0.5200126  0.9262964  0.99649197 0.98316675 0.9997919\n",
            "  0.84689146 0.99688196 0.9993237  0.9998863  0.9991823  0.9998889\n",
            "  0.99884796 0.96414363 0.9879261  0.99592316 0.99886143 0.999943\n",
            "  0.6838547  0.9990465  0.9020389  0.92745113 0.99475384 0.89858145\n",
            "  0.9925696  0.9985876  0.5159374  0.9997905  0.9793528  0.99962234\n",
            "  0.99562836 0.9544687  0.9831676  0.99729866 0.98135906 0.9944007\n",
            "  0.99772805 0.9987816 ]], shape=(1, 128), dtype=float32)\n",
            "tf.Tensor(\n",
            "[73  0  0  0 61 61  0  0 74  0  0  1  1 68  0  0 61  0  0  0 79  0  1  1\n",
            " 62  0 65  0  0 65  0 74  0  0  1  1  0 79  0 65  0  0 65  0 74  0  0  0\n",
            "  1  1 37  0  0 65  0 65  0  0 65 69 69 74  0  0 67  0  0  1  1 66  0 69\n",
            " 78  0 75  0 73  0  0  0  0  1  1 82  0  0 61  0 81  0  0  0 80  0  0 68\n",
            "  0  0 61  0 72  0 72  0  1  1 79  0 78 61  0  0 80  0 69 69 69 75  0 74\n",
            "  0  1  1 75  0  0 74  0], shape=(128,), dtype=int64)\n",
            "Prediction tf.Tensor(b'man has been seen Feeeing firom vauthall sration on', shape=(), dtype=string)\n",
            "Label: tf.Tensor(b'man had been seen fleeing from Vauxhall station on', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqeIC-ZOMWYq"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "Here are a few functions to help us load our data into a format compatible with\n",
        "Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xrj9lg9WXs_"
      },
      "source": [
        "# The default list of characters used in the recognition model\n",
        "DEFAULT_CHARS = \" !\\\"#%&'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz{|}£¤§°²ÀÉàâçèéêëîôùûœſ€⊥𐌰𐌳𐌴𐌵𐌸𐌺𐌻𐌼𐌾𐍆𐍈\"\n",
        "# The default list of non-punctuation characters needed for the word beam search decoding algorithm\n",
        "DEFAULT_NON_PUNCTUATION = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz£¤§°²ÀÉàâçèéêëîôùûœſ€⊥𐌰𐌳𐌴𐌵𐌸𐌺𐌻𐌼𐌾𐍆\"\n",
        "# The default list of punctuation characters needed for hte word beam search decoding algorithm\n",
        "DEFAULT_PUNCTUATION = \" !\\\"#%&'()*+,-./0123456789:;<=>?[]_{|}\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV_Jg5RXWa1w"
      },
      "source": [
        "def str_charset_to_lists(charset):\n",
        "    \"\"\"\n",
        "    Turns string containing all desired characters into list of chars and indices. This is required for mapping\n",
        "    between integer and char representations for use in the recognition model.\n",
        "\n",
        "    :param charset: charset as string of chars to be represented in model.\n",
        "    \"\"\"\n",
        "    chars = list(charset)\n",
        "    indices = list(range(1, len(chars) + 1))\n",
        "    return chars, indices\n",
        "\n",
        "\n",
        "def get_char2idx(charset):\n",
        "    \"\"\"\n",
        "    A tensorflow lookup table is created and returned which allows us to encode word transcriptions on the fly\n",
        "    in the tf.data api. A standard python dictionary won't work when tensorflow is running in graph mode. This\n",
        "    function will return a lookup table to convert between chars and indices.\n",
        "\n",
        "    :param charset: string containing all desired characters to be represented\n",
        "    :return: A tensorflow lookup table to convert characters to integers\n",
        "    \"\"\"\n",
        "    chars, indices = str_charset_to_lists(charset)\n",
        "\n",
        "    char2idx = tf.lookup.StaticHashTable(\n",
        "        tf.lookup.KeyValueTensorInitializer(\n",
        "            keys=tf.constant(chars, dtype=tf.string),\n",
        "            values=tf.constant(indices, dtype=tf.int64),\n",
        "            key_dtype=tf.string,\n",
        "            value_dtype=tf.int64\n",
        "        ),\n",
        "        default_value=0,\n",
        "        name='char2idx_lookup'\n",
        "    )\n",
        "\n",
        "    return char2idx\n",
        "\n",
        "\n",
        "def get_idx2char(charset):\n",
        "    \"\"\"\n",
        "    A tensorflow lookup table is created and returned which allows us to encode word transcriptions on the fly\n",
        "    in the tf.data api. A standard python dictionary won't work when tensorflow is running in graph mode. This\n",
        "    function will return a lookup table to convert between indices and chars.\n",
        "\n",
        "    :param charset: string containing all desired characters to be represented.\n",
        "    :return: A tensorflow lookup table to convert integers to characters\n",
        "    \"\"\"\n",
        "    chars, indices = str_charset_to_lists(charset)\n",
        "\n",
        "    idx2char = tf.lookup.StaticHashTable(\n",
        "        tf.lookup.KeyValueTensorInitializer(\n",
        "            keys=tf.constant(indices, dtype=tf.int64),\n",
        "            values=tf.constant(chars, dtype=tf.string),\n",
        "            key_dtype=tf.int64,\n",
        "            value_dtype=tf.string\n",
        "        ),\n",
        "        default_value='',\n",
        "        name='idx2char_lookup'\n",
        "    )\n",
        "\n",
        "    return idx2char\n",
        "\n",
        "\n",
        "def pad_or_truncate(t, sequence_size=128):\n",
        "    \"\"\"\n",
        "    Pad or truncate a tensor to a fixed sequence length. Works for use in the tf.data api in graph mode.\n",
        "\n",
        "    :param t: The tensor to pad or truncate\n",
        "    :param sequence_size: The final sequence length of the tensor\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    dim = tf.size(t)\n",
        "    return tf.cond(tf.equal(dim, sequence_size), lambda: t,\n",
        "                   lambda: tf.cond(tf.greater(dim, sequence_size), lambda: tf.slice(t, [0], [sequence_size]),\n",
        "                                   lambda: tf.concat([t, tf.zeros(sequence_size - dim, dtype=tf.int64)], 0)))\n",
        "\n",
        "\n",
        "def merge_repeating_values(t):\n",
        "    \"\"\"\n",
        "    Merge repeating indices/characters in a tensor. Utilizes only tf.* functions which makes it\n",
        "    usable in graph mode.\n",
        "\n",
        "    :param t: The tensor to have repeated indices/characters merged\n",
        "    :return: A new tensor with repeating values merged\n",
        "    \"\"\"\n",
        "    t2 = tf.roll(tf.pad(t, [[0, 1]], constant_values=-1), -1, 0)[:tf.size(t)]\n",
        "    not_equal = tf.math.not_equal(t, t2)\n",
        "    indices = tf.where(not_equal)\n",
        "    return tf.reshape(tf.gather(t, indices), [-1])\n",
        "\n",
        "\n",
        "def str_to_idxs(string, char2idx, sequence_size):\n",
        "    \"\"\"\n",
        "    Perform the actual lookup to convert a string to its integer representation. This function also performs\n",
        "    padding according to the given sequence size. Works for use in the tf.data api in graph mode.\n",
        "\n",
        "    :param string: The string to be converted\n",
        "    :param char2idx: The tf lookup table\n",
        "    :param sequence_size: The final sequence length\n",
        "    :return: The converted string now in its integer representation\n",
        "    \"\"\"\n",
        "    idxs = tf.map_fn(lambda char: char2idx.lookup(char), tf.strings.unicode_split(string, 'UTF-8'), dtype=tf.int64)\n",
        "    return pad_or_truncate(idxs, sequence_size=sequence_size)\n",
        "\n",
        "\n",
        "def idxs_to_str(idxs, idx2char, merge_repeated=True):\n",
        "    \"\"\"\n",
        "    Perform the actual lookup to convert an integer to its string representation.\n",
        "    Works for use in the tf.data api in graph mode.\n",
        "\n",
        "    :param idxs: The idxs to be converted\n",
        "    :param idx2char: The tf lookup table\n",
        "    :param merge_repeated: Bool indicating whether or not to merge repeating values in the idx tensor\n",
        "    :return: The converted idxs now in its string representation\n",
        "    \"\"\"\n",
        "    if merge_repeated:\n",
        "        idxs = merge_repeating_values(idxs)\n",
        "\n",
        "    string = tf.map_fn(lambda idx: idx2char.lookup(idx), idxs, dtype=tf.string)\n",
        "    string = tf.strings.reduce_join(string)\n",
        "    return tf.strings.strip(string)\n",
        "\n",
        "\n",
        "def str_to_idxs_batch(batch, char2idx, sequence_size=128):\n",
        "    \"\"\"\n",
        "    Perform the same function as str_to_idxs, except a batch of strings are given as input\n",
        "\n",
        "    :param batch: A batch of strings as tensor, list, or numpy array\n",
        "    :param char2idx: The tf lookup table\n",
        "    :param sequence_size: The final sequence length of each string\n",
        "    :return: The converted strings now in its integer representation\n",
        "    \"\"\"\n",
        "    return tf.map_fn(lambda string: str_to_idxs(string, char2idx, sequence_size=sequence_size), batch,\n",
        "                     dtype=tf.int64)\n",
        "\n",
        "\n",
        "def idxs_to_str_batch(batch, idx2char, merge_repeated=True):\n",
        "    \"\"\"\n",
        "    Perform the same function as idxs_to_str, except a batch of idxs are given as input\n",
        "\n",
        "    :param batch: A batch of idxs as tensor, list, or numpy array\n",
        "    :param idx2char: The tf lookup table\n",
        "    :param merge_repeated: Bool indicating whether or not to merge repeating values in the idx tensor\n",
        "    :return: The converted idxs now in its string representation\n",
        "    \"\"\"\n",
        "    return tf.map_fn(lambda idxs: idxs_to_str(idxs, idx2char, merge_repeated=merge_repeated), batch,\n",
        "                     dtype=tf.string)\n",
        "\n",
        "\n",
        "def img_resize_with_pad(img_tensor, desired_size, pad_value=255):\n",
        "    \"\"\"\n",
        "    The standard tf.image.resize_with_pad function does not allow for specifying the pad value,\n",
        "    so we create a function with that capability here. Aspect ratio will be preserved.\n",
        "\n",
        "    :param img_tensor: The image tensor to be resized and padded\n",
        "    :param desired_size: The desired size (height, width)\n",
        "    :param pad_value: The value to pad the tensor with\n",
        "    \"\"\"\n",
        "    img_size = tf.shape(img_tensor)\n",
        "\n",
        "    img_ratio = img_size[0] / img_size[1]\n",
        "    desired_ratio = desired_size[0] / desired_size[1]\n",
        "\n",
        "    if img_ratio >= desired_ratio:\n",
        "        # Solve by height\n",
        "        new_height = desired_size[0]\n",
        "        new_width = int(desired_size[0] // img_ratio)\n",
        "    else:\n",
        "        new_height = int(desired_size[1] * img_ratio)\n",
        "        new_width = desired_size[1]\n",
        "        # Solve by width\n",
        "\n",
        "    resized_img = tf.image.resize(img_tensor, (new_height, new_width), method=tf.image.ResizeMethod.BICUBIC)\n",
        "\n",
        "    pad_height = desired_size[0] - new_height\n",
        "    pad_width = desired_size[1] - new_width\n",
        "\n",
        "    img_padded = tf.pad(resized_img, [[pad_height, 0], [0, pad_width], [0, 0]], constant_values=pad_value)\n",
        "\n",
        "    return img_padded\n",
        "\n",
        "\n",
        "def read_and_encode_image(img_path, img_size=(64, 1024)):\n",
        "    \"\"\"\n",
        "    Used by both encode_img_and_transcription (training) and encode_img_with_name (inference). This method\n",
        "    simply loads the image given a file path and performs the necessary encoding/resizing/transposing that\n",
        "    is necessary for use on the recognition model.\n",
        "\n",
        "    :param img_path: The path to the desired image\n",
        "    :param img_size: The size of the image after resizing/padding\n",
        "    :return: The encoded image in its tensor/integer representation\n",
        "    \"\"\"\n",
        "    img_bytes = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_image(img_bytes, channels=1, expand_animations=False)\n",
        "    img = img_resize_with_pad(img, img_size)\n",
        "    img = tf.image.per_image_standardization(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def encode_img_and_transcription(img_path, transcription, char2idx, sequence_size=128, img_size: tuple = (64, 1024)):\n",
        "    \"\"\"\n",
        "    The actual function to map image paths and string transcriptions to its tensor/integer representation.\n",
        "\n",
        "    :param img_path: The path to the desired image\n",
        "    :param transcription: The transcription of the image in integer form\n",
        "    :param char2idx: The tf lookup table\n",
        "    :param sequence_size: The final sequence length for transcriptions\n",
        "    :param img_size: The size of the image after resizing/padding\n",
        "    :return: The image and transcription in their tensor/integer representations.\n",
        "    \"\"\"\n",
        "    img = read_and_encode_image(img_path, img_size=img_size)\n",
        "    line = str_to_idxs(transcription, char2idx, sequence_size)\n",
        "    return img, line\n",
        "\n",
        "\n",
        "def encode_img_with_name(img_path, img_size=(64, 1024)):\n",
        "    \"\"\"\n",
        "    Used to map img_paths to encoded images for inference. Returned is the encoded image and image name.\n",
        "\n",
        "    :param img_path: The file path to the image\n",
        "    :param img_size: The size of the image after resizing/padding\n",
        "    :return: The encoded image and image path\n",
        "    \"\"\"\n",
        "    img = read_and_encode_image(img_path, img_size)\n",
        "    return img, img_path\n",
        "\n",
        "\n",
        "def get_dataset_size(csv_path):\n",
        "    \"\"\"\n",
        "    The tf.data api has a hard time producing the the dataset size. The cardinality() method often\n",
        "    returns unknown even with the CsvDataset. This function uses pandas to get the length.\n",
        "\n",
        "    :param csv_path: The path to csv containing information about the dataset\n",
        "    :return: The size of the dataset\n",
        "    \"\"\"\n",
        "    return len(pd.read_csv(csv_path, sep='\\t', header=None, names=['img_path', 'transcription']))\n",
        "\n",
        "\n",
        "def get_encoded_dataset_from_csv(csv_path, char2idx, max_seq_size, img_size):\n",
        "    \"\"\"\n",
        "    Using the tf.data api, load the desired csv with img_path and transcription data, encode the images and\n",
        "    transcriptions for use on the recognition model and return the desired tf dataset.\n",
        "\n",
        "    :param csv_path: The path to the tab delimited csv file containing | Image Path | Transcription |\n",
        "    :param char2idx: The tf lookup table to map characters to their respective integer representation\n",
        "    :param max_seq_size: The final sequence length for transcriptions\n",
        "    :param img_size: The size of the image after resizing/padding (height, width).\n",
        "    :return: The tf dataset containing encoded images and their respective transcriptions\n",
        "    \"\"\"\n",
        "    path_sep = os.path.sep\n",
        "    path_prefix = tf.strings.join(csv_path.split('/')[:-1], path_sep)\n",
        "    return tf.data.experimental.CsvDataset(csv_path, ['img', 'trans'], field_delim='\\t', use_quote_delim=False).map(\n",
        "        lambda img_path, transcription: encode_img_and_transcription(\n",
        "            tf.strings.join([path_prefix, tf.strings.reduce_join(tf.strings.split(img_path, '/'), separator=path_sep)],\n",
        "                            separator=path_sep),\n",
        "            transcription, char2idx, max_seq_size, img_size),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "def get_encoded_inference_dataset_from_img_path(img_path, img_size):\n",
        "    \"\"\"\n",
        "    Using the tf.data api, load all images from the desired path and return a dataset containing encoded images\n",
        "    and the image name (without path or extension information).\n",
        "\n",
        "    :param img_path: The path to the directory containing images\n",
        "    :param img_size: The size of the image after resizing/padding (height, width)\n",
        "    :return: The tf dataset containing encoded images and their respective string names\n",
        "    \"\"\"\n",
        "    return tf.data.Dataset.list_files(img_path + '/*', shuffle=False).map(\n",
        "        lambda path: encode_img_with_name(path, img_size),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmnqfENWL5it"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "Here is the necessary code we need to build the model for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jphfCKKIXYdP"
      },
      "source": [
        "class FullGatedConv2D(kl.Conv2D):\n",
        "    \"\"\"\n",
        "    Gated Convolutional Layer as described in the paper, Gated Convolutional Recurrent Neural Networks for Multilingual\n",
        "    Handwriting Recognition (https://ieeexplore-ieee-org.erl.lib.byu.edu/document/8270042). Code obtained from the Flor\n",
        "    implementation (https://github.com/arthurflor23/handwritten-text-recognition)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        \"\"\"\n",
        "        :param filters: The number of filters to be used for the convolution\n",
        "        :param kwargs: Additional kwargs to be passed to Conv2D\n",
        "        \"\"\"\n",
        "        super(FullGatedConv2D, self).__init__(filters=filters * 2, **kwargs)\n",
        "        self.nb_filters = filters\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass of gated convolutional layer\n",
        "\n",
        "        :param inputs: The input to the gated convolution as tensor\n",
        "        :return: The output of the gated convolution as tensor\n",
        "        \"\"\"\n",
        "        output = super(FullGatedConv2D, self).call(inputs)\n",
        "        linear = kl.Activation(\"linear\")(output[:, :, :, :self.nb_filters])\n",
        "        sigmoid = kl.Activation(\"sigmoid\")(output[:, :, :, self.nb_filters:])\n",
        "        return kl.Multiply()([linear, sigmoid])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = super(FullGatedConv2D, self).compute_output_shape(input_shape)\n",
        "        return tuple(output_shape[:3]) + (self.nb_filters,)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(FullGatedConv2D, self).get_config()\n",
        "        config['nb_filters'] = self.nb_filters\n",
        "        del config['filters']\n",
        "        return config"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmMgHX0nXYiz"
      },
      "source": [
        "class FlorRecognizer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Handwriting Recognition Model as described in the Blog Post,\n",
        "    https://medium.com/@arthurflor23/handwritten-text-recognition-using-tensorflow-2-0-f4352b7afe16.\n",
        "    This model combines ideas from the following papers:\n",
        "    - https://ieeexplore-ieee-org.erl.lib.byu.edu/document/8270042\n",
        "    - http://www.jpuigcerver.net/pubs/jpuigcerver_icdar2017.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, vocabulary_size=197):\n",
        "        \"\"\"\n",
        "        Define the model in terms of Keras layers\n",
        "\n",
        "        :param vocabulary_size: The number of possible classes that a character could belong to\n",
        "        \"\"\"\n",
        "        super(FlorRecognizer, self).__init__(name='flor_recognizer')\n",
        "\n",
        "        self.permute = kl.Permute([2, 1, 3])\n",
        "        self.conv1 = tf.keras.Sequential(name='conv1')\n",
        "        self.conv1.add(\n",
        "            kl.Conv2D(filters=16, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", kernel_initializer=\"he_uniform\"))\n",
        "        self.conv1.add(kl.PReLU(shared_axes=[1, 2]))\n",
        "        self.conv1.add(kl.BatchNormalization(renorm=True))\n",
        "        self.conv1.add(FullGatedConv2D(filters=16, kernel_size=(3, 3), padding=\"same\"))\n",
        "\n",
        "        self.conv2 = tf.keras.Sequential(name='conv2')\n",
        "        self.conv2.add(\n",
        "            kl.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"he_uniform\"))\n",
        "        self.conv2.add(kl.PReLU(shared_axes=[1, 2]))\n",
        "        self.conv2.add(kl.BatchNormalization(renorm=True))\n",
        "        self.conv2.add(FullGatedConv2D(filters=32, kernel_size=(3, 3), padding=\"same\"))\n",
        "\n",
        "        self.conv3 = tf.keras.Sequential(name='conv3')\n",
        "        self.conv3.add(\n",
        "            kl.Conv2D(filters=64, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", kernel_initializer=\"he_uniform\"))\n",
        "        self.conv3.add(kl.PReLU(shared_axes=[1, 2]))\n",
        "        self.conv3.add(kl.BatchNormalization(renorm=True))\n",
        "        self.conv3.add(\n",
        "            FullGatedConv2D(filters=64, kernel_size=(3, 3), padding=\"same\", kernel_constraint=kc.MaxNorm(4, [0, 1, 2])))\n",
        "        self.dropout1 = kl.Dropout(rate=0.2, name='dropout1')\n",
        "\n",
        "        self.conv4 = tf.keras.Sequential(name='conv4')\n",
        "        self.conv4.add(\n",
        "            kl.Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"he_uniform\"))\n",
        "        self.conv4.add(kl.PReLU(shared_axes=[1, 2]))\n",
        "        self.conv4.add(kl.BatchNormalization(renorm=True))\n",
        "        self.conv4.add(\n",
        "            FullGatedConv2D(filters=128, kernel_size=(3, 3), padding=\"same\", kernel_constraint=kc.MaxNorm(4, [0, 1, 2])))\n",
        "        self.dropout2 = kl.Dropout(rate=0.2, name='dropout2')\n",
        "\n",
        "        self.conv5 = tf.keras.Sequential(name='conv5')\n",
        "        self.conv5.add(\n",
        "            kl.Conv2D(filters=256, kernel_size=(2, 4), strides=(2, 4), padding=\"same\", kernel_initializer=\"he_uniform\"))\n",
        "        self.conv5.add(kl.PReLU(shared_axes=[1, 2]))\n",
        "        self.conv5.add(kl.BatchNormalization(renorm=True))\n",
        "        self.conv5.add(\n",
        "            FullGatedConv2D(filters=256, kernel_size=(3, 3), padding=\"same\", kernel_constraint=kc.MaxNorm(4, [0, 1, 2])))\n",
        "        self.dropout3 = kl.Dropout(rate=0.2, name='dropout3')\n",
        "\n",
        "        self.conv6 = tf.keras.Sequential(name='conv6')\n",
        "        self.conv6.add(\n",
        "            kl.Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", kernel_initializer=\"he_uniform\"))\n",
        "        self.conv6.add(kl.PReLU(shared_axes=[1, 2]))\n",
        "        self.conv6.add(kl.BatchNormalization(renorm=True))\n",
        "\n",
        "        self.mp = kl.MaxPooling2D(pool_size=(1, 2), strides=(1, 2), padding=\"valid\", name='mp')\n",
        "\n",
        "        self.gru1 = tf.keras.Sequential(name='gru1')\n",
        "        self.gru1.add(kl.Bidirectional(kl.GRU(units=256, return_sequences=True, dropout=0.5)))\n",
        "        self.gru1.add(kl.Dense(units=512))\n",
        "        self.gru1.add(kl.PReLU())\n",
        "\n",
        "        self.gru2 = tf.keras.Sequential(name='gru2')\n",
        "        self.gru2.add(kl.Bidirectional(kl.GRU(units=256, return_sequences=True, dropout=0.5)))\n",
        "        self.gru2.add(kl.Dense(units=vocabulary_size))\n",
        "\n",
        "    def call(self, x, training=False, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward pass of the Recognizer. The training parameter is passed to certain layers\n",
        "        in the model that act differently during training than they do during inference.\n",
        "\n",
        "        :param x: The input to the recognizer (batch, height, width, channels)\n",
        "        :param training: T/F - Is the model in training or inference mode?\n",
        "        :param kwargs: Additional parameters\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        out = self.permute(x)\n",
        "        # CNN\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.dropout1(out, training=training)\n",
        "        out = self.conv4(out)\n",
        "        out = self.dropout2(out, training=training)\n",
        "        out = self.conv5(out)\n",
        "        out = self.dropout3(out, training=training)\n",
        "        out = self.conv6(out)\n",
        "\n",
        "        # MaxPool and Reshape\n",
        "        out = self.mp(out)\n",
        "        # out = tf.squeeze(out)\n",
        "        out = tf.reshape(out, (-1, out.shape[1], out.shape[2] * out.shape[3]))\n",
        "\n",
        "        # RNN\n",
        "        out = self.gru1(out)\n",
        "        out = self.gru2(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wcTBmEQTcKv"
      },
      "source": [
        "## Choose a Pre-trained Model\n",
        "\n",
        "Out of all of our pre-trained models, let's find the best model to start with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXtPQ2BzT0Q_"
      },
      "source": [
        "# First, load in all of our pre-trained models\n",
        "iam_model = Recognizer()\n",
        "iam_model.load_weights()\n",
        "\n",
        "rimes_model = Recognizer()\n",
        "rimes_model.load_weights()\n",
        "\n",
        "bentham_model = Recognizer()\n",
        "bentham_model.load_weights()\n",
        "\n",
        "models = [iam_model, rimes_model, bentham_model]\n",
        "\n",
        "# Take a subset of the dataset to use for testing\n",
        "dataset = get_inference_dataset_from_...\n",
        "\n",
        "# Metrics to help us select the best model \n",
        "mean_confidence = tf.keras.metrics.Mean(name=\"confidence\")\n",
        "best_model = None\n",
        "best_score = 1000\n",
        "\n",
        "# Iterate over each of the models\n",
        "for model in models:\n",
        "    mean_confidence.reset_states()\n",
        "\n",
        "    # Iterate over the subset of the dataset\n",
        "    for img, img_name in dataset:\n",
        "        output = model(img)\n",
        "        prediction, confidence = predict_with_confidence(output)\n",
        "    \n",
        "    if mean_confidence.result() < best_score:\n",
        "        best_model = model\n",
        "\n",
        "print('Best Model:', best_model.name)\n",
        "print('Confidence Score:', best_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McGtcqyMctVx"
      },
      "source": [
        "# Run through the dataset and see if there are any lines where we are highly\n",
        "# confident in our predictions\n",
        "model = best_model\n",
        "confidence_threshold = .4\n",
        "\n",
        "training_set = []\n",
        "labeling_set = []\n",
        "\n",
        "for img, img_name in dataset:\n",
        "    output = model(img)\n",
        "    prediction, confidence = predict_with_confidence(output)\n",
        "\n",
        "    if confidence > confidence_threshold:\n",
        "        training_set.append([img, img_name])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}